{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJNMI-UW_uMT"
      },
      "source": [
        "#  Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "EecCEyMk_uMV"
      },
      "source": [
        "PCA по своей сути является алгоритмом уменьшения размерности, но он также может быть полезен как инструмент для визуализации, фильтрации шума, извлечения и инженерии признаков и многого другого. После краткого концептуального обсуждения алгоритма PCA мы рассмотрим несколько примеров этих дополнительных приложений."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "fOl-ZoJy_uMW",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ZDXRhagv_uMX"
      },
      "source": [
        "## Введение в метод главных компонент\n",
        "\n",
        "Метод главных компонент (PCA) — это быстрый и гибкий метод для уменьшения размерности данных. Его поведение легче всего визуализировать, рассматривая двумерный набор данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "m9imUrmd_uMX",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "be4256be-d3dc-4831-d32f-39235b0ab5ab"
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(1)\n",
        "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.axis('equal');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "yuv0I9me_uMX"
      },
      "source": [
        "Видно, что между переменными x и y существует почти линейная зависимость. Это напоминает данные линейной регрессии, которые мы уже рассматривали, но задача здесь немного другая: вместо того чтобы пытаться предсказать значения y на основе значений x, задача заключается в том, чтобы изучить взаимосвязь между значениями x и y.\n",
        "\n",
        "В методе главных компонент эта взаимосвязь количественно оценивается путем нахождения списка главных осей в данных и использования этих осей для описания набора данных. Воспользуемся PCA из Scikit-Learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "72jrlA8U_uMY",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "819c2114-8186-40f6-f70d-b369f88d99aa"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "VWwZLKEW_uMY"
      },
      "source": [
        "Обучение модели извлекает некоторые величины из данных, наиболее важными из которых являются компоненты и дисперсия."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "wnWxU_fq_uMY",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "8b453ede-05dd-4d7c-c1bf-d04202a71e4f"
      },
      "outputs": [],
      "source": [
        "print(pca.components_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "qp-jKUNO_uMY",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "198e2a59-d83c-4e13-eeee-cd3f81de79ce"
      },
      "outputs": [],
      "source": [
        "print(pca.explained_variance_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "38VUSjBQ_uMZ"
      },
      "source": [
        "Чтобы понять, что означают эти числа, давайте визуализируем их в виде векторов на входных данных, используя компоненты для определения направления вектора и дисперсию для определения длины вектора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "WxsNy3hB_uMZ",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "ce42b5c5-3588-4ff5-c76a-bbf8f115cbc5"
      },
      "outputs": [],
      "source": [
        "def draw_vector(v0, v1, ax=None):\n",
        "    ax = ax or plt.gca()\n",
        "    arrowprops=dict(arrowstyle='->', linewidth=2,\n",
        "                    shrinkA=0, shrinkB=0)\n",
        "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
        "\n",
        "# plot data\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
        "    v = vector * 3 * np.sqrt(length)\n",
        "    draw_vector(pca.mean_, pca.mean_ + v)\n",
        "plt.axis('equal');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "pt5zVSjj_uMZ"
      },
      "source": [
        "Эти векторы представляют собой главные оси данных, и длина каждого вектора указывает на то, насколько \"важна\" эта ось в описании распределения данных — более точно, это мера дисперсии данных при проекции на эту ось. Проекция каждой точки данных на главные оси является главной компонентой данных.\n",
        "\n",
        "Преобразование из осей данных в главные оси является *аффинным преобразованием*, что означает, что оно состоит из переноса, поворота и масштабирования (может быть описано матрицей)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "FS9ytXFO_uMZ"
      },
      "source": [
        "### PCA для уменьшения размерности\n",
        "\n",
        "Использование PCA для уменьшения размерности подразумевает обнуление одного или нескольких наименьших главных компонент, что приводит к проекции данных, которая сохраняет максимальную дисперсию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "xbpDjXfw_uMZ",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "b29d4431-9006-4d53-e041-5685942db57a"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=1)\n",
        "pca.fit(X)\n",
        "X_pca = pca.transform(X)\n",
        "print(\"Исходный размер:\", X.shape)\n",
        "print(\"Размер после преобразования:\", X_pca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "R6vyRZOx_uMa"
      },
      "source": [
        "Преобразованные данные были сведены к одному измерению.\n",
        "\n",
        "Чтобы понять эффект этого снижения размерности, мы можем выполнить обратное преобразование этих сокращенных данных и построить его вместе с исходными данными."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "WO5SX579_uMa",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "01a090b7-6b79-4e43-8423-f9ff69ed93b7"
      },
      "outputs": [],
      "source": [
        "X_new = pca.inverse_transform(X_pca)\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
        "plt.axis('equal');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5SFsdtbs_uMa"
      },
      "source": [
        "Светлые точки — это исходные данные, а темные точки — их спроецированная версия.\n",
        "\n",
        "Это проясняет, что означает уменьшение размерности PCA: информация вдоль наименее важной главной оси или осей удаляется, оставляя только компонент(ы) данных с самой высокой дисперсией.\n",
        "Доля удаляемой дисперсии (пропорциональная разбросу точек относительно линии, образованной на предыдущем рисунке) является примерной мерой того, сколько «информации» отбрасывается при этом уменьшении размерности."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "T8lCp-_F_uMa"
      },
      "source": [
        "### PCA для визуализации: рукописные цифры\n",
        "\n",
        "Польза снижения размерности может быть не совсем очевидна только в двух измерениях, но она становится ясной при рассмотрении многомерных данных.\n",
        "\n",
        "Давайте рассмотрим применение PCA к набору данных цифр, с которым мы уже работали."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "MSdkYuMw_uMa",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "f483e750-23dd-47ff-b08b-09160094ed13"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "digits.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "KBNhyu96_uMa"
      },
      "source": [
        "Напомним, что набор данных digits состоит из изображений размером 8 × 8 пикселей, что означает, что они являются 64-мерными.\n",
        "\n",
        "Чтобы получить некоторое представление о связях между этими точками, мы можем использовать PCA, чтобы спроецировать их на более управляемое количество измерений, скажем, на два."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "U6kA4Owq_uMa",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "3404e932-d746-46cb-f9d3-2fcdf1c21b50"
      },
      "outputs": [],
      "source": [
        "pca = PCA(2)  # project from 64 to 2 dimensions\n",
        "projected = pca.fit_transform(digits.data)\n",
        "print(digits.data.shape)\n",
        "print(projected.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "-sjf5Wd9_uMa",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "56aaf140-e361-4047-ca7f-b228eb0eefee"
      },
      "outputs": [],
      "source": [
        "plt.scatter(projected[:, 0], projected[:, 1],\n",
        "            c=digits.target, edgecolor='none', alpha=0.5,\n",
        "            cmap=plt.cm.get_cmap('rainbow', 10))\n",
        "plt.xlabel('component 1')\n",
        "plt.ylabel('component 2')\n",
        "plt.colorbar();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jwvcUqkb_uMb"
      },
      "source": [
        "### Что означают компоненты?\n",
        "\n",
        "Здесь мы можем пойти немного дальше и начать спрашивать, что *означают* сокращенные измерения.\n",
        "Это значение можно понять в терминах комбинаций базисных векторов.\n",
        "Например, каждое изображение в обучающем наборе определяется набором из 64 значений пикселей, которые мы назовем вектором $x$:\n",
        "\n",
        "$$\n",
        "x = [x_1, x_2, x_3 \\cdots x_{64}]\n",
        "$$\n",
        "\n",
        "Один из способов думать об этом — в терминах базиса пикселей.\n",
        "То есть, чтобы построить изображение, мы умножаем каждый элемент вектора на пиксель, который он описывает, а затем складываем результаты вместе, чтобы построить изображение:\n",
        "\n",
        "$$\n",
        "{\\rm image}(x) = x_1 \\cdot{\\rm (pixel~1)} + x_2 \\cdot{\\rm (pixel~2)} + x_3 \\cdot{\\rm (pixel~3)} \\cdots x_{64} \\cdot{\\rm (pixel~64)}\n",
        "$$\n",
        "\n",
        "Один из способов, который мы могли бы себе представить, чтобы уменьшить размерность этих данных, — это обнулить все, кроме нескольких из этих базисных векторов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "PWmsVv_U_uMb"
      },
      "source": [
        "Но попиксельное представление — это не единственный выбор базиса. Мы также можем использовать другие базисные функции, каждая из которых содержит некоторый предопределенный вклад от каждого пикселя, и записать что-то вроде:\n",
        "\n",
        "$$\n",
        "{\\rm image}(x) = {\\rm mean} + x_1 \\cdot{\\rm (basis~1)} + x_2 \\cdot{\\rm (basis~2)} + x_3 \\cdot{\\rm (basis~3)} \\cdots\n",
        "$$\n",
        "\n",
        "PCA можно рассматривать как процесс выбора оптимальных базисных функций, так что сложения только первых нескольких из них достаточно для подходящей реконструкции основной массы элементов в наборе данных.\n",
        "Главные компоненты, которые действуют как низкоразмерное представление наших данных, являются просто коэффициентами, которые умножают каждый из элементов в этой серии.\n",
        "На следующем рисунке показано похожее изображение реконструкции той же цифры с использованием среднего значения плюс первые восемь базисных функций PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "KzLHcE8l_uMb"
      },
      "source": [
        "### Выбор количества компонентов\n",
        "\n",
        "Важнейшей частью использования PCA на практике является возможность оценить, сколько компонентов необходимо для описания данных.\n",
        "Это можно определить, посмотрев на кумулятивное *объясненное отношение дисперсии* как функцию количества компонент."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "PiFgzIBi_uMb",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "b05b131c-f357-4f72-eeef-7b756d9bf034"
      },
      "outputs": [],
      "source": [
        "pca = PCA().fit(digits.data)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Q6l33aws_uMb"
      },
      "source": [
        "Эта кривая количественно определяет, какая часть общей 64-мерной дисперсии содержится в первых $N$ компонентах.\n",
        "Например, мы видим, что с цифровыми данными первые 10 компонентов содержат приблизительно 75% дисперсии, в то время как вам нужно около 50 компонентов, чтобы описать почти 100% дисперсии.\n",
        "\n",
        "Это говорит нам о том, что наша 2-мерная проекция теряет много информации (измеряемой по объясненной дисперсии) и что нам понадобится около 20 компонентов, чтобы сохранить 90% дисперсии. Рассмотрение этого графика для многомерного набора данных может помочь вам понять уровень избыточности, присутствующий в его признаках."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "gaYFhQAt_uMb"
      },
      "source": [
        "## PCA для фильтрации шума\n",
        "\n",
        "PCA также можно использовать для фильтрации зашумленных данных.\n",
        "Идея заключается в следующем: любые компоненты с дисперсией, намного большей, чем влияние шума, должны быть относительно незатронуты шумом.\n",
        "Таким образом, если вы реконструируете данные, используя только самое большое подмножество главных компонентов, вы должны предпочтительно сохранить сигнал и отбросить шум.\n",
        "\n",
        "Давайте посмотрим, как это выглядит с цифровыми данными."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "4zv1VF7o_uMb",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "64ebbd3d-6903-4022-c28b-8a43d1fd88ff"
      },
      "outputs": [],
      "source": [
        "def plot_digits(data):\n",
        "    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
        "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(data[i].reshape(8, 8),\n",
        "                  cmap='binary', interpolation='nearest',\n",
        "                  clim=(0, 16))\n",
        "plot_digits(digits.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "yhnohBjj_uMc"
      },
      "source": [
        "Теперь давайте добавим немного случайного шума, чтобы создать зашумленный набор данных, и перерисуем."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYvfedWQ_uMe",
        "outputId": "9fc8b327-ca16-4859-fde7-bce604bb832a"
      },
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "rng.normal(10, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "RECX--Q2_uMe",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "79f9fed1-b4c7-4f5d-828b-a9da49abe2d8"
      },
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "noisy = rng.normal(digits.data, 4)\n",
        "plot_digits(noisy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "gsqZ1V9C_uMe"
      },
      "source": [
        "Визуализация наглядно демонстрирует наличие этого случайного шума. \n",
        "\n",
        "Давайте обучим модель PCA на зашумленных данных, попросив, чтобы проекция сохранила 50% дисперсии."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "rOjzImOE_uMe",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "212cef39-f88f-4be5-c72f-ef8f27ca1c6c"
      },
      "outputs": [],
      "source": [
        "pca = PCA(0.50).fit(noisy)\n",
        "pca.n_components_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "vZIFFoAl_uMf"
      },
      "source": [
        "Здесь 50% дисперсии составляют 12 главных компонентов из 64 исходных признаков. \n",
        "\n",
        "Теперь мы вычисляем эти компоненты, а затем используем обратное преобразование для реконструкции отфильтрованных цифр."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "iwzpx002_uMf",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "0a1059b6-b76c-4879-9245-26d2dbe31c19"
      },
      "outputs": [],
      "source": [
        "components = pca.transform(noisy)\n",
        "filtered = pca.inverse_transform(components)\n",
        "plot_digits(filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "cybYZ_cc_uMf"
      },
      "source": [
        "Это свойство сохранения сигнала/фильтрации шума делает PCA очень полезной процедурой выбора признаков — например, вместо того, чтобы обучать классификатор на очень многомерных данных, вы можете вместо этого обучать классификатор на представлении главного компонента меньшей размерности, что автоматически будет отфильтровывать случайный шум во входных данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "OPWrSLgi_uMf"
      },
      "source": [
        "## Пример: Eigenfaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Q7qDAJS3_uMf",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "30d9b452-5d13-485f-cce4-43e60a60228b"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "faces = fetch_lfw_people(min_faces_per_person=60)\n",
        "print(faces.target_names)\n",
        "print(faces.images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "iI4924vs_uMf"
      },
      "source": [
        "Давайте рассмотрим главные оси, которые охватывают этот набор данных.\n",
        "Поскольку это большой набор данных, мы будем использовать \"случайный\" svd_solver в `PCA`, он использует рандомизированный метод для более быстрого приближения первых $N$ главных компонентов, чем стандартный подход, за счет некоторой потери точности. Этот компромисс может быть полезен для высокоразмерных данных (здесь размерность почти 3000)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "q8lekwQG_uMf",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "c017ae4e-53fe-4b77-9a9d-c578b98fe620"
      },
      "outputs": [],
      "source": [
        "pca = PCA(150, svd_solver='randomized', random_state=42)\n",
        "pca.fit(faces.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Ew3nwHeV_uMf"
      },
      "source": [
        "В этом случае может быть интересно визуализировать изображения, связанные с первыми несколькими главными компонентами (эти компоненты технически известны как *собственные векторы*, поэтому такие типы изображений часто называют *собственными лицами*):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "-DVJN9eI_uMf",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "1eece02f-08cd-4a29-a161-bcb174c6300f"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 8, figsize=(9, 4),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "glAbD5e4_uMf"
      },
      "source": [
        "Результаты очень интересны и дают нам представление о том, как изменяются изображения: например, первые несколько собственных лиц (сверху слева), похоже, связаны с углом освещения лица, а последующие главные векторы, похоже, выделяют определенные черты, такие как глаза, носы и губы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "iie2TBFx_uMg",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "2d12833e-ea95-4aff-c291-a5323bf71239"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "x-Cj8yCt_uMg"
      },
      "source": [
        "150 выбранных нами компонентов составляют чуть более 90% дисперсии.\n",
        "\n",
        "Это привело бы нас к мысли, что, используя эти 150 компонентов, мы восстановим большинство основных характеристик данных.\n",
        "\n",
        "Чтобы сделать это более конкретным, мы можем сравнить входные изображения с изображениями, восстановленными из этих 150 компонентов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "RmiL1ENN_uMg",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Compute the components and projected faces\n",
        "pca = pca.fit(faces.data)\n",
        "components = pca.transform(faces.data)\n",
        "projected = pca.inverse_transform(components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "G3nFoVxE_uMg",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "58defe72-e873-475d-c571-c9403150ec34"
      },
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),\n",
        "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "for i in range(10):\n",
        "    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n",
        "    ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')\n",
        "\n",
        "ax[0, 0].set_ylabel('full-dim\\ninput')\n",
        "ax[1, 0].set_ylabel('150-dim\\nreconstruction');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "f6NxbIXx_uMg"
      },
      "source": [
        "Эта визуализация ясно показывает, почему выбор признаков PCA используют: он уменьшает размерность данных почти в 20 раз, при этом проецируемые изображения содержат достаточно информации, чтобы мы могли на глаз распознать людей на каждом изображении. "
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "formats": "ipynb,md"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
