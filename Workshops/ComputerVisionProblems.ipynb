{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Примеры решения различных задач машинного зрения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как Вы знаете из лекции к основным задачам относятся:\n",
    "1. *Классификация изображений* - определение категории или метки всего изображения.\n",
    "2. *Детекция объектов* - Поиск и локализация объектов на изображении с указанием их координат.   \n",
    "3. *Сегментация изображений*\n",
    "   - *Семантическая сегментация* - классификация каждого пикселя изображения на основе принадлежности к определённому классу.\n",
    "   - *Инстанс-сегментация* - обнаружение отдельных экземпляров объектов с учетом их границ.\n",
    "4. *Распознавание объектов (лиц)* - детекция и идентификация объектов (по сути, совмещение задач классификации и детекции)\n",
    "5. *Работа с пространственными данными (облаками точек)* - определение глубины изображения и построение трёхмерной модели.\n",
    "6. *Анализ и обработка видео* - трекинг объектов, детектирование движений и распознавания действий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Больше примеров можно найти [тут](https://github.com/Charmve/computer-vision-in-action/tree/main/notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from segment_anything import (SamPredictor, \n",
    "                              sam_model_registry)\n",
    "\n",
    "from sentence_transformers import (SentenceTransformer, \n",
    "                                   util)\n",
    "\n",
    "from transformers import (BlipProcessor, \n",
    "                          BlipForConditionalGeneration)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import urllib.request\n",
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, \n",
    "                  local_path):\n",
    "    if not os.path.exists(local_path):\n",
    "        print(f\"Downloading {local_path} from {url} ...\")\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"Downloaded {local_path}\")\n",
    "    else:\n",
    "        print(f\"{local_path} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://djl.ai/examples/src/test/resources/dog_bike_car.jpg\"\n",
    "response = requests.get(image_url)\n",
    "image_data = np.asarray(bytearray(response.content), dtype=\"uint8\")\n",
    "image = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
    "image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "\n",
    "labels_path = \"imagenet_labels.json\"\n",
    "\n",
    "download_file(labels_url, labels_path)\n",
    "\n",
    "model = models.resnet50(weights=True)\n",
    "model.eval()\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "img = Image.fromarray(image)\n",
    "img_tensor = preprocess(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(img_tensor)\n",
    "    # get 3 highest probabilities\n",
    "    _, predicted = torch.topk(outputs, 5)\n",
    "\n",
    "predicted = predicted[0].tolist()\n",
    "\n",
    "with open(labels_path, \"r\") as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "predicted_label = \"\"\n",
    "for label in predicted:\n",
    "    predicted_label += str(labels[label]) + \", \"\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Predicted: {predicted_label}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection with Yolo V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_cfg_url = \"https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\"\n",
    "yolo_weights_url = \"https://pjreddie.com/media/files/yolov3.weights\"\n",
    "coco_names_url = \"https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\"\n",
    "\n",
    "yolo_cfg_path = \"yolov3.cfg\"\n",
    "yolo_weights_path = \"yolov3.weights\"\n",
    "coco_names_path = \"coco.names\"\n",
    "\n",
    "download_file(yolo_cfg_url, yolo_cfg_path)\n",
    "download_file(yolo_weights_url, yolo_weights_path)\n",
    "download_file(coco_names_url, coco_names_path)\n",
    "\n",
    "with open(coco_names_path, \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(yolo_cfg_path, yolo_weights_path)\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "outs = net.forward(output_layers)\n",
    "\n",
    "image_copy = image.copy()\n",
    "height, width, _ = image_copy.shape\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)\n",
    "\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "for i in indices:\n",
    "    i = i[0] if isinstance(i, (list, tuple, np.ndarray)) else i\n",
    "    x, y, w, h = boxes[i]\n",
    "    label = str(classes[class_ids[i]])\n",
    "    conf = confidences[i]\n",
    "    cv2.rectangle(image_copy, (x, y), (x+w, y+h), (128, 0, 128), 2)\n",
    "    cv2.putText(image_copy, f\"{label} {conf:.2f}\", (x, y - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (128, 0, 128), 2)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(image_copy)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance segmentation with Segment Anything Model (SAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Пример](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb) от авторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/facebookresearch/segment-anything.git -q\n",
    "# !curl https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O sam_vit_h.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = sam_model_registry[\"vit_h\"](checkpoint=\"sam_vit_h_4b8939.pth\")\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "predictor.set_image(image)\n",
    "\n",
    "input_point = np.array([[200, 375]])\n",
    "input_label = np.array([1])\n",
    "\n",
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    multimask_output=True,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(image)\n",
    "for mask in masks:\n",
    "    plt.imshow(mask, alpha=0.5, cmap='jet')\n",
    "plt.scatter(input_point[:, 0], input_point[:, 1], color='red', s=100)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "img_emb = model.encode(Image.fromarray(image))\n",
    "\n",
    "texts = ['Haskey', 'Dog', 'Cat', 'Bicycle']\n",
    "text_emb = model.encode(texts)\n",
    "\n",
    "cos_scores = util.cos_sim(img_emb, text_emb)[0]\n",
    "print(cos_scores)\n",
    "\n",
    "predicted_text = texts[torch.argmax(cos_scores).item()]\n",
    "print(f\"Best description: {predicted_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM description generation with BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "inputs = processor(images=Image.fromarray(image), return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated caption: {caption}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
