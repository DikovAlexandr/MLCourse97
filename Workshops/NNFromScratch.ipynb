{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua-hci7EA68n"
      },
      "source": [
        "## Лабораторная работа 1 \"Полносвязные нейронные сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDezYyEwA68q"
      },
      "source": [
        "В современных фреймворках, нейронные сети конструируются в виде разнообразных слоев. Каждый слой реализует два метода forward и backward. forward предназначен для расчета выхода слоя, backward для расчета градиента. В лабораторной лаботе вам необходимо рализовать оба метода для линейного, сигмоидального и Relu.\n",
        "После чего необходимо сконструировать нейронную сеть для решения задачи классификации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOcnRh-fA68r"
      },
      "source": [
        "Функция forward будет вычислять по $x$ значение $y$, backward — по $\\frac{\\partial L}{\\partial y}$ вычислять $\\frac{\\partial L}{\\partial x}$ и обновлять внутри себя $\\frac{\\partial L}{\\partial w}$.\n",
        "\n",
        "Важным требованием к реализации является векторизация всех слоев: все операции должны быть сведены к матричным, не должно быть циклов. Это значительно уменьшает временные затраты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kpFL1MBRA68s"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List, Union\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, \n",
        "                             confusion_matrix,\n",
        "                             ConfusionMatrixDisplay)\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs5d527zA68t"
      },
      "source": [
        "### Часть 1: Линейный слой"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvGaWU5TA68t"
      },
      "source": [
        "Приведем пример вычисления градиентов для линейного слоя: $y = Wx$, $x \\in \\mathbb{R}^{K \\times n}$, $y \\in \\mathbb{R}^{K \\times n}$, $W \\in \\mathbb{R}^{n \\times m}$, где $K$ — число объектов.\n",
        "\n",
        "Рассмотрим $L$ как функцию от выходов нейронной сети: $L = L(y_{11}, y_{12}, \\dots)$\n",
        "\n",
        "$$y_{kt} = (Wx)_{kt} = \\sum_{z=1}^{n} x_{kz}W_{zt}$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial x_{ij}} = \\sum_{kt} \\frac{\\partial L}{\\partial y_{kt}}\\frac{\\partial y_{kt}}{\\partial x_{ij}} = \\sum_{kt} \\frac{\\partial L}{\\partial y_{kt}}\\frac{\\partial \\sum_z x_{kz}w_{zt}}{\\partial x_{ij}}= \\sum_{t} \\frac{\\partial L}{\\partial y_{it}}\\frac{\\partial w_{jt}}{\\partial x_{ij}}$$\n",
        "\n",
        "$$\\frac{\\partial{L}}{\\partial x} = \\frac{\\partial{L}}{\\partial y}W^T$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Parameters:\n",
        "    \"\"\"\n",
        "    Класс для хранения весов и градиентов.\n",
        "    \"\"\"\n",
        "    def __init__(self, shape: Tuple[int, int]):\n",
        "        # Weights\n",
        "        self.weights = np.random.randn(*shape) * 0.1\n",
        "        # Gradients\n",
        "        self.grad = np.zeros_like(self.weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Linear:\n",
        "    \"\"\"\n",
        "    Линейный (полносвязный) слой.\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 input_size: int, \n",
        "                 output_size: int):\n",
        "        self.w = Parameters((input_size, output_size))\n",
        "        self.bias = Parameters((1, output_size))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Прямой проход через слой.\n",
        "        \n",
        "        Аргументы:\n",
        "        X: np.ndarray\n",
        "            Входной массив формы (N, input_size).\n",
        "        \n",
        "        Возвращает:\n",
        "        y: np.ndarray\n",
        "            Выходной массив формы (N, output_size).\n",
        "        \"\"\"\n",
        "        self.X = X.copy()\n",
        "        self.y = X @ self.w.weights + self.bias.weights  \n",
        "              \n",
        "        return self.y\n",
        "        \n",
        "    def backward(self, dLdy: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Обратное распространение ошибки.\n",
        "        \n",
        "        Вычисляет:\n",
        "            1. Градиент по входу: dL/dx = dL/dy @ W.T.\n",
        "            2. Градиент по весам: dL/dW = X.T @ dL/dy.\n",
        "            3. Градиент по смещениям: сумма по батчу.\n",
        "        \n",
        "        Аргументы:\n",
        "        dLdy: np.ndarray\n",
        "            Градиент ошибки по выходу слоя.\n",
        "        \n",
        "        Возвращает:\n",
        "        dLdx: np.ndarray\n",
        "            Градиент ошибки по входу слоя.\n",
        "        \"\"\"\n",
        "        # Градиент по входу: dL/dx = dL/dy @ W.T\n",
        "        dLdx = dLdy @ self.w.weights.T\n",
        "        # Градиент по весам: dL/dW = X.T @ dL/dy\n",
        "        self.w.grad = self.X.T.dot(dLdy)\n",
        "        # Градиент по смещениям\n",
        "        self.bias.grad = dLdy.sum(0)\n",
        "        \n",
        "        return dLdx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dmsEI9e05D2J"
      },
      "outputs": [],
      "source": [
        "def mse(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Вычисляет сумму квадратов ошибок (MSE).\n",
        "    \n",
        "    Аргументы:\n",
        "    y_pred: np.ndarray\n",
        "        Предсказанные значения.\n",
        "    y_true: np.ndarray\n",
        "        Истинные значения.\n",
        "    \n",
        "    Возвращает:\n",
        "    float\n",
        "        Сумма квадратов ошибок.\n",
        "    \"\"\"\n",
        "    return ((y_pred - y_true) ** 2).sum()\n",
        "\n",
        "def grad_mse(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Вычисляет градиент функции потерь MSE по предсказаниям.\n",
        "    \n",
        "    Аргументы:\n",
        "    y_pred: np.ndarray\n",
        "        Предсказанные значения.\n",
        "    y_true: np.ndarray\n",
        "        Истинные значения.\n",
        "    \n",
        "    Возвращает:\n",
        "    np.ndarray\n",
        "        Градиент MSE, 2 * (y_pred - y_true).\n",
        "    \"\"\"\n",
        "    return 2 * (y_pred - y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Тесты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "layer = Linear(input_size=4, output_size=3)\n",
        "\n",
        "X = np.random.randn(5, 4)\n",
        "output = layer.forward(X)\n",
        "assert output.shape == (5, 3), \"Неверная форма выхода прямого прохода\"\n",
        "\n",
        "dLdy = np.ones_like(output)\n",
        "dl_dx = layer.backward(dLdy)\n",
        "assert layer.w.grad.shape == (4, 3), \"Неверная форма градиента весов\"\n",
        "assert layer.bias.grad.shape == (3,), \"Неверная форма градиента смещений\"\n",
        "assert dl_dx.shape == (5, 4), \"Неверная форма градиента по входу\"\n",
        "\n",
        "print(\"Тест forward/backward для Linear слоя пройден.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = np.array([1.0, 2.0, 3.0])\n",
        "y_true = np.array([1.5, 2.5, 3.5])\n",
        "\n",
        "loss = mse(y_pred, y_true)\n",
        "expected_loss = ((y_pred - y_true) ** 2).sum()\n",
        "assert np.isclose(loss, expected_loss), \"Неверное значение MSE\"\n",
        "\n",
        "print(\"Тест для mse пройден.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grad = grad_mse(y_pred, y_true)\n",
        "expected_grad = 2 * (y_pred - y_true)\n",
        "assert np.allclose(grad, expected_grad), \"Неверное значение градиента MSE\"\n",
        "\n",
        "print(\"Тест для grad_mse пройден.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ohoXSZLHFpt"
      },
      "source": [
        "### Часть 2: Relu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qxQS0140NPOP"
      },
      "outputs": [],
      "source": [
        "class Relu:\n",
        "    \"\"\"\n",
        "    Функция активации ReLu\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Прямой проход через ReLU.\n",
        "        \n",
        "        Аргументы:\n",
        "        X: np.ndarray\n",
        "            Входной массив.\n",
        "            \n",
        "        Возвращает:\n",
        "        np.ndarray\n",
        "            Результат применения ReLU.\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "\n",
        "        return np.maximum(0, X)\n",
        "    \n",
        "    def backward(self, dLdy: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Обратное распространение через ReLU.\n",
        "        \n",
        "        Аргументы:\n",
        "        dLdy: np.ndarray\n",
        "            Градиент по выходу.\n",
        "            \n",
        "        Возвращает:\n",
        "        np.ndarray\n",
        "            Градиент по входу.\n",
        "        \"\"\"\n",
        "        dLdx = dLdy * (self.X > 0)\n",
        "        \n",
        "        return dLdx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Тесты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_relu = np.array([[-1, 2], [3, -4]])\n",
        "relu = Relu()\n",
        "forward_relu = relu.forward(X_relu)\n",
        "backward_relu = relu.backward(np.ones_like(X_relu))\n",
        "print(\"ReLU forward:\\n\", forward_relu)\n",
        "print(\"ReLU backward:\\n\", backward_relu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI3xlSHqHJ8_"
      },
      "source": [
        "### Часть 3: Sigmoid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kXATpUeUHmwo"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    \"\"\"\n",
        "    Функция активации сигмоида\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Прямой проход через сигмоиду.\n",
        "        \n",
        "        Аргументы:\n",
        "        X: np.ndarray\n",
        "            Входной массив формы (N, d).\n",
        "            \n",
        "        Возвращает:\n",
        "        np.ndarray\n",
        "            Результат применения сигмоиды той же формы.\n",
        "        \"\"\"\n",
        "        self.output = 1 / (1 + np.exp(-X))\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, dLdy: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Обратное распространение через сигмоиду.\n",
        "        \n",
        "        Вычисляет градиент: dLdx = dLdy * (output * (1 - output)).\n",
        "        \n",
        "        Аргументы:\n",
        "        dLdy: np.ndarray\n",
        "            Градиент по выходу.\n",
        "            \n",
        "        Возвращает:\n",
        "        np.ndarray\n",
        "            Градиент по входу.\n",
        "        \"\"\"\n",
        "        dSdx = self.output * (1 - self.output)\n",
        "        dLdx = dLdy * dSdx\n",
        "        \n",
        "        return dLdx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Тесты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sigmoid = Sigmoid()\n",
        "forward_sigmoid = sigmoid.forward(X_relu)\n",
        "backward_sigmoid = sigmoid.backward(np.ones_like(X_relu))\n",
        "print(\"Sigmoid forward:\\n\", forward_sigmoid)\n",
        "print(\"Sigmoid backward:\\n\", backward_sigmoid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kELDDVTUHcNW"
      },
      "source": [
        "### Часть 4: Функция потерь"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sjT4ZTnqHxSa"
      },
      "outputs": [],
      "source": [
        "class NLLLoss:\n",
        "    \"\"\"\n",
        "    Класс вычисляет функцию потерь\n",
        "    Применяет softmax к входам и вычисляет Negative Likelihood Loss.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, X: np.ndarray, y: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Прямой проход через слой NLLLoss.\n",
        "        \n",
        "        Аргументы:\n",
        "        X: np.ndarray\n",
        "            Входной массив формы (N, C), где C число классов.\n",
        "        y: np.ndarray\n",
        "            Вектор меток формы (N,).\n",
        "            \n",
        "        Возвращает:\n",
        "        float\n",
        "            Значение NLL loss.\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y \n",
        "\n",
        "        exp_X = np.exp(X)\n",
        "        self.probs = exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
        "\n",
        "        eps = 1e-10\n",
        "        logprobs = -np.log(self.probs[np.arange(X.shape[0]), y] + eps)\n",
        "        loss = np.mean(logprobs)\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Обратное распространение через NLLLoss.\n",
        "        Вычисляет градиент по входу:\n",
        "            dLdx = (probs - one_hot(y)) / N.\n",
        "            \n",
        "        Возвращает:\n",
        "        np.ndarray\n",
        "            Градиент по входу.\n",
        "        \"\"\"\n",
        "        N = self.probs.shape[0]\n",
        "        dLdx = self.probs.copy()\n",
        "        dLdx[np.arange(N), self.y] -= 1\n",
        "        dLdx /= N\n",
        "        return dLdx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Тесты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_nll = np.array([[2.0, 1.0, 0.1],\n",
        "                  [0.5, 1.5, 1.0]])\n",
        "y_nll = np.array([0, 2])\n",
        "nll_loss = NLLLoss()\n",
        "loss_value = nll_loss.forward(X_nll, y_nll)\n",
        "grad_nll = nll_loss.backward()\n",
        "print(\"NLLLoss loss:\", loss_value)\n",
        "print(\"NLLLoss gradient:\\n\", grad_nll)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3eMqBrnA68u"
      },
      "source": [
        "### Часть 5: Численный градиент"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6gL63ZVA68u"
      },
      "source": [
        "Реализуем функцию проверки численного градиента. Для этого для каждой переменной, по которой считается градиент, надо вычислить численный градиент: $f'(x) \\approx \\frac{f(x+\\epsilon)-f(x-\\epsilon)}{2\\epsilon}$. Функция возвращает максимальное абсолютное отклонение аналитического градиента от численного. В качестве $\\epsilon$ рекомендуется взять $10^{-6}$. При правильной реализации максимальное отличие будет иметь порядок $10^{-8}-10^{-6}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4NMWZB18A68u"
      },
      "outputs": [],
      "source": [
        "def check_gradient(func: callable, \n",
        "                   X: np.ndarray,\n",
        "                   return_max_deviation: bool=False) -> Union[np.ndarray, float]:\n",
        "    \"\"\"\n",
        "    Вычисляет численный градиент и сравнивает его с аналитическим.\n",
        "\n",
        "    Параметры:\n",
        "    func: callable\n",
        "        Функция, которая принимает X и возвращает кортеж (loss, grad).\n",
        "    X: np.ndarray\n",
        "        Тензор размера (n, m), по которому считается градиент.\n",
        "    return_max_deviation: bool\n",
        "        Если True, функция возвращает кортеж (численный градиент, максимальное отклонение),\n",
        "        иначе — только численный градиент.\n",
        "    \n",
        "    Возвращает:\n",
        "        Численный градиент, либо кортеж (численный градиент, максимальное отклонение).\n",
        "    \"\"\"\n",
        "    eps = 10**(-6)\n",
        "    numerical_grad = np.zeros_like(X)\n",
        "\n",
        "    for i in range(X.shape[0]):\n",
        "        for j in range(X.shape[1]):\n",
        "            orig = X[i, j]\n",
        "            X[i, j] = orig + eps\n",
        "            plus_eps, _ = func(X)\n",
        "            X[i, j] = orig - eps\n",
        "            minus_eps, _ = func(X)\n",
        "            numerical_grad[i, j] = (plus_eps - minus_eps) / (2 * eps)\n",
        "            X[i, j] = orig\n",
        "\n",
        "    _, analytical_grad = func(X)\n",
        "    max_deviation = np.max(np.abs(numerical_grad - analytical_grad))\n",
        "    \n",
        "    if return_max_deviation:\n",
        "        return numerical_grad, max_deviation\n",
        "    else :\n",
        "        return numerical_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Тесты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def quad_func(X):\n",
        "    loss = np.sum(X ** 2)\n",
        "    grad = 2 * X\n",
        "    return loss, grad\n",
        "\n",
        "X = np.random.randn(3, 4)\n",
        "\n",
        "numerical_grad, max_dev = check_gradient(quad_func, X.copy(), return_max_deviation=True)\n",
        "assert max_dev < 1e-6, f\"Максимальное отклонение слишком велико: {max_dev}\"\n",
        "\n",
        "_, analytical_grad = quad_func(X)\n",
        "assert np.allclose(numerical_grad, analytical_grad, atol=1e-6), \"Градиенты не сходятся\"\n",
        "\n",
        "print(\"Тесты check_gradient пройдены.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid_loss(X):\n",
        "    s = Sigmoid()\n",
        "    out = s.forward(X)\n",
        "    loss = np.sum(out)\n",
        "    grad = s.backward(np.ones_like(out))\n",
        "    return loss, grad\n",
        "\n",
        "X = np.random.randn(3, 4)\n",
        "\n",
        "numerical_grad, max_dev = check_gradient(sigmoid_loss, X.copy(), return_max_deviation=True)\n",
        "assert max_dev < 1e-5, f\"Максимальное отклонение для сигмоиды слишком велико: {max_dev}\"\n",
        "\n",
        "_, analytical_grad = sigmoid_loss(X)\n",
        "assert np.allclose(numerical_grad, analytical_grad, atol=1e-5), \"Градиенты не сходятся для сигмоиды\"\n",
        "\n",
        "print(\"Тесты Sigmoid пройдены.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3izDY1lA681"
      },
      "source": [
        "### Часть 6: Нейронная сеть"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOYn1yHRA681"
      },
      "source": [
        "Мы можем написать класс, который будет собирать всю сеть вместе."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "F5Wam9ZTA681"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, \n",
        "                 input_size: int, \n",
        "                 hidden_size: int, \n",
        "                 output_size: int):\n",
        "        self.layers = [Linear(input_size, hidden_size),\n",
        "                       Relu(),\n",
        "                       Linear(hidden_size, hidden_size),\n",
        "                       Relu(),\n",
        "                       Linear(hidden_size, output_size)]\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Прямой проход через сеть.\n",
        "\n",
        "        Аргументы:\n",
        "        X: np.ndarray\n",
        "            Входной массив формы (N, input_size).\n",
        "\n",
        "        Возвращает:\n",
        "        np.ndarray\n",
        "            Выход сети формы (N, output_size).\n",
        "        \"\"\"\n",
        "        out = X.copy()\n",
        "        for layer in self.layers:\n",
        "            out = layer.forward(out)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dLdy: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Обратное распространение ошибки через сеть.\n",
        "\n",
        "        Аргументы:\n",
        "        dLdy: np.ndarray\n",
        "            Градиент функции потерь по выходу сети.\n",
        "\n",
        "        Возвращает:\n",
        "        np.ndarray\n",
        "            Градиент функции потерь по входу сети.\n",
        "        \"\"\"\n",
        "        grad = dLdy\n",
        "        for layer in reversed(self.layers):\n",
        "            grad = layer.backward(grad)\n",
        "        return grad\n",
        "    \n",
        "    def get_parameters(self) -> List[Parameters]:\n",
        "        \"\"\"\n",
        "        Извлекает параметры (веса и смещения) всех линейных слоев сети.\n",
        "\n",
        "        Возвращает:\n",
        "        params: List\n",
        "            Список параметров для линейных слоев.\n",
        "        \"\"\"\n",
        "        params = []\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, Linear):\n",
        "                params.append(layer.w)\n",
        "                params.append(layer.bias)\n",
        "        return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Тесты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_size = 4\n",
        "hidden_size = 5\n",
        "output_size = 3\n",
        "n_samples = 10\n",
        "\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "X = np.random.randn(n_samples, input_size)\n",
        "\n",
        "out = nn.forward(X)\n",
        "assert out.shape == (n_samples, output_size), (\n",
        "    \"Неверная форма выхода нейронной сети на прямом проходе.\"\n",
        ")\n",
        "\n",
        "dLdy = np.ones_like(out)\n",
        "grad = nn.backward(dLdy)\n",
        "assert grad.shape == (n_samples, input_size), (\n",
        "    \"Неверная форма градиента обратного прохода.\"\n",
        ")\n",
        "\n",
        "params = nn.get_parameters()\n",
        "assert len(params) == 6, \"Неверное количество параметров.\"\n",
        "\n",
        "print(\"Тесты NeuralNetwork пройдены.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So2-ij1TIKu4"
      },
      "source": [
        "#### Часть 7: Оптимизатор\n",
        "\n",
        "Реализуем оптимизатор, похожий по функционалу на тот, которые используется во фреймворке Pytorch. \n",
        "\n",
        "Он состоит из трех методов (включая `__init__`):\n",
        "- `__init__` инициализирует параметры,\n",
        "- `step` - обновляет веса,\n",
        "- `zero_grad` - обнуляет градиенты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "v9iVJgy0IJ2R"
      },
      "outputs": [],
      "source": [
        "class Optimizer:\n",
        "    \"\"\"\n",
        "    Класс оптимизатора.\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 parameters: List[Parameters],\n",
        "                 lr: float):\n",
        "        self.parameters = parameters\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self) -> None:\n",
        "        \"\"\"\n",
        "        Выполняет один шаг оптимизации: обновляет веса параметров\n",
        "        по правилу: W = W - lr * grad.\n",
        "        \"\"\"\n",
        "        for param in self.parameters:\n",
        "            param.weights -= self.lr * param.grad\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        \"\"\"\n",
        "        Обнуляет градиенты всех параметров.\n",
        "        \"\"\"\n",
        "        for param in self.parameters:\n",
        "            param.grad = np.zeros_like(param.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Тесты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param = Parameters((2, 2))\n",
        "param.weights = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
        "param.grad = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
        "\n",
        "lr = 0.1\n",
        "optimizer = Optimizer([param], lr)\n",
        "\n",
        "weights_old = param.weights.copy()\n",
        "optimizer.step()\n",
        "expected_weights = weights_old - lr * param.grad\n",
        "assert np.allclose(param.weights, expected_weights, atol=1e-7), \"Метод step() обновляет веса неверно.\"\n",
        "\n",
        "optimizer.zero_grad()\n",
        "assert np.all(param.grad == 0), \"Метод zero_grad() не обнуляет градиенты корректно.\"\n",
        "\n",
        "print(\"Тесты Optimizer пройдены.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target = np.array([[3.0, 5.0]])\n",
        "param = Parameters((1, 2))\n",
        "param.weights = np.array([[0.0, 0.0]])\n",
        "\n",
        "optimizer = Optimizer([param], 0.1)\n",
        "\n",
        "num_iterations = 50\n",
        "trajectory = []\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "    trajectory.append(param.weights.copy())\n",
        "    param.grad = 2 * (param.weights - target)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "trajectory = np.concatenate(trajectory, axis=0)\n",
        "\n",
        "x = np.linspace(-1, 5, 200)\n",
        "y = np.linspace(-1, 7, 200)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = (X - 3) ** 2 + (Y - 5) ** 2\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "contours = plt.contour(X, Y, Z, \n",
        "                       levels=np.logspace(0, 3, 20), \n",
        "                       cmap='viridis')\n",
        "plt.clabel(contours, inline=True, fontsize=8)\n",
        "plt.plot(trajectory[:, 0], trajectory[:, 1], marker='o', color='red')\n",
        "plt.scatter(target[0, 0], target[0, 1], marker='x')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbOqs9hEA682"
      },
      "source": [
        "### Часть 8: Обучение на простых данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLVj0ntMA682"
      },
      "source": [
        "Создадим архитектуру вида 4 -> 100 -> 100 -> 3:\n",
        "* Linear(4, 100)\n",
        "* Relu()\n",
        "* Linear(100, 100)\n",
        "* Relu()\n",
        "* Linear(100, 3)\n",
        "\n",
        "* В качестве функции потерь будем использовать NLLLoss.\n",
        "\n",
        "* Для обновления весов воспользуемся написанным ранее оптимизатор.\n",
        "* Построим график сходимости (величина NLL после каждого обновления)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "oyHcWVFYN0po",
        "outputId": "b7d760c9-1609-4a99-d275-9a88cd2078d0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris(as_frame=True)\n",
        "print(iris.data.shape)\n",
        "iris.data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(iris.target.shape)\n",
        "print(iris.target.value_counts())\n",
        "\n",
        "classes = iris.target.unique()\n",
        "print(classes)\n",
        "\n",
        "iris.target.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(iris.data.to_numpy(), \n",
        "                                                    iris.target.to_numpy(), \n",
        "                                                    stratify=iris.target, \n",
        "                                                    test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_size = X_train.shape[1]\n",
        "hidden_size = 100\n",
        "output_size = np.unique(y_train).shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "nn = NeuralNetwork(input_size=input_size,\n",
        "                   hidden_size=hidden_size,\n",
        "                   output_size=output_size)\n",
        "\n",
        "sigmoid = Sigmoid()\n",
        "criterion = NLLLoss()\n",
        "\n",
        "optimizer = Optimizer(nn.get_parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 2000\n",
        "batch_size = 32\n",
        "\n",
        "loss_history = []\n",
        "num_samples = X_train.shape[0]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    indices = np.random.permutation(num_samples)\n",
        "    epoch_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for start in range(0, num_samples, batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        batch_indices = indices[start:start+batch_size]\n",
        "        X_batch = X_train[batch_indices]\n",
        "        y_batch = y_train[batch_indices]\n",
        "        \n",
        "        logits = nn.forward(X_batch)\n",
        "        loss = criterion.forward(logits, y_batch)\n",
        "        epoch_loss += loss\n",
        "        num_batches += 1\n",
        "        \n",
        "        dLdx = criterion.backward()\n",
        "        nn.backward(dLdx)\n",
        "        \n",
        "        optimizer.step()\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        avg_epoch_loss = epoch_loss / num_batches\n",
        "        loss_history.append(avg_epoch_loss)\n",
        "    \n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Avg Loss: {avg_epoch_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(np.linspace(0, num_epochs, len(loss_history)), loss_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "# plt.yscale('log')\n",
        "plt.title('Loss History')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "logits = nn.forward(X_test)\n",
        "print(round(accuracy_score(np.argmax(logits, axis=1), y_test), 3))\n",
        "\n",
        "cm = confusion_matrix(np.argmax(logits, axis=1), y_test)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=classes)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
