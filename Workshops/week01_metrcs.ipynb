{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрики для оценки моделей машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики для классификации\n",
    "В задачах классификации метрики оценивают, насколько хорошо модель предсказывает классы. \n",
    "\n",
    "Различают метрики для бинарной и многоклассовой классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.array([0, 1, 1, 0, 0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([0, 1, 0, 0, 1, 1, 1, 0, 1, 0])\n",
    "y_proba = np.array([0.1, 0.9, 0.4, 0.2, 0.8, 0.95, 0.6, 0.3, 0.85, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy (Точность)\n",
    "Accuracy — это доля правильно классифицированных объектов от общего числа примеров:\n",
    "\n",
    "> Если из 100 вопросов модель ответила правильно на 90, то ее точность 90%.\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{\\text{Количество правильных ответов}}{\\text{Общее количество вопросов}} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "- **Где:**  \n",
    "  - $ TP $ (True Positive) — количество верно предсказанных положительных примеров.  \n",
    "  - $ TN $ (True Negative) — количество верно предсказанных отрицательных примеров.  \n",
    "  - $ FP $ (False Positive) — количество ошибочно отнесенных к положительному классу примеров.  \n",
    "  - $ FN $ (False Negative) — количество ошибочно отнесенных к отрицательному классу примеров.  \n",
    "\n",
    "Представьте, что у вас есть модель, которая предсказывает, есть ли у человека собака.\n",
    "\n",
    "Пусть у нас есть 10 человек:\n",
    "- 5 человек с собаками (Положительный класс)\n",
    "- 5 человек без собак (Отрицательный класс)\n",
    "\n",
    "Наша модель сделала следующие предсказания:\n",
    "- **Истинные положительные (TP):** Модель правильно определила, что у 4 человек есть собаки.\n",
    "- **Истинные отрицательные (TN):** Модель правильно определила, что у 4 человек нет собак.\n",
    "- **Ложные положительные (FP):** Модель ошибочно определила, что у 1 человека есть собака (хотя ее нет).\n",
    "- **Ложные отрицательные (FN):** Модель ошибочно определила, что у 1 человека нет собаки (хотя она есть).\n",
    "\n",
    "Тогда точность будет:\n",
    "$$\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{4 + 4}{4 + 4 + 1 + 1} = \\frac{8}{10} = 0.8 \\text{ или } 80\\%\n",
    "$$\n",
    "\n",
    "Это означает, что модель была права в 80% случаев.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>Замечание:</b> \n",
    "Представьте, что вы разрабатываете модель для обнаружения очень редкого заболевания. Из 1000 человек только 10 болеют (1%). Остальные 990 здоровы (99%). Если ваша модель просто всегда будет предсказывать \"здоров\", то ее точность будет 0.99 или 99%. Модель с точностью 99% кажется отличной, но она не обнаружила ни одного больного человека и ее высокая точность абсолютно бесполезна.\n",
    "Accuracy не учитывает важность ошибок разного рода (ложных положительных и ложных отрицательных) и не подходит для несбалансированных классов.\n",
    "</div>\n",
    "\n",
    "**Применение:**  \n",
    "- Хорошо подходит, когда классы сбалансированы.  \n",
    "- Не подходит при сильном дисбалансе классов, так как можно получить высокую точность, просто предсказывая преобладающий класс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision (Точность, положительная предсказательная способность)\n",
    "Precision показывает, какая доля предсказанных положительных классов действительно является положительной:\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "**Применение:**  \n",
    "- Используется, когда важно минимизировать количество ложных срабатываний (FP), то есть цена ошибки первого рода высока, например, в медицинских тестах.\n",
    "\n",
    "> **Пример: Спам-фильтр почты**\n",
    "> Допустим, мы обучаем модель отправлять спам в отдельную папку.\n",
    "> - *Положительный класс (Positive) - письмо является спамом.\n",
    "> - *False Positive (FP) -важное письмо от начальника или уведомление из банка ошибочно помечено как спам и удалено с глаз долой.\n",
    ">\n",
    "> В данном случае **Precision** критически важна. Мы не хотим, чтобы модель ошибалась и закидывала нормальные письма в спам. Лучше пропустить немного реального спама во «Входящие» (понизив Recall), чем потерять одно важное письмо (понизив Precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "print(\"Precision:\", precision_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall (Полнота, чувствительность)\n",
    "Recall показывает, какая доля реальных положительных примеров была найдена моделью:\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "**Применение:**  \n",
    "- Важен в задачах, где критично не пропускать положительные примеры, например, при выявлении мошенничества.\n",
    "\n",
    "> **Пример: Диагностика тяжелых заболеваний**\n",
    "> Допустим, модель анализирует снимки легких, чтобы найти болезнь.\n",
    "> - *Положительный класс (Positive):* Человек болен.\n",
    "> - *False Negative (FN):* Человек болен, но модель сказала, что он здоров, и отправила его домой без лечения.\n",
    ">\n",
    "> В данном случае **Recall** — самая важная метрика. Если мы пропустим больного пациента, это может стоить ему жизни. Пусть лучше модель ложно сработает на здоровом человеке (False Positive), и врачи проведут дополнительные анализы, чем она пропустит реально больного. Мы готовы жертвовать Precision ради высокого Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "print(\"Recall:\", recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score\n",
    "F1-score — это гармоническое среднее Precision и Recall:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
    "$$\n",
    "\n",
    "**Применение:**  \n",
    "- Используется, когда важно найти баланс между Precision и Recall.\n",
    "- Полезна (но не идеальна) в задачах с несбалансированными классами (в отличие от Accuracy), так как требует хорошего показателя и по точности, и по полноте одновременно.\n",
    "\n",
    "Почему именно среднее гармоническое? Среднее гармоническое обладает свойством «пессимизма»: оно всегда ближе к меньшему из чисел.\n",
    "\n",
    "> **Пример:**\n",
    "> Представьте модель, которая предсказывает, что все объекты — положительные.\n",
    "> - У неё будет идеальный Recall = 1.0 (ведь всех положительных она нашла).\n",
    "> - Но Precision будет очень низким (например, 0.1), так как она назвала положительными кучу лишнего.\n",
    ">\n",
    "> 1. Среднее арифметическое: $\\frac{1.0 + 0.1}{2} = 0.55$.\n",
    ">\n",
    "> 2. F1-score: $2 \\times \\frac{1.0 \\times 0.1}{1.0 + 0.1} \\approx 0.18$.\n",
    "\n",
    "#### Обобщение\n",
    "F1-score предполагает, что точность и полнота для нас **одинаково важны**. Однако это не всегда так. Существует более общая формула $F_\\beta$, где коэффициент $\\beta$ определяет вес Recall по сравнению с Precision:\n",
    "\n",
    "$$\n",
    "F_\\beta = (1 + \\beta^2) \\times \\frac{Precision \\times Recall}{(\\beta^2 \\times Precision) + Recall}\n",
    "$$\n",
    "\n",
    "- $F_1$ ($\\beta = 1$) — стандартная F-мера. Precision и Recall равнозначны.\n",
    "- $F_2$ ($\\beta = 2$) — Recall важнее, чем Precision, и так далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "print(\"F1-score:\", f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC (Площадь под ROC-кривой)\n",
    "ROC AUC — это метрика для оценки качества бинарной классификации, которая оценивает модель в целом, не привязываясь к конкретному порогу принятия решения (threshold). Сама ROC-кривая показывает зависимость между True Positive Rate (TPR) и False Positive Rate (FPR):\n",
    "\n",
    "$$\n",
    "TPR = \\frac{TP}{TP + FN}, \\quad FPR = \\frac{FP}{FP + TN}\n",
    "$$\n",
    "\n",
    "AUC (Area Under Curve) — это площадь под ROC-кривой. Чем больше AUC, тем лучше модель.\n",
    "\n",
    "**Применение:**  \n",
    "- Позволяет оценить модель при различных порогах классификации.  \n",
    "- Подходит для задач с несбалансированными классами.\n",
    "\n",
    "Модель классификации обычно выдает вероятность принадлежности к классу 1 (например, `0.85`, `0.12`, `0.64`). Чтобы превратить это в ответ «Да/Нет», нам нужен **порог (threshold)**. Обычно это `0.5`, но его можно менять.\n",
    "Для того чтобы построить метрику:\n",
    "1. Сортируем все предсказания модели от самой высокой уверенности к самой низкой.\n",
    "2. Начинаем двигать порог принадлежности к классу от `1.0` до `0.0`.\n",
    "    - Если порог очень строгий (1.0), мы никого не классифицируем как 1. $TPR=0, FPR=0$ (точка 0,0).\n",
    "    - Если мы немного снижаем порог, мы начинаем захватывать истинные единицы (TPR растет), но иногда случайно захватываем и нули (FPR тоже растет).\n",
    "    - Если порог опустить до `0.0`, мы всех назовем классом 1. $TPR=1$, $FPR=1$ (точка 1,1).\n",
    "\n",
    "В итоге наша кривая — это линия, соединяющая точки (FPR, TPR) для всех возможных порогов.\n",
    "\n",
    "Свойства:\n",
    "- 0.5 — модель гадает случайно (как монетка). График идет по диагонали.\n",
    "- 1.0 — идеальная модель.\n",
    "\n",
    "> В дополнение можно сказать про **Precision-Recall Curve (PR AUC)** — во многом похожую метрику, с той лишь разницей, что она строится в осях Recall (X) и Precision (Y). ROC AUC может быть слишком оптимистичным на сильно несбалансированных выборках, так что если целевой класс очень редкий, лучше смотреть на **PR AUC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc_score(y_true, y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Случайное угадывание')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Loss (Logarithmic Loss)\n",
    "Логарифмическая функция потерь измеряет расхождение между предсказанной вероятностью принадлежности к классу и реальным классом:\n",
    "\n",
    "$$\n",
    "LogLoss = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n",
    "$$\n",
    "\n",
    "**Применение:**  \n",
    "- Используется в задачах, где важны вероятностные предсказания, например, в кредитном скоринге."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "print(\"Log Loss:\", log_loss(y_true, y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 0]\n",
    "y_pred = [1, 1, 1, 0, 0, 1]\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Многоклассовая классификация\n",
    "\n",
    "Подходы к решению многоклассовых задач:\n",
    "\n",
    "- One-vs-All (OvA) / One-vs-Rest (OvR)\n",
    "\n",
    "    Идея: Для каждого класса обучается отдельный бинарный классификатор. Этот классификатор учится отличать данный класс от всех остальных классов.\n",
    "\n",
    "    Обучение: Если у нас K классов, то обучается K бинарных классификаторов. Для i-го классификатора все примеры класса i считаются положительными примерами, а все остальные – отрицательными.\n",
    "\n",
    "    Предсказание: Для нового объекта каждый из K классификаторов выдает вероятность принадлежности к своему классу. Объект относят к классу с наибольшей вероятностью.\n",
    "\n",
    "\n",
    "- One-vs-One (OvO)\n",
    "\n",
    "    Идея: Для каждой пары классов обучается отдельный бинарный классификатор.\n",
    "\n",
    "    Обучение: Если у нас K классов, то обучается K * (K - 1) / 2 бинарных классификаторов. Каждый классификатор обучается только на данных, относящихся к двум классам.\n",
    "\n",
    "    Предсказание: Для нового объекта каждый классификатор выдает свой \"голос\" за один из двух классов. Объект относят к классу, за который проголосовало большинство классификаторов.\n",
    "\n",
    "    Преимущества: Менее подвержен проблеме дисбаланса классов (по сравнению с OvA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "X, y = make_classification(n_samples=300, \n",
    "                           n_features=2, \n",
    "                           n_informative=2, \n",
    "                           n_redundant=0,\n",
    "                           n_clusters_per_class=1, \n",
    "                           random_state=42, \n",
    "                           n_classes=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ova_lr = OneVsRestClassifier(LogisticRegression())\n",
    "ova_lr.fit(X_train, y_train)\n",
    "ova_lr_pred = ova_lr.predict(X_test)\n",
    "print(\"OvA (Logistic Regression):\\n\", classification_report(y_test, ova_lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovo_lr = OneVsOneClassifier(LogisticRegression())\n",
    "ovo_lr.fit(X_train, y_train)\n",
    "ovo_lr_pred = ovo_lr.predict(X_test)\n",
    "print(\"OvO (Logistic Regression):\\n\", classification_report(y_test, ovo_lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "Z_ova = ova_lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_ova = Z_ova.reshape(xx.shape)\n",
    "\n",
    "Z_ovo = ovo_lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_ovo = Z_ovo.reshape(xx.shape)\n",
    "\n",
    "# Two plots in one figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "ax1.contourf(xx, yy, Z_ova, cmap=plt.cm.viridis, alpha=0.8)\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.viridis, edgecolors='k')\n",
    "ax1.set_xlabel(\"Feature 1\")\n",
    "ax1.set_ylabel(\"Feature 2\")\n",
    "ax1.set_title(\"OvA Logistic Regression\")\n",
    "\n",
    "ax2.contourf(xx, yy, Z_ovo, cmap=plt.cm.viridis, alpha=0.8)\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.viridis, edgecolors='k')\n",
    "ax2.set_xlabel(\"Feature 1\")\n",
    "ax2.set_ylabel(\"Feature 2\")\n",
    "ax2.set_title(\"OvO Logistic Regression\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для многоклассовой классификации\n",
    "В многоклассовом случае confusion matrix расширяется до таблицы, где строки – истинные классы, а столбцы – предсказанные. Здесь используют следующие подходы для вычисления метрик:\n",
    "- Macro-average – усреднение метрик по каждому классу без учёта частот классов.\n",
    "- Micro-average – агрегирование TP, FP, FN по всем классам и последующее вычисление метрик.\n",
    "- Weighted-average – взвешенное усреднение, где для каждого класса вес равен его доле в выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "y_true = [2, 0, 2, 1, 0, 0]\n",
    "y_pred = [0, 1, 2, 2, 0, 2]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\", )\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_per_class(cm):\n",
    "    \"\"\"\n",
    "    Рассчитывает precision, recall, и F1-score для каждого класса по матрице ошибок.\n",
    "    \"\"\"\n",
    "    n_classes = cm.shape[0]\n",
    "    metrics = {}\n",
    "    for i in range(n_classes):\n",
    "        tp = cm[i, i]\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        tn = np.sum(cm) - tp - fp - fn\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        metrics[f'Class {i}'] = {'precision': round(precision, 3), \n",
    "                                           'recall': round(recall, 3), \n",
    "                                           'f1': round(f1, 3)}\n",
    "    return metrics\n",
    "\n",
    "metrics_per_class = calculate_metrics_per_class(confusion_matrix(y_true, y_pred))\n",
    "print(\"Metrics per class:\")\n",
    "for class_name, metrics in metrics_per_class.items():\n",
    "    print(f\"{class_name}: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_average(cm):\n",
    "    \"\"\"\n",
    "    Рассчитывает micro-averaged precision, recall, и F1-score.\n",
    "    \"\"\"\n",
    "    tp_total = np.trace(cm)\n",
    "    fp_total = np.sum(cm) - tp_total\n",
    "    fn_total = np.sum(cm) - tp_total\n",
    "\n",
    "    precision = tp_total / (tp_total + fp_total)\n",
    "    recall = tp_total / (tp_total + fn_total)\n",
    "    f1 = precision\n",
    "\n",
    "    return {'precision': round(precision, 3), \n",
    "            'recall': round(recall, 3), \n",
    "            'f1': round(f1, 3)}\n",
    "\n",
    "def macro_average(metrics_per_class):\n",
    "    \"\"\"\n",
    "    Рассчитывает macro-averaged precision, recall, и F1-score.\n",
    "    \"\"\"\n",
    "    precisions = [metrics['precision'] for metrics in metrics_per_class.values()]\n",
    "    recalls = [metrics['recall'] for metrics in metrics_per_class.values()]\n",
    "    f1s = [metrics['f1'] for metrics in metrics_per_class.values()]\n",
    "\n",
    "    precision = np.mean(precisions)\n",
    "    recall = np.mean(recalls)\n",
    "    f1 = np.mean(f1s)\n",
    "\n",
    "    return {'precision': round(precision, 3), \n",
    "            'recall': round(recall, 3), \n",
    "            'f1': round(f1, 3)}\n",
    "\n",
    "def weighted_average(metrics_per_class, y_true):\n",
    "    \"\"\"\n",
    "    Рассчитывает weighted-averaged precision, recall, и F1-score.\n",
    "    \"\"\"\n",
    "    class_counts = np.bincount(np.array([label for label in y_true]))\n",
    "    total_samples = len(y_true)\n",
    "\n",
    "    precision_weighted = 0\n",
    "    recall_weighted = 0\n",
    "    f1_weighted = 0\n",
    "\n",
    "    for i, class_name in enumerate(metrics_per_class.keys()):\n",
    "      weight = class_counts[i] / total_samples\n",
    "      precision_weighted += weight * metrics_per_class[class_name]['precision']\n",
    "      recall_weighted += weight * metrics_per_class[class_name]['recall']\n",
    "      f1_weighted += weight * metrics_per_class[class_name]['f1']\n",
    "\n",
    "    return {'precision': round(precision_weighted, 3), \n",
    "            'recall': round(recall_weighted, 3), \n",
    "            'f1': round(f1_weighted, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_metrics = micro_average(confusion_matrix(y_true, y_pred))\n",
    "macro_metrics = macro_average(metrics_per_class)\n",
    "weighted_metrics = weighted_average(metrics_per_class, y_true)\n",
    "\n",
    "print(\"Micro Average:\", micro_metrics)\n",
    "print(\"Macro Average:\", macro_metrics)\n",
    "print(\"Weighted Average:\", weighted_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Micro (sklearn):\",  f1_score(y_true, y_pred, average='micro'))\n",
    "print(\"Macro (sklearn):\", f1_score(y_true, y_pred, average='macro'))\n",
    "print(\"Weighted (sklearn):\", f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики для регрессии\n",
    "Регрессионные метрики оценивают, насколько точно модель предсказывает числовые значения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.array([3.2, 2.8, 4.5, 3.7, 5.1])\n",
    "y_pred = np.array([3.0, 3.1, 4.2, 3.9, 5.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "Средняя абсолютная ошибка (MAE) — это среднее абсолютное отклонение предсказанных значений от реальных:\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "**Преимущества:**  \n",
    "- Интерпретируемая, измеряется в тех же единицах, что и целевая переменная.  \n",
    "- Не штрафует за большие ошибки так же сильно, как MSE.\n",
    "\n",
    "**Применение:**  \n",
    "- Подходит, когда важна средняя ошибка предсказаний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)\n",
    "Среднеквадратичная ошибка (MSE) увеличивает штраф за большие ошибки:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "**Преимущества:**  \n",
    "- Удобна для оптимизации (градиентный спуск), так как дифференцируема.  \n",
    "- Чувствительна к выбросам.\n",
    "\n",
    "**Применение:**  \n",
    "- Используется в задачах, где важно минимизировать большие ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)\n",
    "Корень из MSE:\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{MSE}\n",
    "$$\n",
    "\n",
    "**Преимущества:**  \n",
    "- Измеряется в тех же единицах, что и целевая переменная.  \n",
    "- Подчеркивает крупные ошибки.\n",
    "\n",
    "**Применение:**  \n",
    "- Подходит, когда важно учитывать большие ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R² (Коэффициент детерминации)\n",
    "R² измеряет, какую долю дисперсии целевой переменной объясняет модель:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "- $ y_i $ - истинное значение целевой переменной.\n",
    "- $ \\hat{y}_i $ - предсказанное значение модели.\n",
    "- $ \\bar{y} $ - среднее значение целевой переменной по всем наблюдениям:\n",
    "\n",
    "**Применение:**  \n",
    "- Позволяет оценить, насколько хорошо модель объясняет данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "print(\"R² Score:\", r2_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPE (Mean Absolute Percentage Error)\n",
    "MAPE (средняя абсолютная процентная ошибка) измеряет среднюю величину ошибок модели в процентах относительно истинных значений. Это удобная метрика для оценки регрессионных моделей, поскольку позволяет интерпретировать ошибку в понятных процентных единицах.\n",
    "\n",
    "$$\n",
    "MAPE = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|\n",
    "$$\n",
    "\n",
    "**Преимущества**\n",
    "- Легко интерпретировать, результат показывает среднюю ошибку в процентах от фактического значения.\n",
    "- Универсальна для разных масштабов, так как относительная ошибка не зависит от единиц измерения.\n",
    "\n",
    "**Ограничения:**\n",
    "- Если в данных встречаются нулевые значения \\( y_i \\), формула может приводить к делению на ноль, что требует специальных подходов (например, добавление малой константы).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Рассчитывает среднюю абсолютную процентную ошибку (MAPE).\n",
    "    \n",
    "    Параметры:\n",
    "    y_true : массив истинных значений\n",
    "    y_pred : массив предсказанных значений\n",
    "    \n",
    "    Возвращает:\n",
    "    MAPE в процентах\n",
    "    \"\"\"\n",
    "    percentage_errors = np.abs((y_true - y_pred) / (y_true + epsilon))\n",
    "    \n",
    "    mape = np.mean(percentage_errors) * 100\n",
    "    return mape\n",
    "\n",
    "print(\"MAPE:\", mape(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики для кластеризации\n",
    "В задачах кластеризации используются метрики, оценивающие качество разбиения данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y_true = make_blobs(n_samples=100, centers=3, random_state=42)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score (Силуэтный коэффициент)\n",
    "Измеряет, насколько объекты внутри кластера ближе друг к другу, чем к объектам из других кластеров:\n",
    "\n",
    "$$\n",
    "s_i = \\frac{b_i - a_i}{\\max(a_i, b_i)}\n",
    "$$\n",
    "\n",
    "- $ a_i $ — среднее расстояние от объекта до других объектов внутри его кластера.  \n",
    "- $ b_i $ — среднее расстояние от объекта до объектов ближайшего кластера.\n",
    "\n",
    "**Применение:**  \n",
    "- Используется для оценки качества кластеризации без истинных меток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "print(\"Silhouette Score:\", silhouette_score(X, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies-Bouldin Index\n",
    "Оценивает среднее отношение внутрикластерного расстояния к межкластерному:\n",
    "\n",
    "$$\n",
    "DBI = \\frac{1}{N} \\sum_{i=1}^{N} \\max_{j \\neq i} \\frac{\\sigma_i + \\sigma_j}{d_{ij}}\n",
    "$$\n",
    "\n",
    "**Применение:**  \n",
    "- Чем меньше значение, тем лучше кластеризация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "\n",
    "print(\"Davies-Bouldin Index:\", davies_bouldin_score(X, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted Rand Index (ARI)\n",
    "Измеряет сходство между предсказанными кластерами и истинными метками:\n",
    "\n",
    "$$\n",
    "ARI = \\frac{\\sum_{ij} \\binom{n_{ij}}{2} - \\left[\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2} \\right] / \\binom{N}{2}}{0.5 \\left[\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2} \\right] - \\left[\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2} \\right] / \\binom{N}{2}}\n",
    "$$\n",
    "\n",
    "**Применение:**  \n",
    "- Используется, когда есть истинные метки кластеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "\n",
    "print(\"Adjusted Rand Index:\", adjusted_rand_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "# help(sklearn.metrics)\n",
    "dir(sklearn.metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
