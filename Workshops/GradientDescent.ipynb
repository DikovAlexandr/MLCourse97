{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентные методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:43:24.015593Z",
     "start_time": "2020-02-19T18:43:23.252434Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random_seed = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем матрицы признаков $X$ и правильного вектора весов $w_{true}$. Вектор цели (или матрица в общем случае) $Y$ вычисляется как $X\\mathbf{w}_{\\text{true}}$ с гауссовым шумом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_objects = 300\n",
    "batch_size = 10\n",
    "num_steps = 43\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "w_true = np.random.normal(size=(n_features, ))\n",
    "\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "\n",
    "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :] \n",
    "\n",
    "Y = X.dot(w_true) + np.random.normal(0, 1, n_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Напоминание:*\n",
    "\n",
    "В случае линейной модели\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = \\tilde{X} \\mathbf{w}\n",
    "$$\n",
    "и функции потерь **MSE**\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{y}, \\tilde{X}, \\mathbf{w}) = MSE(\\mathbf{y}, \\tilde{X}, \\mathbf{w}) = \\|\\mathbf{y} - \\tilde{X}\\mathbf{w}\\|^2_2 = \\sum_i (y_i - \\mathbf{x}^{\\top}_i \\mathbf{w})^2\n",
    "$$\n",
    "аналитическое решение принимает простую форму:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{w}} = (\\tilde{X}^\\top \\tilde{X})^{-1} \\tilde{X}^\\top \\mathbf{y}.\n",
    "$$\n",
    "\n",
    "Давайте проверим, как это работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_star = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n",
    "w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, аналитическое решение довольно близко к исходному."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте сгенерируем набор данных с коррелированными признаками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "n_objects = 300\n",
    "batch_size = 10\n",
    "num_steps = 43\n",
    "eps = 1e-3\n",
    "\n",
    "w_true = np.random.normal(size=(n_features, ))\n",
    "\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "\n",
    "X[:, -1] = X[:, -2] + np.random.uniform(-eps, eps, X[:, -2].shape)\n",
    "\n",
    "Y = X.dot(w_true) + np.random.normal(0, 1, (n_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_star = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n",
    "w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, второй и третий коэффициенты найдены неверно. Практически это означает, что модель является *нестабильной*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это можно исправить с помощью **регуляризации**.\n",
    "\n",
    "Давайте используем норму L2 вектора весов как член регуляризации, чтобы ограничить желаемое решение.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{reg}}(\\mathbf{y}, \\tilde{X}, \\mathbf{w}) = MSE(\\mathbf{y}, \\tilde{X}, \\mathbf{w}) + \\lambda\\|\\mathbf{w}\\|_2^2\n",
    "= \\|\\mathbf{y} - \\tilde{X}\\mathbf{w}\\|_2^2 + \\lambda\\|\\mathbf{w}\\|_2^2\n",
    "= \\sum_i \\Bigl(y_i - \\mathbf{x}_i^\\top\\mathbf{w}\\Bigr)^2 + \\lambda\\sum_{j}w_j^2.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "В этом случае также доступно аналитическое решение:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{w}}_{\\text{reg}} = (X^\\top X + \\lambda I_p)^{-1}X^\\top Y,\n",
    "$$\n",
    "где $I_p$ — диагональная матрица, состоящая из единиц (размера p).\n",
    "\n",
    "**Будьте внимательны с членом регуляризации**\n",
    "\n",
    "Если вы включили столбец единиц в матрицу X, поскольку регуляризировать член смещения (свободный) в линейной модели не нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_star_reg = np.linalg.inv(X.T.dot(X) \n",
    "                           + 0.02*np.eye(n_features)).dot(X.T).dot(Y)\n",
    "w_star_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналитическое решение, описанное выше, включает в себя обращение матрицы $X^\\top X$ (или $X^\\top X + \\lambda I$), что довольно затратно с точки зрения вычислительных ресурсов. Сложность обращения матрицы можно оценить как $O(p^3 + p^2 N)$ ($p$ - размерность вектора признаков). \n",
    "\n",
    "Это приводит к тому, что на практике пользуются итеративным методам оптимизации, которые более эффективны и, фактически, являются основным подходом к оптимизации в машинном обучении.\n",
    "\n",
    "Градиентный спуск является одним из самых популярных методов оптимизации и включает с себя целое семейство методов. \n",
    "\n",
    "Стоит отметить, что цель минимизации (в частности, значение функции потерь) должна быть **дифференцируемой** относительно параметров модели. Используя градиентный спуск, вектор весов $\\tilde{\\mathbf{w}}^{(t+1)}$ на шаге $t+1$ можно выразить в следующем виде:\n",
    "$$\n",
    "\\tilde{\\mathbf{w}}^{(t+1)} = \\tilde{\\mathbf{w}}^{(t)} - ​​\\eta_t \\nabla \\mathcal{L}(\\tilde{\\mathbf{w}}^{(t)}),\n",
    "$$\n",
    "где $\\eta_t$ является шагом градиентного спуска (иногда его еще называют скоростью обучения).\n",
    "\n",
    "Градиент в случае функции потерь MSE принимает следующий вид:\n",
    "\n",
    "$$\n",
    "\\nabla \\mathcal{L}(\\mathbf{w}) = -2X^\\top \\mathbf{y} + 2X^\\top X\\mathbf{w} = 2X^\\top (X \\tilde{\\mathbf{w}} - \\mathbf{y}).\n",
    "$$\n",
    "\n",
    "В этом случае сложность составляет $O(pN)$. \n",
    "\n",
    "Чтобы сделать градиентный спуск еще более эффективным можно использовать гипотезу об однородных данных, переходя к стохастическому градиентному спуску, который вычисляет градиент только по некоторому случайному подмножеству точек данных, поэтому окончательная сложность уменьшается до $O(pK)$, где $K << N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_descent(w_history, \n",
    "                      X, Y, \n",
    "                      w_true=None, \n",
    "                      title='Trajectory', \n",
    "                      margin=0.1, \n",
    "                      contour_levels=np.logspace(0, 2, 40)):\n",
    "    w1_min = w_history[:, 0].min() - margin\n",
    "    w1_max = w_history[:, 0].max() + margin\n",
    "    w2_min = w_history[:, 1].min() - margin\n",
    "    w2_max = w_history[:, 1].max() + margin\n",
    "\n",
    "    A, B = np.meshgrid(np.linspace(w1_min, w1_max, 100),\n",
    "                       np.linspace(w2_min, w2_max, 100))\n",
    "    \n",
    "    levels = np.empty_like(A)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            w_tmp = np.array([A[i, j], B[i, j]])\n",
    "            levels[i, j] = np.mean((np.dot(X, w_tmp) - Y)**2)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(r'$w_1$')\n",
    "    plt.ylabel(r'$w_2$')\n",
    "    plt.xlim(w1_min, w1_max)\n",
    "    plt.ylim(w2_min, w2_max)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    \n",
    "    CS = plt.contour(A, B, levels, levels=contour_levels, cmap=plt.cm.rainbow_r)\n",
    "    plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "    \n",
    "    if w_true is not None:\n",
    "        plt.scatter(w_true[0], w_true[1], c='r', label='True weights')\n",
    "    plt.scatter(w_history[:, 0], w_history[:, 1], label='Descent points')\n",
    "    plt.plot(w_history[:, 0], w_history[:, 1], label='Trajectory')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация траектории градиентного спуска\n",
    "\n",
    "Давайте внимательно рассмотрим путь оптимизации в простом двумерном пространстве (где признаки находятся в разных масштабах). Будем использовать функцию потерь MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Графики ниже показывают значения $\\tilde{\\mathbf{w}}^{(t)}$ на каждом шаге $t$. Красная точка в является конечной для $\\tilde{\\mathbf{w}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_objects = 300\n",
    "batch_size = 10\n",
    "num_steps = 43\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "w_true = np.random.normal(size=(n_features, ))\n",
    "\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "\n",
    "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :] \n",
    "\n",
    "Y = X.dot(w_true) + np.random.normal(0, 1, n_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "w_0 = np.random.uniform(-2, 2, n_features)-0.5\n",
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "step_size = 1e-2\n",
    "\n",
    "for i in range(num_steps):\n",
    "    w -= step_size * 2. * np.dot(X.T, (X.dot(w) - Y)) / Y.size\n",
    "    w_list.append(w.copy())\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_descent(w_list, X, Y, \n",
    "                  w_true=w_true, \n",
    "                  title='GD trajectory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вектор градиента ортогонален эквипотенциальной поверхности (линиям уровня). Вот почему путь оптимизации не такой гладкий. \n",
    "\n",
    "Давайте визуализируем направления градиента, для большей наглядности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "A_mini, B_mini = np.meshgrid(np.linspace(-3, 3, 40), np.linspace(-3, 3, 40))\n",
    "\n",
    "levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "        \n",
    "plt.figure(figsize=(8, 8))\n",
    "CS = plt.contour(A, B, levels, \n",
    "                 levels=np.logspace(-1, 1.5, num=40), cmap=plt.cm.rainbow_r)\n",
    "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "        \n",
    "gradients = np.empty_like(A_mini)\n",
    "for i in range(A_mini.shape[0]):\n",
    "    for j in range(A_mini.shape[1]):\n",
    "        w_tmp = np.array([A_mini[i, j], B_mini[i, j]])\n",
    "        antigrad = - 2 * 1e-3 * np.dot(X.T, np.dot(X, w_tmp) - Y) / Y.shape[0]\n",
    "        plt.arrow(A_mini[i, j], B_mini[i, j], \n",
    "                  antigrad[0], antigrad[1], head_width=0.02)\n",
    "\n",
    "plt.title('Antigradients demonstration')\n",
    "plt.xlabel(r'$w_1$')\n",
    "plt.ylabel(r'$w_2$')\n",
    "plt.xlim((w_true[0] - 1.5, w_true[0] + 1.5))\n",
    "plt.ylim((w_true[1] - .5, w_true[1] + .5))\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте взглянем на стохастический градиентный спуск. \n",
    "\n",
    "Пусть число элементов, вычисляемых функцией потерь для каждого состояния (`batch_size`), будет равно $10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "batch_size = 10\n",
    "num_steps = 10\n",
    "w = w_0.copy()\n",
    "w_history_list = [w.copy()]\n",
    "lr = 1e-2\n",
    "\n",
    "for i in range(2*num_steps):\n",
    "    sample_indices = np.random.randint(0, n_objects, batch_size)\n",
    "    X_batch = X[sample_indices, :]\n",
    "    Y_batch = Y[sample_indices]\n",
    "    w -= 2 * lr * np.dot(X_batch.T, X_batch @ w - Y_batch) / batch_size\n",
    "    w_history_list.append(w.copy())\n",
    "w_history_list = np.array(w_history_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_descent(w_history_list, X, Y, \n",
    "                  w_true=w_true,\n",
    "                  title='SGD trajectory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из графика, SGD «блуждает» вокруг оптимумов. \n",
    "\n",
    "Это контролируется размером шага SGD $\\eta_k$, а сходимость в общем случае не гарантируется. Для сходимости метода SGD при заданной последовательности шагов $\\{\\eta_k\\}$ необходимо, чтобы выполнялись [условия Роббинса-Монро](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586):\n",
    "$$\n",
    "\\sum_{k = 1}^\\infty \\eta_k = \\infty, \\qquad \\sum_{k = 1}^\\infty \\eta_k^2 < \\infty.\n",
    "$$\n",
    "Более интуитивно эти условия можно объяснить следующим образом:\n",
    "1. Последовательность шагов $\\{\\eta_k\\}$ должна расходиться, чтобы метод оптимизации мог достичь любой точки в заданном пространстве параметров,\n",
    "2. В то же время она должна расходиться «не так быстро»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализируем траектории SGD, которые генерируются последовательностью шагов, удовлетворяющих условиям Роббинса-Монро:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "lr_0 = 2e-2\n",
    "num_steps = 100\n",
    "\n",
    "for i in range(num_steps):\n",
    "    lr = lr_0 / ((i+1) ** 0.5005)\n",
    "    sample_indices = np.random.randint(0, n_objects, batch_size)\n",
    "    X_batch = X[sample_indices, :]\n",
    "    Y_batch = Y[sample_indices]\n",
    "    w -= 2 * lr * np.dot(X_batch.T, X_batch @ w - Y_batch) / batch_size\n",
    "\n",
    "    w_list.append(w.copy())\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_descent(w_list, X, Y, \n",
    "                  w_true=w_true,\n",
    "                  title='SGD trajectory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один подход - градиентный спуск с импульсом (Momentum)\n",
    "\n",
    "Метод импульса помогает ускорить сходимость и смягчить колебания, добавляя \"инерцию\" к обновлениям:\n",
    "$$\n",
    "\\mathbf{v}^{(t+1)} = \\gamma \\mathbf{v}^{(t)} + \\eta \\nabla \\mathcal{L}(\\tilde{\\mathbf{w}}^{(t)}),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{w}}^{(t+1)} = \\tilde{\\mathbf{w}}^{(t)} - \\mathbf{v}^{(t+1)},\n",
    "$$\n",
    "где:\n",
    "- $\\mathbf{v}^{(t)}$ — накопленный градиент (скорость),\n",
    "- $\\gamma$ — коэффициент импульса (обычно от 0.8 до 0.99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "lr_0 = 1e-2\n",
    "gamma = 0.6\n",
    "v = np.zeros_like(w)\n",
    "\n",
    "for i in range(num_steps):\n",
    "    grad = 2 * np.dot(X.T, (np.dot(X, w) - Y)) / n_objects\n",
    "    v = gamma * v + lr * grad\n",
    "    w -= v\n",
    "    w_list.append(w.copy())\n",
    "\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_descent(w_list, X, Y, \n",
    "                  w_true=w_true,\n",
    "                  title='Momentum GD Trajectory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "**Идея:** Метод AdaGrad корректирует скорость обучения для каждого параметра в зависимости от суммы квадратов предыдущих градиентов. Это позволяет уменьшать шаги по направлениям, где градиенты были большими, и сохранять более высокие шаги там, где градиенты малы.\n",
    "\n",
    "Пусть $ g_t $ — градиент на шаге $ t $. Тогда суммарное накопление квадратов градиентов:\n",
    "$$\n",
    "G_t = \\sum_{i=1}^t g_i^2.\n",
    "$$\n",
    "Обновление параметров:\n",
    "$$\n",
    "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\circ g_t,\n",
    "$$\n",
    "где:\n",
    "- $\\eta$ — базовая скорость обучения,\n",
    "- $\\epsilon$ — небольшая константа для избежания деления на ноль,\n",
    "- $\\circ$ обозначает поэлементное деление.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "lr = 0.5\n",
    "epsilon = 1e-8\n",
    "accum = np.zeros_like(w)\n",
    "\n",
    "for t in range(num_steps):\n",
    "    grad = 2 * np.dot(X.T, (np.dot(X, w) - Y)) / n_objects\n",
    "    accum += grad**2\n",
    "    w -= lr * grad / (np.sqrt(accum) + epsilon)\n",
    "    w_list.append(w.copy())\n",
    "\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_descent(w_list, X, Y, \n",
    "                  w_true=w_true,\n",
    "                  title='AdaGrad Trajectory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "**Идея:**  \n",
    "RMSProp решает проблему затухания скорости обучения в AdaGrad за счёт использования экспоненциального скользящего среднего квадратов градиентов вместо их простой суммы.\n",
    "\n",
    "**Обновление:**  \n",
    "Обновляем экспоненциальное скользящее среднее:\n",
    "$$\n",
    "E[g^2]_t = \\beta E[g^2]_{t-1} + (1-\\beta) g_t^2,\n",
    "$$\n",
    "где $\\beta$ — коэффициент затухания (обычно около 0.9). Затем обновляем параметры:\n",
    "$$\n",
    "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\, g_t.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "lr = 0.1\n",
    "beta=0.7\n",
    "epsilon=1e-8\n",
    "\n",
    "E_grad2 = np.zeros_like(w)\n",
    "for t in range(num_steps):\n",
    "    grad = 2 * np.dot(X.T, (np.dot(X, w) - Y)) / n_objects\n",
    "    E_grad2 = beta * E_grad2 + (1 - beta) * (grad**2)\n",
    "    w -= lr * grad / (np.sqrt(E_grad2) + epsilon)\n",
    "    w_list.append(w.copy())\n",
    "\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_descent(w_list, X, Y, \n",
    "                  w_true=w_true,\n",
    "                  title='RMSProp Trajectory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam (Adaptive Moment Estimation)\n",
    "\n",
    "**Идея:**  \n",
    "Adam сочетает в себе идеи импульсного метода и RMSProp. Он отслеживает как скользящее среднее градиентов, так и скользящее среднее квадратов градиентов. Также применяется коррекция смещения для обоих моментов.\n",
    "\n",
    "**Обновление:**  \n",
    "1. Обновляем экспоненциальное скользящее среднее градиентов:\n",
    "   $$\n",
    "   m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t.\n",
    "   $$\n",
    "2. Обновляем экспоненциальное скользящее среднее квадратов градиентов:\n",
    "   $$\n",
    "   v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2.\n",
    "   $$\n",
    "3. Коррекция смещения:\n",
    "   $$\n",
    "   \\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}.\n",
    "   $$\n",
    "4. Обновление параметров:\n",
    "   $$\n",
    "   w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\, \\hat{m}_t.\n",
    "   $$\n",
    "   \n",
    "Обычно выбирают $\\beta_1 = 0.9$, $\\beta_2 = 0.999$ и $\\epsilon = 10^{-8}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "lr = 0.1\n",
    "m = np.zeros_like(w)\n",
    "v = np.zeros_like(w)\n",
    "beta1=0.9\n",
    "beta2=0.999\n",
    "epsilon=1e-8\n",
    "\n",
    "for t in range(1, num_steps + 1):\n",
    "    grad = 2 * np.dot(X.T, (np.dot(X, w) - Y)) / n_objects\n",
    "    m = beta1 * m + (1 - beta1) * grad\n",
    "    v = beta2 * v + (1 - beta2) * (grad**2)\n",
    "    m_hat = m / (1 - beta1**t)\n",
    "    v_hat = v / (1 - beta2**t)\n",
    "    w -= lr * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "    w_list.append(w.copy())\n",
    "\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_descent(w_list, X, Y,\n",
    "                  w_true=w_true,\n",
    "                  title='Adam Trajectory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение скорости сходимости\n",
    "Наконец, важно сравнить скорость сходимости для полного и стохастического GD. Давайте сгенерируем случайный набор данных и построим график значения функции потерь по номеру итерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 50\n",
    "n_objects = 1000\n",
    "num_steps = 500\n",
    "batch_size = 10\n",
    "\n",
    "w_true = np.random.uniform(-2, 2, n_features)\n",
    "\n",
    "X = np.random.uniform(-10, 10, (n_objects, n_features))\n",
    "Y = X.dot(w_true) + np.random.normal(0, 5, n_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sgd = 1e-3\n",
    "lr_gd = 1e-3\n",
    "w_sgd = np.random.uniform(-4, 4, n_features)\n",
    "w_gd = w_sgd.copy()\n",
    "\n",
    "residuals_sgd = [np.mean(np.power(np.dot(X, w_sgd) - Y, 2))]\n",
    "residuals_gd = [np.mean(np.power(np.dot(X, w_gd) - Y, 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_steps):\n",
    "    lr = lr_gd / ((i + 1) ** 0.51)\n",
    "    grad_gd = 2 * np.dot(X.T, (np.dot(X, w_gd) - Y)) / n_objects\n",
    "    w_gd = w_gd - lr * grad_gd\n",
    "    residuals_gd.append(np.mean((np.dot(X, w_gd) - Y) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_steps * 5):\n",
    "    lr = lr_sgd / ((i + 1) ** 0.51)\n",
    "    sample = np.random.randint(n_objects, size=batch_size)\n",
    "    grad_sgd = 2 * np.dot(X[sample].T, \n",
    "                          (np.dot(X[sample], w_sgd) - Y[sample])) / batch_size\n",
    "    w_sgd = w_sgd - lr * grad_sgd\n",
    "    residuals_sgd.append(np.mean((np.dot(X, w_sgd) - Y) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.arange(num_steps + 1) * n_objects, residuals_gd, label='GD')\n",
    "plt.plot(np.arange(num_steps * 5 + 1) * batch_size, residuals_sgd, label='SGD')\n",
    "plt.xlim((-1, (num_steps+1)*n_objects))\n",
    "plt.legend()\n",
    "plt.xlabel('Iter num')\n",
    "plt.ylabel(r'Q($w$)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как показано, SGD достигает оптимума в течение нескольких итераций. \n",
    "\n",
    "Обычно более крупные модели демонстрируют большие колебания значений функции потерь в процессе сходимости методов на основе стохастического градиента. На практике размер шага SGD может быть скорректирован для достижения лучшей скорости сходимости, и существует несколько методов, которые реализуют размер шага адаптивного градиентного спуска: AdaGrad, Adam, RMSProp и т. д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительно: Анализ нестабильности\n",
    "Используя новую технику, давайте проанализируем поведение линейной регрессии в случае мультиколлинеарных признаков.\n",
    "\n",
    "В случае (мульти-)коллинеарных признаков решение *нестабильно*. Давайте посмотрим на *число обусловленности* нашей матрицы:\n",
    "$$\\kappa(a) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}$$\n",
    "где $\\sigma _{\\max }(A)$ и $\\sigma _{\\min }(A)$ — максимальные и минимальные сингулярные значения матрицы $A$ соответственно. Следовательно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_by_grad(X, Y, num_steps, w_0, lr):\n",
    "    w = w_0.copy()\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        w -= 2 * lr * np.dot(X.T, np.dot(X, w) - Y) / Y.shape[0]\n",
    "    return w\n",
    "\n",
    "def get_w_by_stoch_grad(X, Y, num_steps, w_0, lr_0, n_objects):\n",
    "    w = w_0.copy()\n",
    "    lr_0 = 0.45\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        lr = lr_0 / ((i+1)**0.51)\n",
    "        sample = np.random.randint(n_objects, size=batch_size)\n",
    "        w -= 2 * lr * np.dot(X[sample].T, np.dot(X[sample], w) - Y[sample]) / Y.shape[0]\n",
    "    return w\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.linalg.norm(y_true-y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "sgd_lr = 0.1\n",
    "num_steps = 250\n",
    "noise_eps_seq = np.logspace(-2, -6, 20)\n",
    "\n",
    "w_0 = np.random.uniform(-2, 2, (n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_numbers = []\n",
    "vector_norms_list = []\n",
    "rmse_list = []\n",
    "results_list = []\n",
    "for eps in noise_eps_seq:\n",
    "    local_condition_numbers = []\n",
    "    local_vector_norms_list = []\n",
    "    local_rmse_list = []\n",
    "    for i in range(50):\n",
    "        X[:, -1] = 2 * (X[:, -2] + np.random.uniform(-eps, eps, X[:, -2].shape))\n",
    "\n",
    "        a = np.linalg.eigvals(X.T.dot(X))\n",
    "        local_condition_numbers.append(a.max() / a.min())\n",
    "\n",
    "        w_star = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n",
    "        w_star_grad = get_w_by_grad(X, Y, num_steps, w_0, lr)\n",
    "        w_star_sgd = get_w_by_stoch_grad(X, Y, num_steps, w_0, sgd_lr, n_objects)\n",
    "        local_vector_norms_list.append([\n",
    "            np.linalg.norm(w_star),\n",
    "            np.linalg.norm(w_star_grad), \n",
    "            np.linalg.norm(w_star_sgd),\n",
    "        ])\n",
    "\n",
    "        analytical_predict = X.dot(w_star)\n",
    "        grad_predict = X.dot(w_star_grad)\n",
    "        sgd_predict = X.dot(w_star_sgd)\n",
    "        \n",
    "        local_rmse_list.append([\n",
    "            rmse(Y, analytical_predict),\n",
    "            rmse(Y, grad_predict),\n",
    "            rmse(Y, sgd_predict),\n",
    "        ])\n",
    "        \n",
    "        results_list.append([w_star, w_star_grad, w_star_sgd])\n",
    "\n",
    "    condition_numbers.append([np.mean(local_condition_numbers), np.std(local_condition_numbers)])\n",
    "    vector_norms_list.append([\n",
    "        np.mean(np.array(local_vector_norms_list), axis=0),\n",
    "        np.std(np.array(local_vector_norms_list), axis=0),\n",
    "    ])\n",
    "    rmse_list.append(np.mean(np.array(local_rmse_list), axis=0))\n",
    "\n",
    "condition_numbers = np.array(condition_numbers)\n",
    "vector_norms_list = np.array(vector_norms_list)\n",
    "rmse_list = np.array(rmse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(mu, sigma, points, title=None, greater_than_zero=False, log_scale=False):\n",
    "    if greater_than_zero:\n",
    "        mu = np.clip(mu, 0, mu.max())\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    plt.plot(points, mu, \"r\", label=\"mean value\")\n",
    "    _x = np.concatenate((points, points[::-1]))\n",
    "    _y = np.concatenate((\n",
    "        [mu[i] - sigma[i] for i in range(len(points))], \n",
    "        [mu[i] + sigma[i] for i in range(len(points)-1, -1, -1)],\n",
    "    ))\n",
    "    if greater_than_zero:\n",
    "        _y = np.clip(_y, 0, _y.max())\n",
    "    plt.fill(_x, _y, fc='r', alpha=.2, ec=None, label='+- sigma range')\n",
    "    \n",
    "    if log_scale:\n",
    "        ax.set_xscale('log')\n",
    "        \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualise(\n",
    "    np.log(condition_numbers[:, 0]), \n",
    "    np.log(condition_numbers[:, 1]),\n",
    "    noise_eps_seq, \n",
    "    title='Condition number in log scale by noise level',\n",
    "    greater_than_zero=True,\n",
    "    log_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise(\n",
    "    np.log(vector_norms_list[:, 0, 0]), \n",
    "    np.log(vector_norms_list[:, 1, 0]),\n",
    "    noise_eps_seq, \n",
    "    title='Vector norm in log scale for analytical solution by noise level',\n",
    "    greater_than_zero=True,\n",
    "    log_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise(\n",
    "    vector_norms_list[:, 0, 1], \n",
    "    vector_norms_list[:, 1, 1],\n",
    "    noise_eps_seq, \n",
    "    title='Vector norm in original scale for gradient solution by noise level',\n",
    "    greater_than_zero=True,\n",
    "    log_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise(\n",
    "    vector_norms_list[:, 0, 2], \n",
    "    vector_norms_list[:, 1, 2],\n",
    "    noise_eps_seq, \n",
    "    title='Vector norm in original scale for sgd solution by noise level',\n",
    "    greater_than_zero=True,\n",
    "    log_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия \"из коробки\"\n",
    "\n",
    "Наконец, давайте кратко рассмотрим реализованные версии линейной регрессии из sklearn. Основные классы:\n",
    "\n",
    "* [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) — классическая линейная регрессия (*на самом деле, это просто `scipy.linalg.lstsq`, обернутая в класс sklearn `Predictor`) __аналитический__ решатель.\n",
    "* [Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) — линейная регрессия с регуляризацией L1.\n",
    "\n",
    "* [Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) — Линейная регрессия с регуляризацией L2.\n",
    "\n",
    "Чтобы минимизировать любую другую функцию ошибки, вы можете использовать [SGDRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)\n",
    "\n",
    "Давайте сравним скорость аналитических и градиентных решений из реализаций sklearn.\n",
    "\n",
    "Будет `%%time` из IPython.\n",
    "\n",
    "Для измерения качества будет использоваться оценка $R^2$. Он сравнивает нашу модель с моделью, всегда предсказывающей среднее $\\mathbf{y}$:\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_i (y_i - a(x_i))^2}{\\sum_i (y_i - \\overline{y}_i)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/mortal_combat.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 700\n",
    "n_objects = 100000\n",
    "num_steps = 150\n",
    "\n",
    "w_true = np.random.uniform(-2, 2, (n_features, 1))\n",
    "\n",
    "X = np.random.uniform(-100, 100, (n_objects, n_features))\n",
    "Y = X.dot(w_true) + np.random.normal(0, 10, (n_objects, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, Y)\n",
    "print(f'R2: {lr.score(X, Y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr = Ridge(alpha=0.9, solver='sparse_cg')\n",
    "lr.fit(X, Y)\n",
    "print(f'R2: {lr.score(X, Y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собственная версия линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте используем стандартные интерфейсы `sklearn` для реализации запечатанной версии нашей версии линейной регрессии с использованием SGD\n",
    "\n",
    "Сначала нам нужно унаследовать базовые классы, затем реализовать 3 основных этапа жизни регрессора как методы:\n",
    "- инициализация гиперпараметров - конструктор класса\n",
    "- обучение параметров на известных объектах - метод `fit`\n",
    "- оценка цели для неизвестных объектов - метод `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:48:24.750053Z",
     "start_time": "2020-02-19T18:48:24.746577Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:50:28.936505Z",
     "start_time": "2020-02-19T18:50:28.927717Z"
    }
   },
   "outputs": [],
   "source": [
    "class LinearRergessionSGD(BaseEstimator, RegressorMixin):\n",
    "    '''\n",
    "    LinearRergession with L2 regularization and SGD optimizer\n",
    "    '''\n",
    "    def __init__(self, C: float=1.0,\n",
    "                 batch_size: int=25,\n",
    "                 lr: float=1e-2,\n",
    "                 num_steps: int=200) -> None:\n",
    "        # self.C = C\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        w = np.random.randn(X.shape[1])[:, None]\n",
    "        n_objects = len(X)\n",
    "\n",
    "        for i in range(self.num_steps):\n",
    "            sample_indices = np.random.randint(n_objects, size=self.batch_size)\n",
    "            w -= 2 * self.lr * np.dot(X[sample_indices].T, \n",
    "                                      np.dot(X[sample_indices], w) \n",
    "                                      - Y[sample_indices]) / self.batch_size\n",
    "\n",
    "        self.w = w\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X@self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:50:42.390314Z",
     "start_time": "2020-02-19T18:50:40.904780Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 700\n",
    "n_objects = 100000\n",
    "num_steps = 150\n",
    "\n",
    "w_true = np.random.uniform(-2, 2, (n_features, 1))\n",
    "\n",
    "X = np.random.uniform(-100, 100, (n_objects, n_features)) * np.arange(n_features)\n",
    "Y = X.dot(w_true) + np.random.normal(0, 10, (n_objects, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:50:43.599348Z",
     "start_time": "2020-02-19T18:50:43.596610Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:55:10.577345Z",
     "start_time": "2020-02-19T18:55:10.180458Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:56:42.186020Z",
     "start_time": "2020-02-19T18:56:42.118206Z"
    }
   },
   "outputs": [],
   "source": [
    "own_lr = LinearRergessionSGD().fit(x_train, y_train)\n",
    "print(f'R2: {own_lr.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что-то пошло не так. Что это может быть?\n",
    "\n",
    "Во время нашего SGD мы столкнулись со слишком большими значениями для хранения в float.\n",
    "\n",
    "Это приводит нас к нормализации признаков - масштабируем признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:00:31.413133Z",
     "start_time": "2020-02-19T19:00:31.410170Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:03:03.598988Z",
     "start_time": "2020-02-19T19:03:01.932987Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# scaler = Normalizer()\n",
    "scaler.fit(x_train)\n",
    "x_scaled = scaler.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:03:22.671680Z",
     "start_time": "2020-02-19T19:03:22.643142Z"
    }
   },
   "outputs": [],
   "source": [
    "own_lr = LinearRergessionSGD().fit(x_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:04:02.064690Z",
     "start_time": "2020-02-19T19:04:01.902071Z"
    }
   },
   "outputs": [],
   "source": [
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:04:09.395759Z",
     "start_time": "2020-02-19T19:04:09.383991Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'R2: {own_lr.score(x_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы помните мы не реализовали метод `score`. Но базовый класс `sklearn` предоставляет нам его уже реализованным (и некоторые другие)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
