{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 7: Обработка естественного языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## История развития"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Последовательность** — это дискретный набор элементов из конечного множества. Каждый элемент представляется вектором, между элементами задано отношение порядка.\n",
    "\n",
    "### Цель\n",
    "- Научиться обрабатывать последовательности переменной длины.\n",
    "\n",
    "### Марковское свойство\n",
    "Если задана последовательность из $ t $ элементов $ x $, Марковское свойство наблюдается, если для предсказания следующего состояния достаточно конечного множества предыдущих элементов.\n",
    "\n",
    "На текстах Марковское свойство, как правило, не выполняется из-за сложной структуры языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "text = (\n",
    "    \"Тайлер находит мне работу официанта, потом он же сует мне в рот пистолет и \" \n",
    "    \"говорит: «Первый шаг к бессмертию — это смерть». Хотя долгое время мы с \"\n",
    "    \"Тайлером были лучшими друзьями. Меня всегда спрашивают — знаю ли я Тайлера \"\n",
    "    \"Дердена. Ствол пушки уперся мне в глотку, Тайлер говорит: — На самом деле мы не умрем. \"\n",
    "    \"Языком я чувствую дырочки глушителя, которые мы насверлили в стволе пистолета. \"\n",
    "    \"Шум от выстрела почти полностью возникает из-за расширения газов, плюс легкий \"\n",
    "    \"звуковой хлопок от пули — из-за ее скорости. Чтобы сделать глушитель, \"\n",
    "    \"нужно просто насверлить дырочек в стволе пушки, много дырочек. \"\n",
    "    \"Тогда газ выйдет через них, и скорость пули упадет ниже сверхзвуковой.\"\n",
    ")\n",
    "\n",
    "words = re.findall(r\"[\\w']+\", text.lower())\n",
    "\n",
    "vocab = sorted(set(words))\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "context_size = 2\n",
    "data = []\n",
    "for i in range(len(words) - context_size):\n",
    "    context = words[i:i + context_size]\n",
    "    target = words[i + context_size]\n",
    "    data.append((context, target))\n",
    "\n",
    "input_seqs = []\n",
    "target_seqs = []\n",
    "for context, target in data:\n",
    "    input_seqs.append([word2idx[w] for w in context])\n",
    "    target_seqs.append(word2idx[target])\n",
    "\n",
    "input_tensor = torch.LongTensor(input_seqs) # (num_samples, context_size)\n",
    "target_tensor = torch.LongTensor(target_seqs) # (num_samples)\n",
    "\n",
    "def predict_next_words(model, context, word2idx, idx2word, top_k=5):\n",
    "    model.eval()\n",
    "    context_idx = torch.LongTensor([[word2idx.get(w, 0) for w in context]])\n",
    "    with torch.no_grad():\n",
    "        logits = model(context_idx)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, top_k, dim=-1)\n",
    "        topk_words = [idx2word[idx.item()] for idx in topk_indices[0]]\n",
    "    return topk_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рекуррентный блок\n",
    "- **Пошаговая обработка:**\n",
    "  $$\n",
    "  h_{t+1} = f_0(h_t, x_t),\n",
    "  $$\n",
    "  где $ h_t $ — скрытое состояние на шаге $ t $, $ x_t $ — вход на шаге $ t $.\n",
    "\n",
    "- **Применение:**\n",
    "  - Используется для генерации последовательностей, предсказывая следующий токен, аналогично задаче классификации.\n",
    "  - Конкатенируем $ h_t $ и $ x_t $, применяем линейный слой с нелинейной активацией для отображения в нужную размерность:\n",
    "    $$\n",
    "    h_{t+1} = \\sigma(W[h_t, x_t] + b),\n",
    "    $$\n",
    "    где $ \\sigma $ — функция активации (например, ReLU, sigmoid).\n",
    "\n",
    "- **Передача скрытого состояния:**\n",
    "  При необходимости скрытое состояние $ h_t $ передается в классификатор для предсказания следующего элемента (токена).\n",
    "\n",
    "### Проблемы с градиентами\n",
    "- **Взрывающийся градиент:**\n",
    "  - Градиент увеличивается экспоненциально на длинных последовательностях.\n",
    "  - Решение: нормировать градиент на фиксированное значение (gradient clipping).\n",
    "\n",
    "- **Затухающий градиент:**\n",
    "  - Градиент уменьшается с ростом длины последовательности, обратная связь отдаленных элементов ослабевает.\n",
    "  - Решение: использовать функции активации с насыщением (sigmoid, tanh, т.е ограниченные сверху и снизу) и нормализацию (например, Layer Norm).\n",
    "\n",
    "- **Функция активации ReLU:**\n",
    "  - Может привести к неограниченному сверху значению градиентов.\n",
    "  - Частичное решение: использовать нормализацию, например Layer Norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Weijiang-Feng/publication/318332317/figure/fig1/AS:614309562437664@1523474221928/The-standard-RNN-and-unfolded-RNN.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x) # (batch, seq_len, embedding_dim)\n",
    "        output, _ = self.rnn(embedded) # output: (batch, seq_len, hidden_dim)\n",
    "\n",
    "        last_output = output[:, -1, :] # (batch, hidden_dim)\n",
    "        logits = self.fc(last_output) # (batch, vocab_size)\n",
    "        return logits\n",
    "\n",
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "\n",
    "model = RNN(vocab_size, embedding_dim, hidden_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_tensor)\n",
    "    loss = criterion(logits, target_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "test_context = [\"тайлера\"]\n",
    "predicted_word = predict_next_words(model, test_context, word2idx, idx2word, top_k=5)\n",
    "print(f\"Context: {test_context} -> Predicted next word: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переход от RNN к LSTM\n",
    "- **Разделение информации на два потока:**\n",
    "  - Один поток используется для сложения и умножения без функций активации («долгосрочная память»).\n",
    "\n",
    "- **Механизм забывания:**\n",
    "  - Используется бинарная классификация для оценки важности каждой компоненты вектора.\n",
    "  - Вычисленная вероятность важности умножается на текущий вектор (механизм забывания).\n",
    "\n",
    "### LSTM (Long Short-Term Memory)\n",
    "- **Компоненты:**\n",
    "  1. **Входной элемент:** $ x_t $.\n",
    "  2. **Скрытое состояние:** $ h_t $.\n",
    "  3. **Ячейка памяти:** $ c_t $.\n",
    "\n",
    "- **Основные этапы:**\n",
    "  1. **Поток забывания:**\n",
    "     $$\n",
    "     f_t = \\sigma(W_f[h_{t-1}, x_t] + b_f),\n",
    "     $$\n",
    "     где $ f_t $ определяет, какая часть старой информации будет забыта.\n",
    "\n",
    "  2. **Поток обновления:**\n",
    "     $$\n",
    "     i_t = \\sigma(W_i[h_{t-1}, x_t] + b_i),\n",
    "     $$\n",
    "     $$\n",
    "     \\tilde{c}_t = \\tanh(W_c[h_{t-1}, x_t] + b_c),\n",
    "     $$\n",
    "     где $ i_t $ определяет важность нового кандидата $ \\tilde{c}_t $.\n",
    "\n",
    "  3. **Обновление ячейки памяти:**\n",
    "     $$\n",
    "     c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t.\n",
    "     $$\n",
    "\n",
    "  4. **Поток выхода:**\n",
    "     $$\n",
    "     o_t = \\sigma(W_o[h_{t-1}, x_t] + b_o),\n",
    "     $$\n",
    "     $$\n",
    "     h_t = o_t \\odot \\tanh(c_t).\n",
    "     $$\n",
    "\n",
    "- **Особенности:**\n",
    "  - Ячейка памяти $ c_t $ обеспечивает долгосрочную память.\n",
    "  - Гейты позволяют эффективно управлять потоками информации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlarchive.com/wp-content/uploads/2024/04/New-Project-3-1.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTM: сохраняет как скрытое состояние (h_t), так и ячейку памяти (c_t)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, \n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x) # (batch, seq_len, embedding_dim)\n",
    "        # LSTM возвращает:\n",
    "        #   output: все скрытые состояния для каждого временного шага,\n",
    "        #   (h_n, c_n): скрытое состояние и ячейку памяти для последнего временного шага.\n",
    "        output, (h_n, c_n) = self.lstm(embedded)\n",
    "        # Используем выход последнего временного шага для предсказания следующего слова\n",
    "        last_output = output[:, -1, :] # (batch, hidden_dim)\n",
    "        logits = self.fc(last_output) # (batch, vocab_size)\n",
    "        return logits\n",
    "\n",
    "embedding_dim = 20\n",
    "hidden_dim = 30\n",
    "\n",
    "model = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_tensor) # (num_samples, vocab_size)\n",
    "    loss = criterion(logits, target_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "test_context = [\"тайлера\"]\n",
    "predicted_word = predict_next_words(model, test_context, word2idx, idx2word, top_k=5)\n",
    "print(f\"Context: {test_context} -> Predicted next word: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU (Gated Recurrent Unit)\n",
    "- **Отличия от LSTM:**\n",
    "  - Упрощенная структура: нет отдельной ячейки памяти $ c_t $.\n",
    "  - Объединение гейтов забывания и обновления в один гейт:\n",
    "    $$\n",
    "    z_t = \\sigma(W_z[h_{t-1}, x_t] + b_z).\n",
    "    $$\n",
    "  - Обновление состояния:\n",
    "    $$\n",
    "    h_t = z_t \\odot h_{t-1} + (1 - z_t) \\odot \\tilde{h}_t,\n",
    "    $$\n",
    "    где $ \\tilde{h}_t $ — новый кандидат состояния.\n",
    "\n",
    "- **Преимущества:**\n",
    "  - Меньше параметров, чем в LSTM.\n",
    "  - Более быстрые вычисления."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Saddam-Abdulwahab-2/publication/328462205/figure/fig4/AS:684914898923521@1540307845043/Gated-Recurrent-Unit-GRU.ppm\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super(GRULanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, \n",
    "                          num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x) # (batch, seq_len, embedding_dim)\n",
    "        output, h_n = self.gru(embedded) # output: (batch, seq_len, hidden_dim)\n",
    "\n",
    "        last_output = output[:, -1, :] # (batch, hidden_dim)\n",
    "        logits = self.fc(last_output) # (batch, vocab_size)\n",
    "        return logits\n",
    "\n",
    "embedding_dim = 20\n",
    "hidden_dim = 30\n",
    "\n",
    "model = GRULanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_tensor) # (num_samples, vocab_size)\n",
    "    loss = criterion(logits, target_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "test_context = [\"тайлера\"]\n",
    "predicted_word = predict_next_words(model, test_context, word2idx, idx2word, top_k=5)\n",
    "print(f\"Context: {test_context} -> Predicted next word: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.licdn.com/dms/image/v2/D4D12AQG--7uzouFkvg/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1661014662428?e=2147483647&v=beta&t=MkJaE0aeAH52nkK-sf1XwoTPlvVs750gGT4TjTLtW70\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Современные модификации: Mamba\n",
    "- **Описание:**\n",
    "  - Новая архитектура, оптимизирующая задачи генерации и классификации последовательностей.\n",
    "  - Упрощенная структура гейтов для повышения вычислительной эффективности.\n",
    "\n",
    "- **Особенности:**\n",
    "  - Использование адаптивных механизмов нормализации.\n",
    "  - Улучшенная способность моделировать долгосрочные зависимости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Токен** — атомарный элемент последовательности. Токеном может быть:\n",
    "- Слово,\n",
    "- Символ,\n",
    "- Морфема.\n",
    "\n",
    "### Требования к векторным представлениям текста\n",
    "Векторное представление текста должно:\n",
    "1. Сохранять информацию о порядке слов.\n",
    "2. Быть низкой размерности для уменьшения вычислительных затрат.\n",
    "3. Быть не слишком разряженным.\n",
    "4. Учитывать различные формы одного слова как идентичные (например, «кот» и «коты»).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/publication/343595281/figure/fig4/AS:963538206089244@1606736818940/Visualization-of-the-word-embedding-space.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TensorFlow Projector](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основные методы представления текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud nltk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def visualize_document_vectors(matrix, documents, title):\n",
    "    \"\"\"\n",
    "    Снижает размерность матрицы векторов документов до 3D с помощью PCA\n",
    "    и визуализирует результаты в виде 3D scatter plot.\n",
    "\n",
    "    Аргументы:\n",
    "        matrix : разреженная матрица размерности (n_documents, n_features)\n",
    "        documents : список документов (строк) для аннотации точек\n",
    "        title : заголовок графика\n",
    "    \"\"\"\n",
    "    dense_matrix = matrix.toarray()\n",
    "    pca = PCA(n_components=3)\n",
    "    vectors_3d = pca.fit_transform(dense_matrix)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(vectors_3d[:, 0], vectors_3d[:, 1], vectors_3d[:, 2], s=50, c='blue')\n",
    "\n",
    "    # Добавляем аннотации: номер документа\n",
    "    for idx, (x, y, z) in enumerate(vectors_3d):\n",
    "        ax.text(x, y, z, f\"Doc {idx+1}\", fontsize=12)\n",
    "\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel(\"PCA компонент 1\", fontsize=14)\n",
    "    ax.set_ylabel(\"PCA компонент 2\", fontsize=14)\n",
    "    ax.set_zlabel(\"PCA компонент 3\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "documents = [\"Москва – столица России, один из самых больших городов в мире.\",\n",
    "            \"В Санкт-Петербурге много красивых зданий, и город знаменит своими музеями.\",\n",
    "            \"Сочи известен своими курортами и горнолыжными курортами на Красной Поляне.\",\n",
    "            \"Новосибирск – третий по численности населения город России.\",\n",
    "            \"Екатеринбург является одним из самых важных промышленных центров страны.\"]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "russian_stop_words = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words (BoW)\n",
    "- Простой метод, представляющий текст в виде вектора частотности слов.\n",
    "- **Механизм:**\n",
    "  1. Каждое слово из словаря получает свою позицию в векторе.\n",
    "  2. Для каждого текста создается вектор, в котором подсчитывается количество упоминаний каждого слова.\n",
    "- **Недостатки:**\n",
    "  - Игнорирует порядок слов.\n",
    "  - Разряженность векторов при большом словаре.\n",
    "  - Не учитывает семантическую близость слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer(stop_words=russian_stop_words)\n",
    "bow_matrix = bow_vectorizer.fit_transform(documents)\n",
    "visualize_document_vectors(bow_matrix, documents, \"Векторное представление (Bag of Words)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words=russian_stop_words)\n",
    "bigram_matrix = bigram_vectorizer.fit_transform(documents)\n",
    "visualize_document_vectors(bigram_matrix, documents, \"Векторное представление (Биграммы)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordNet\n",
    "- Лексическая база данных, представляющая слова и их отношения в виде графа.\n",
    "- **Особенности:**\n",
    "  - Узлы графа — слова, а связи — их семантические отношения (синонимы, антонимы, гипонимы и т.д.).\n",
    "  - Создан вручную, что делает процесс трудоемким.\n",
    "- **Недостатки:** Ограниченная масштабируемость и адаптивность к новым данным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://devopedia.org/images/article/103/3039.1596456731.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "- **Описание:** Метод для взвешивания значимости слов в документе.\n",
    "- **Формула:**\n",
    "  $$\n",
    "  TF\\text{-}IDF(w, d, D) = TF(w, d) \\times IDF(w, D)\n",
    "  $$\n",
    "  где:\n",
    "  - $ TF(w, d) = \\frac{\\text{Частота слова } w \\text{ в документе } d}{\\text{Общее количество слов в документе } d} $\n",
    "  - $ IDF(w, D) = \\log\\frac{|D|}{1 + |\\{d \\in D : w \\in d\\}|} $\n",
    "\n",
    "- **Особенности:**\n",
    "  - Уменьшает значимость часто встречающихся слов (например, «и», «в»).\n",
    "  - Учитывает редкие, информативные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=russian_stop_words)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "visualize_document_vectors(tfidf_matrix, documents, \"Векторное представление (TF-IDF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Информативные векторные представления - эмбеддинги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Методы создания эмбеддингов\n",
    "\n",
    "1. **Разложение матриц:**\n",
    "   - Создается матрица совместной частоты слов.\n",
    "   - С помощью методов, таких как сингулярное разложение (SVD), матрица преобразуется в более низкоранговые представления.\n",
    "\n",
    "2. **Word2Vec:**\n",
    "   - **Описание:** Метод построения векторных представлений слов с использованием нейронных сетей.\n",
    "   - **Используемые термины:**\n",
    "     - **N-граммы:** Последовательности из $ N $ токенов (слов или символов). Позволяет учитывать локальный контекст.\n",
    "     - **Skip-граммы:** Нечетные последовательности токенов с пропуском центрального элемента.\n",
    "   - **Принципы:**\n",
    "     - Будем оценивать для каждого слова вероятность другого слова быть с ним в одном контексте.\n",
    "     - Балансируем объекты так, чтобы часто используемые слова меньше использовались для обучения.\n",
    "     - Для обучения используем negative sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "tokenized_docs = [simple_preprocess(doc, deacc=True) for doc in documents]\n",
    "model = Word2Vec(sentences=tokenized_docs, vector_size=50, window=3, min_count=1, workers=1, seed=42)\n",
    "word_vectors = model.wv\n",
    "\n",
    "words = list(word_vectors.key_to_index.keys())\n",
    "\n",
    "vectors = [word_vectors[word] for word in words]\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "vectors_3d = pca.fit_transform(vectors)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(vectors_3d[:, 0], vectors_3d[:, 1], vectors_3d[:, 2], s=100)\n",
    "\n",
    "# Добавляем подписи для каждого слова\n",
    "for i, word in enumerate(words):\n",
    "    ax.text(vectors_3d[i, 0], vectors_3d[i, 1], vectors_3d[i, 2],\n",
    "            word, fontsize=12, ha=\"center\", va=\"center\")\n",
    "\n",
    "ax.set_xlabel(\"PCA компонент 1\", fontsize=14)\n",
    "ax.set_ylabel(\"PCA компонент 2\", fontsize=14)\n",
    "ax.set_zlabel(\"PCA компонент 3\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Контрастное обучение (Contrastive Learning)\n",
    "- **Описание:** Метод, сближающий похожие (контекстно близкие) слова и отдаляющий непохожие.\n",
    "- **Механизм Negative Sampling:**\n",
    "  - Часто встречающиеся слова с большей вероятностью становятся «негативными примерами».\n",
    "  - Пример: для пары слов (контекстное и целевое) подбираются случайные слова, которые не должны быть близкими по контексту.\n",
    "- **Цель:** Преобразовать многоклассовую классификацию в двуклассовую задачу: «связанное слово или нет»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Авторегрессионные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основы авторегрессионных моделей\n",
    "- Последовательно обрабатывают элементы последовательности, формируя скрытый латентный вектор, кодирующий всю прочитанную последовательность.\n",
    "- На каждом шаге:\n",
    "  1. Формируется распределение вероятностей следующего токена.\n",
    "  2. Выбирается токен с максимальной вероятностью (argmax).\n",
    "  3. Этот токен подается снова на вход модели.\n",
    "\n",
    "- **Правдоподобие сгенерированной последовательности:**\n",
    "  $$\n",
    "  P(x_1, x_2, \\dots, x_T) = \\prod_{t=1}^T P(x_t | x_1, x_2, \\dots, x_{t-1}).\n",
    "  $$\n",
    "\n",
    "### Проблемы авторегрессии\n",
    "- Предсказания основываются на собственных предыдущих предсказаниях.\n",
    "- Ошибки критичны, так как накапливаются, приводя к деградации качества сгенерированной последовательности.\n",
    "\n",
    "### Лучевой поиск (Beam Search)\n",
    "- Метод для улучшения генерации последовательностей.\n",
    "- На каждом этапе генерируется $ k $ (ширина луча) продолжений для каждого из текущих кандидатов.\n",
    "- Продолжаются только $ k $ лучших последовательностей по правдоподобию.\n",
    "\n",
    "- **Проблема коротких последовательностей:**\n",
    "  - Короткие последовательности имеют более высокое правдоподобие.\n",
    "  - Решение: нормировка оценки правдоподобия на длину последовательности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Механизм внимания (Attention)\n",
    "- Используется для обработки последовательностей разной длины.\n",
    "- **Идея:** на каждом шаге вычисляется взвешенная сумма предыдущих состояний на основе их значимости для текущего состояния.\n",
    "\n",
    "1. Для текущего состояния вычисляется скалярное произведение с каждым из предыдущих состояний.\n",
    "2. Пропускается через softmax для получения весов.\n",
    "3. Предыдущие состояния суммируются с учетом этих весов, так находится следующее состояние.\n",
    "\n",
    "- **Применение:** позволяет учитывать значимость всех предыдущих элементов последовательности для текущего состояния.\n",
    "\n",
    "### Self-Attention\n",
    "- **Особенности:**\n",
    "  - Квадратичная сложность относительно длины последовательности.\n",
    "  - Оценивает отношение между элементами последовательности (например, словами в тексте).\n",
    "  - Идейно изменяет скалярное произведение, поскольку слово один к слову два относится не также как второе к первому,нужно учитывать порядок слов.\n",
    "\n",
    "- **Основные компоненты:**\n",
    "  - **Queries ($ Q $):** отображение эмбеддинга в пространство \"откуда\" (из этого токена \"смотрим\").\n",
    "  - **Keys ($ K $):** отображение эмбеддинга в пространство \"куда\" (на какой токен \"смотрит\").\n",
    "  - **Values ($ V $):** смысловая информация токена (можно использовать сам эмбеддинг).\n",
    "\n",
    "- **Алгоритм:**\n",
    "  1. Умножаем эмбеддинги токенов на обучаемые матрицы $ W_Q $, $ W_K $, $ W_V $:\n",
    "     $$\n",
    "     Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V.\n",
    "     $$\n",
    "  2. Вычисляем внимание между токенами $ i $ и $ j $:\n",
    "     $$\n",
    "     \\text{Attention}(i, j) = \\text{softmax}\\left(\\frac{Q_i K_j^T}{\\sqrt{d}}\\right),\n",
    "     $$\n",
    "     где $ d $ — размерность эмбеддинга (для уменьшения дисперсии).\n",
    "  3. Умножаем значения внимания на $ V $ и складываем для получения нового представления токенов.\n",
    "\n",
    "- **Многоголовое внимание (Multi-Head Attention):**\n",
    "  - Независимо обучаются несколько вариантов матриц $ W_Q, W_K, W_V $.\n",
    "  - Каждая головка создает новое представление токенов.\n",
    "  - Итоговые представления объединяются и умножаются на матрицу $ W_O $:\n",
    "    $$\n",
    "    \\text{Output} = \\text{Concat}(\\text{Head}_1, \\dots, \\text{Head}_h) W_O.\n",
    "    $$\n",
    "\n",
    "- **Преимущества:**\n",
    "  - Учет различных типов взаимосвязей между токенами.\n",
    "  - Распространяется на последовательности любого типа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подробные визуализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Источник](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://jalammar.github.io/images/t/Transformer_decoder.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://jalammar.github.io/images/t/embeddings.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://jalammar.github.io/images/t/encoder_with_tensors.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://jalammar.github.io/images/t/transformer_self_attention_vectors.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://jalammar.github.io/images/t/self-attention-output.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://jalammar.github.io/images/t/self-attention-matrix-calculation.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/image4.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TensorFlow Tensor2Tensor](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура Encoder-Decoder\n",
    "\n",
    "- Архитектура состоит из энкодера, который обрабатывает входные данные, и декодера, который генерирует выходные данные.\n",
    "- Используется в задачах перевода, обобщения текста, генерации описаний и других."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers):** энкодерная архитектура, использующая Transformer.\n",
    "- Направлен на глубокое понимание текста.\n",
    "\n",
    "**Основные задачи при обучении:**\n",
    "1. **Masked Language Modeling (MLM):**\n",
    "   - Часть токенов маскируется специальным символом $[MASK]$, и модель обучается их предсказывать.\n",
    "   - Формула вероятности предсказания:\n",
    "     $$\n",
    "     P(x_{\\text{masked}} | x_{\\text{context}}) = \\prod_{i \\in \\text{masked}} P(x_i | x_{\\text{context}}).\n",
    "     $$\n",
    "2. **Next Sentence Prediction (NSP):**\n",
    "   - Задача бинарной классификации, определяющая, являются ли два предложения последовательными.\n",
    "\n",
    "**Особенности:**\n",
    "- Токены представлены как подстроки из часто встречающихся групп символов, для оптимизации (BPE, byte-pair encoding).\n",
    "- Порядок слов кодируется с помощью позиционных эмбеддингов, например, можно раскладывать по гармоникам синусов и косинусов (один из вариантов):\n",
    "  $$\n",
    "  PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d}), \\quad PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d}),\n",
    "  $$\n",
    "  где $ pos $ — позиция слова, $ i $ — индекс вектора эмбеддинга, $ d $ — размерность эмбеддинга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://sushant-kumar.com/blog/bert-architecture.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, logging as hf_logging\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"DeepPavlov/rubert-base-cased\")\n",
    "text = \"Столица России - это [MASK].\"\n",
    "results = fill_mask(text)\n",
    "\n",
    "print(\"Результаты заполнения маски:\")\n",
    "for res in results:\n",
    "    token_str = res.get(\"token_str\", \"\").strip()\n",
    "    score = res.get(\"score\", 0)\n",
    "    print(f\" - {token_str} (score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT\n",
    "- **GPT (Generative Pre-trained Transformer):** декодерная архитектура.\n",
    "- Основная задача: генерация текста, предсказание следующего токена.\n",
    "\n",
    "**Режимы работы:**\n",
    "1. **Zero-shot:** выполнение задач без дополнительного обучения.\n",
    "2. **Few-shot:** выполнение задач после \"обучения\" в промпте на нескольких примерах.\n",
    "\n",
    "**Особенности:**\n",
    "- Подходит для генерации креативных текстов.\n",
    "- Используется для диалоговых систем, креативного письма и других задач, требующих генерации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20240712150234/GPT-Arcihtecture.webp\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=50256)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "prompt = \"Дай определение предела по Коши\"\n",
    "results = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(\"Сгенерированный текст:\")\n",
    "for res in results:\n",
    "    print(res[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5\n",
    "- **T5 (Text-to-Text Transfer Transformer):** архитектура Encoder-Decoder.\n",
    "- Универсальная модель, преобразующая любую задачу обработки текста в формат \"входной текст → выходной текст\".\n",
    "\n",
    "**Особенности:**\n",
    "- Использует Masked Language Modeling для нескольких токенов сразу.\n",
    "- Применяется для перевода, обобщения, исправления грамматических ошибок и других задач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/publication/371619795/figure/fig2/AS:11431281168463909@1686969039930/Architecture-of-the-T5-model.ppm\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"sberbank-ai/ruT5-base\")\n",
    "\n",
    "text = (\n",
    "    \"Башня Лондона — исторический замок, расположенный на северном берегу реки Темзы в центре Лондона. \"\n",
    "    \"Был построен в 1066 году в период нормандского завоевания Англии. \"\n",
    "    \"Белая Башня, давшая название всему комплексу, была возведена Вильгельмом Завоевателем в 1078 году и стала символом власти и контроля. \"\n",
    "    \"На протяжении веков замок служил и королевским дворцом, и тюрьмой. \"\n",
    "    \"Сегодня он является одной из главных достопримечательностей Лондона.\"\n",
    ")\n",
    "\n",
    "t5_input = \"сократи: \" + text\n",
    "\n",
    "summary = summarizer(t5_input, max_length=50, min_length=25, do_sample=False)\n",
    "\n",
    "print(\"Результат суммаризации:\")\n",
    "print(summary[0][\"summary_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
