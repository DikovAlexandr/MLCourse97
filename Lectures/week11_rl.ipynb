{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71dd680d",
   "metadata": {},
   "source": [
    "# Лекция: Обучение с подкреплением"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779b8ec",
   "metadata": {},
   "source": [
    "Будем говорить про Reinforcement Learning (RL) больше в контексте LLM, поскольку для них есть целый этап обучения связанный с этим подходом, называемый Alignment.\n",
    "\n",
    "Начнем с проблематики, а именно с задачи суммаризации текста. Если для базовых задач мы легко вводили метрики качество, то тут ситуация иная и как оценить качество суммаризации не понятно. Конечно, есть метрика ROUGE (Recall-Oriented Understudy for Gisting Evaluation) - она считается как число слов из целевого ответа входящих в ответ модели (ROUGE-1 для униграм ROUGE-2 для биграм). Однако, как было выявлено экспериментально, эта метрика не выражает действительного качеств суммаризации и имеет асимптоту даже для самых совершенных алгоритмов.\n",
    "\n",
    "Поговорим о классической дилемме из ML - мы обучаем модель (то есть используем в качестве функции потерь) не тоже самое, на чем далее проверяем качество. Победить эту дилемму и оптимизировать именно то, что нужно как раз позволяют методы RL\n",
    "\n",
    "Для моделей в парадигме RL вводят награду, будем использовать оригинальный термин - Reward. OpenAI учили модель играть в Dota 2 записывали различные таблицы - но такой метод бы плохо масштабировался. Важно Reward функции проектировать аккуратно, иначе после оптимизации модель будет делать не то, что нужно, а \"взломает\" наш reward\n",
    "\n",
    "Сделать действительно хороший reward сложно, поэтому можно использовать человека для этого и, чтобы упростить ему жизнь, можно давать два варианта и выбора меньше ними.\n",
    "\n",
    "- Pretrain - просто на больших объемах данных\n",
    "- Finetune (SFT) - используются датасеты инструкции (демонстрации решения задач)\n",
    "- RLHF - обучаем Reward модель давать скалярную оценку на данных от разметки пользователей, затем оптимизируем генерацию чтобы максимизировать оценку генеративной модели\n",
    "\n",
    "Схема и описание этапов обучения модели [ChatGPT](https://openai.com/index/chatgpt/).\n",
    "\n",
    "Поговорим именно про последний этап и для этого введем основные термины.\n",
    "\n",
    "Policy Model - большая генеративная модель (например, после SFT), которую мы в итоге хотим обучить.\n",
    "\n",
    "Reward Model - относительно небольшая модель, которую мы обучаем на парах сравнения, чтобы она предсказывала скалярное reward.\n",
    "\n",
    "Reward модель - должна научиться понимать предпочтения людей в виде скаляра reward (или score), что само по себе очень интересная задача. Прочисть что было придумано в этой области можно в обзорной [статье](https://arxiv.org/abs/2504.12328)\n",
    "\n",
    "Пусть у нас есть два ответа на вопросы - победивший и проигравший. Подадим на вход некоторой модели (можно и породившей эти ответы, но чаще просто трансформер меньшего размера, близкого по архитектуре к policy модели), каждый из них вместе с первоначальным вопросом. Для победившего $ s_win = r_\\theta(x, y_win) $ и для проигравшего \n",
    "$ s_lose = r_\\theta(x, y_lose) $. Модель как обычно предскажет вектор, но на этот раз найдем его проекцию не на словарь, а на скаляр - так получаем reward. Затем применим pairwise ranking loss который использует не абсолютные величины, а их относительные значения (у победившего reward больше, чем у проигравшего) $ -log(\\sigma(s_win - s_lose)) $\n",
    "\n",
    "Поскольку награда в RL выдается за действие, а действие переводит в новое состояние, то это хорошо соотносится с авторегрессионной природой LLM\n",
    "\n",
    "Введем такое понятие как value функция (модель критик) - она указывает сколько reward'а мы можем заработать\n",
    "\n",
    "Считаем advantage это разница в value предсказанных value-функцией и фактически полученным reward между двумя соседними состояниями. Это мера того, насколько конкретное действие (токен) оказалось лучше или хуже, чем \"среднее\" по политике в данном состоянии.\n",
    "\n",
    "Пусть есть 10к-100к промптов, для каждого из них:\n",
    "- Генерируем ответ текущей моделью\n",
    "- Оценивать его Reward моделью\n",
    "- Оценивать его моделью критиком, то есть предсказывать value значения\n",
    "- Дообучаться для улучшения общего reward и отдельных advantage\n",
    "\n",
    "Промптов модно взять и меньше, можно и сотни тысячи, они могут быть сгенерированы другой моделью и т.д.\n",
    "\n",
    "Reward модель при этом дистиллирует понимание прекрасного людей в отношении ответов именно на эти промпты.\n",
    "\n",
    "Reward модель однако все еще может закреплять не очень хорошие практики ответов модели.\n",
    "\n",
    "Есть набор промптов, подаем их в исходную модель (например после SFT) и в нашу текущую модель, которую мы обучаем.\n",
    "\n",
    "Обе модели делаю генерации и для каждого токена замеряем распределение вероятностей и затем считаем дивергенцию Кульбака-Лейблера $ L_{KL} $ между двумя распределениями вероятностей - добавляем это как некоторый регуляризационный член в оптимизируемую функцию. Это не даст очень сильно угодить от генерации правдоподобного текста, который хорошо умеет делать SFT модель.\n",
    "\n",
    "$ L_{PPO} = L_{RL} - \\beta \\cdot L_{KL} + \\gamma \\cdot L_{Value} $\n",
    "\n",
    "$ L_{RL} $ - ожидаемый reward от новой политики, считается через advantage, который показывает, насколько действие (токен) в данном контексте было лучше среднего.\n",
    "\n",
    "$ L_{RL} = \\hat{\\mathbb{E}}t [ \\frac{\\pi\\theta(y_t | x, y_{<t})}{\\pi_{\\text{old}}(y_t | x, y_{<t})} \\cdot A_t ] $\n",
    "\n",
    "$ L_{Value} $ - ошибка предсказания value-функции.\n",
    "\n",
    "Весь RLHF этап по сути не меняет знаний модель, не добавляет новых, а лишь позволяет разблокировать возможность пользоваться уже имеющимися знаниями для решения конкретных задач - то есть происходит выравнивание с ожиданиями человека. Проведение этого [этапа](https://arxiv.org/abs/2203.02155) на промптах исключительно на английском языке учит модель не английскому, а следованию инструкциям, причем и на других языках.\n",
    "\n",
    "Это был метод [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) (PPO) - это on-policy метод, он должен стыкаваться с текущим графом знаний модели - поэтому опастно делать RLHF открытой модели использую датасет сгенерированный какой-либо проприетарной (разные графы знаний - модель ухудшится)\n",
    "\n",
    "Есть библиотека [TLR](https://huggingface.co/docs/trl/main/en/index) - Transformer Reinforcement Learning - в ней предоставляются методы SFT, GRPO, DPO, построения Reward Model и не только. Конкретный пример использования [GRPO](https://huggingface.co/learn/cookbook/en/fine_tuning_llm_grpo_trl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6116af2a",
   "metadata": {},
   "source": [
    "Существует также [Direct Preference Optimization](https://arxiv.org/abs/2305.18290) или DPO, который позволяет напрямую оптимизировать модель под предпочтения пользователей, без обучения отдельной reward-модели.\n",
    "\n",
    "Функция потерь DPO выглядит следующим образом:\n",
    "\n",
    "$L_{DPO} = -\\hat{\\mathbb{E}} [ \\log \\sigma ( \\beta \\log \\frac{\\pi_\\theta(y_{win} | x)}{\\pi_{\\text{ref}}(y_{win} | x)} - \\beta \\log \\frac{\\pi_\\theta(y_{lose} | x)}{\\pi_{\\text{ref}}(y_{lose} | x)} ) ]$\n",
    "\n",
    "в этой формуле:\n",
    "\n",
    "$\\pi_\\theta(y | x)$ - вероятность генерации ответа $y$ на промпт $x$ текущей обучаемой моделью.\n",
    "\n",
    "$\\pi_{\\text{ref}}(y | x)$ - вероятность генерации ответа $y$ на промпт $x$ исходной эталонной моделью (например, после SFT) и она заморожена.\n",
    "\n",
    "$\\log \\frac{\\pi_\\theta(y | x)}{\\pi_{\\text{ref}}(y | x)}$ - это и есть неявная reward, она показывает, насколько текущая модель \"предпочитает\" этот ответ по сравнению с эталонной.\n",
    "\n",
    "$\\beta$ — параметр, регулирующий силу отклонения от эталонной модели (аналогичен параметру $\\beta$ для KL-штрафа в PPO).\n",
    "\n",
    "Вся конструкция внутри $ \\log \\sigma $ - это разница в reward между выигравшим и проигравшим ответами. Мы просто максимизируем вероятность того, что выигравший ответ имеет большую неявную reward, чем проигравший.\n",
    "\n",
    "Пример в библиотеке [TLR](https://huggingface.co/docs/trl/main/en/dpo_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719bb03",
   "metadata": {},
   "source": [
    "GRPO - это еще один вариант on-policy оптимизации для LLM. Главное отличие от классического PPO тут в том, что нет отдельной value-функции, вместо нее в GRPO генерируется для каждого промпта группа $K$ кандидатов (то есть не только победитель и проигравший, а целый конкурс ответов), оценивается их качество (обычно reward-моделью) и строится advantage внутри этой группы (т.е. сравнивает ответы друг с другом).\n",
    "\n",
    "Формально, GRPO записывается примерно следующим образом:\n",
    "\n",
    "Пусть для промпта $x_i$ сгенерировали группу $j=1..K$ кандидатов ответов $y_{ij}$. Каждому сопоставлен reward $s_{ij}=r(x_i,y_{ij})$ (reward-моделью).\n",
    "Внутри группы выбираем baseline, например, ответ со средней оценкой или через leave-one-out и считаем advantage $ A_{ij} = s_{ij} - b_i $. ДалееНаходим отношение вероятностей (importance ratio):\n",
    "\n",
    "$$\n",
    "r_{ij}(\\theta) = \\frac{\\pi_\\theta(y_{ij}\\mid x_i)}{\\pi_{\\text{ref}}(y_{ij}\\mid x_i)}.\n",
    "$$\n",
    "\n",
    "4. **Surrogate (PPO-style) objective** — GRPO использует PPO-подобную суррогатную функцию:\n",
    "\n",
    "$$\n",
    "L_{\\text{GRPO}}(\\theta) = -\\mathbb{E}_{i,j}\\Big[ \\min\\big( r_{ij}(\\theta) A_{ij},\\ \\operatorname{clip}(r_{ij}(\\theta),1-\\epsilon,1+\\epsilon) A_{ij}\\big)\\Big] + \\lambda\\,\\mathbb{E}_i\\big[\\mathrm{KL}(\\pi_{\\text{ref}}(\\cdot|x_i)\\Vert\\pi_\\theta(\\cdot|x_i))\\big].\n",
    "$$\n",
    "\n",
    "Градиент вычисляется как обычно через $\\nabla_\\theta \\log\\pi_\\theta$ и взвешивается преимуществом $A_{ij}$ (с учётом клиппинга). Если $A_{ij}>0$ (ответ лучше baseline), градиент направлен повысить $\\pi_\\theta(y_{ij}|x_i)$ — через $\\nabla_\\theta\\log\\pi_\\theta$. Если $A_{ij}<0$ — наоборот. Клиппинг гарантирует, что обновления не будут слишком большими (по аналогии с PPO). Поскольку baseline берётся по группе, мы избегаем обучения отдельной value-функции, но дисперсия оценки преимущества может быть выше/ниже в зависимости от $K$ и качества reward-модели.\n",
    "\n",
    "Пример использования GRPO в библиотеке [TLR](https://huggingface.co/docs/trl/main/en/grpo_trainer).\n",
    "\n",
    "В качестве метода RLHF в DeepSeek-R1 используется как раз GRPO, представленный в статье [DeepSeekMath](https://arxiv.org/abs/2402.03300). Подробнее про обучение модели рассуждениям можно узнать в статье про [DeepSeek-R1](https://arxiv.org/abs/2501.12948)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc15e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fff52441",
   "metadata": {},
   "source": [
    "Можно мыслить в контексте того, что модель - это просто некая форма хранения данных. Отельным вопросом является то, происходит ли при этом сжатие\n",
    "\n",
    "Арифметическое кодирование\n",
    "\n",
    "Колмогоровская сложность\n",
    "\n",
    "Принцип минимальной длины описания\n",
    "\n",
    "За счет наблюдения большого числа паттернов информации, трансформер обладает лучшими свойствами сжатия чем специализированные алгоритмы\n",
    "\n",
    "Есть даже метрика, связанная с этим - Bits-per-byte, а также [статьи](https://arxiv.org/abs/2309.10668)\n",
    "\n",
    "\n",
    "Dense Attention\n",
    "\n",
    "Sparse Attention (формулы)\n",
    "От n*n до n sqrt(n). Но его в итоге редко применяют\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a56cd07",
   "metadata": {},
   "source": [
    "Flash Attention - вариант аппаратного ускорения. В стандартной реализация self-attention использует матрицу $ L \\times L $ (где $L$ — длина последовательности) - ее тяжело держать в памяти, но операции проводить с ней можно. Идея в том чтобы делать даже больше вычислений, но меньше перекладывания из памяти (в GPU есть HBM (High-Bandwidth Memory) - большая и долгая, SRAM - маленькая и быстрая). Для этого матрицы Q, K, V разбиваются на T блоков по разным измерениям, загружаем такие блоки в SRAM, делаем все операции ($ \\text{softmax} (Q_i @ K_j^T) V_j $) и получаем кусочек выходной матрицы. Однако, считаем статистики softmax между такими шагами и корректируем значения итоговой матрицы при необходимости. Вторая версия этого алгоритма лучше распределяет работу между несколькими ядрами GPU, добавляет параллельность по последовательности, разбивает на блоки матрицы K и V, а не Q, как в первой версии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965c78d",
   "metadata": {},
   "source": [
    "[MoE](https://huggingface.co/blog/moe) - это паттерн сокращения \"активных\" весов модели. Как было ранее упомянуто, большая часть весов модели это полносвязные слои (feed-forward network слои, FFN), идущие после self-attention, поэтому работать будем именно с ними. В модели каждый стандартный FFN-блок заменяют на набор \n",
    "$ E $ (экспертов) (малых FFN) и на каждый токен теперь активируется только небольшой набор экспертов (top-k). Перед применением весов экспертов ставится gate network или router, который определяет какие токены будут поданы в какого эксперта - это простой линейный слой с логитами по экспертам, к его выходу добавляют softmax и получают веса экспертов для токена $ w_{t,e} $, они используются при агрегации ответов экспертов. Обучение router'а происходит просто за счет backprop модели, лишь с [оговоркой](https://arxiv.org/abs/1701.06538) на добавление шума при операции top-k или других доработках, распределяющих нагрузку на экспертов более равномерно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf74d4c",
   "metadata": {},
   "source": [
    "[Обзор](https://arxiv.org/pdf/2402.19473) методов видов RAG\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "AlphaEvolve — это агентский эволюционный фреймворк, который сочетает в себе большие языковые модели (LLM) и эволюционные алгоритмы для автоматического открытия и оптимизации решений сложных задач. Его ключевая идея — не просто сгенерировать код один раз, а итеративно улучшать его, создавая \"популяцию\" решений, скрещивать и мутировать их, а затем отбирать лучшие для следующего поколения.\n",
    "\n",
    "\n",
    "\n",
    "Reasoning - это способность модели выполнять многшаговые рассуждения, разложение задачи и выдачу промежуточных выводов (rationales, [chain-of-thought](https://arxiv.org/abs/2201.11903)).\n",
    "\n",
    "Мы можем вызвать такое рассуждающее поведение у модели используя промпт ([CoT prompting](https://arxiv.org/abs/2005.14165) включающий в себя few-shot, Zero-shot CoT, [ReAct](https://arxiv.org/abs/2210.03629) (думай - делай))\n",
    "\n",
    "Можно обучить модель делать цепочки рассуждения. \n",
    "используя на SFT этапе датасет с цепочками рассуждений. В простейшем случае это отличается от стандартного SFT этапа наличием дополнительного токена `think` или полем `rationale` в шаблоне ответа.\n",
    "\n",
    "Затем проводится этап Instruction tuning, как некое дополнение к RLHF, на парах \"инструкция -> правильное поведение\".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "В [DeepSeek-V3](https://arxiv.org/abs/2412.19437) MTP (multi token prediction) Вместо классической next-token prediction (предсказать только $ x_{t + 1} $ при позиции $ t $), MTP требует у модели предсказать сразу несколько следующих токенов $ x_{t+1} ​ ,..., x_{t+M} $​ (или их распределения) на каждом шаге.\n",
    "\n",
    "Благодаря этому:\n",
    "- больше градиентных сигналов на те же параметры\n",
    "- модель вынуждена формировать представления, которые пригодны сразу для предсказания нескольких шагов\n",
    "- потенциальное ускорение генерации и уменьшение задержки\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[TransMLA](http://arxiv.org/abs/2502.07864) - Multi head latent attention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d3fed5",
   "metadata": {},
   "source": [
    "Отлично — собрал понятный и практичный обзор метрик, которыми обычно оценивают LLM: что они измеряют, как считаются, сильные и слабые стороны, типичные применения и практические советы. На русском, коротко по делу.\n",
    "\n",
    "# Общее замечание перед списком\n",
    "\n",
    "Автоматические метрики редко дают «полную картину». Они полезны для быстрых A/B-сравнений и регресс-тестов, но почти всегда нужно несколько метрик + человеческая оценка для финального решения (особенно для open-ended generation, диалогов, fact-checking и т.п.).\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Perplexity (PPL)\n",
    "\n",
    "**Что меряет:** насколько хорошо модель предсказывает текст в среднем (обратная экспонента к пер-токеновому кросс-энтропийному loss).\n",
    "**Формула (интуитивно):** $ \\text{PPL} = \\exp\\big(\\frac{1}{N}\\sum_{t}-\\log p(x_t\\mid x_{<t})\\big)$.\n",
    "**Интерпретация:** меньше — лучше (модель «менее удивлена» данными).\n",
    "**Плюсы:** простая, пригодна для оценки языковой согласованности и предобучения, хорошо сравнивать на одном и том же датасете/токенизации.\n",
    "**Минусы:** чувствительна к токенизации (разные токенизаторы дают разные PPL), не отражает качество генерации по задачам (правильность ответа, фактичность) и плохо коррелирует с human-judgement для open-ended задач.\n",
    "**Когда использовать:** language modeling, pretraining, мониторинг overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. BLEU / SacreBLEU\n",
    "\n",
    "**Что меряет:** n-gram совпадения между гипотезой и одним/несколькими референсами (обычно до 4-грамм) с модифицированным precision и brevity penalty.\n",
    "**Интерпретация:** больше — лучше. SacreBLEU — стандартизированная реализация BLEU (репродуцируемые метрики).\n",
    "**Плюсы:** простота, широко используется в машинном переводе, хорошо подходит когда референс-ориентированная задача и много референсов.\n",
    "**Минусы:** нечувствителен к перефразированию и синонимам, плохо коррелирует в задачах, где допустим широкий диапазон корректных ответов (summary, open Q/A).\n",
    "**Когда использовать:** машинный перевод, структурированные NLG, где референсы «узкие».\n",
    "\n",
    "---\n",
    "\n",
    "# 3. ROUGE (ROUGE-N, ROUGE-L)\n",
    "\n",
    "**Что меряет:** перекрытие n-gram (ROUGE-N) и длину наибольшей общей подпоследовательности (ROUGE-L) — часто для суммаризации.\n",
    "**Плюсы:** стандарт для суммаризации, прост в реализации.\n",
    "**Минусы:** как и BLEU, чувствителен к лексическим совпадениям; может недооценивать хорошую абстрактную суммаризацию.\n",
    "**Когда использовать:** абстрактная/экстрактивная суммаризация (в сочетании с human eval).\n",
    "\n",
    "---\n",
    "\n",
    "# 4. METEOR / chrF\n",
    "\n",
    "**METEOR:** учитывает синонимы, стемминг и выравнивание — иногда коррелирует лучше с human judgment чем BLEU в небольших корпусах.\n",
    "**chrF:** подсчитывает F-меру для character n-grams — полезно для агглютинативных языков.\n",
    "**Когда:** альтернативы BLEU/ROUGE при специфичных языках или когда нужно учитывать морфологию.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. BERTScore\n",
    "\n",
    "**Что меряет:** сходство токенов на уровне эмбеддингов (BERT/RoBERTa) — сопоставляет гипотезу и референс через cosine similarity эмбеддингов.\n",
    "**Плюсы:** чувствителен к синонимии и перефразированию; часто лучше коррелирует с человеческой оценкой для многих NLG задач.\n",
    "**Минусы:** зависит от выбранных эмбеддингов и может «прощать» фактические ошибки (т.е. похожий семантически, но неверный факт — всё ещё высокий BERTScore).\n",
    "**Когда:** суммаризация, перевод, генерация текста — как дополнение к n-gram метрикам.\n",
    "\n",
    "---\n",
    "\n",
    "# 6. BLEURT / COMET / BLEURT-like learned metrics\n",
    "\n",
    "**Что это:** нейросетевые метрики, дообученные на человеческих оценках (BLEURT, COMET для MT, etc.). Выдают число, которое пытается напрямую предсказать человеческую оценку качества.\n",
    "**Плюсы:** гораздо лучше коррелируют с человеческими рейтингами, учитывают семантику и качество флюэнси. COMET — ориентированная на MT, BLEURT — на более общий NLG.\n",
    "**Минусы:** могут переобучаться на распределении тренировочных данных, требовательны к domain-match; «черный ящик» — меньше объяснимости.\n",
    "**Когда:** когда есть потребность в автоматическом приближении human judgement (A/B тесты, фильтрация кандидатов).\n",
    "\n",
    "---\n",
    "\n",
    "# 7. MAUVE / Distributional metrics\n",
    "\n",
    "**MAUVE:** пытается измерить расхождение между распределиями сгенерированного текста и реальных (human) текстов; оценивает качество/разнообразие в распределенном смысле.\n",
    "**Плюсы:** полезна для обнаружения mode collapse или чрезмерной «маловероятной» генерации.\n",
    "**Минусы:** интерпретация неочевидна, чувствительна к размеру и домену.\n",
    "**Когда:** оценка качества генеративной модели в целом (style, diversity), особенно для open-ended generation.\n",
    "\n",
    "---\n",
    "\n",
    "# 8. Diversity & Degeneration: distinct-n, Self-BLEU\n",
    "\n",
    "**distinct-n:** доля уникальных n-gram среди всех сгенерированных — измеряет разнообразие.\n",
    "**Self-BLEU:** обратный показатель (насколько поколение повторяет себя) — низкая self-BLEU → высокое разнообразие.\n",
    "**Когда:** полезны для диалоговых моделей и творческой генерации, чтобы измерить разнообразие и избежать повторов.\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Task-specific metrics\n",
    "\n",
    "* **Exact Match (EM)** и **F1** — для QA (span extraction): EM — процент полностью совпавших ответов, F1 — токен-уровневое согласие.\n",
    "* **Pass\\@k / pass\\@1** — для code generation: вероятность, что среди k сгенерированных программ есть корректная.\n",
    "* **Accuracy** — для classification / multiple-choice tasks.\n",
    "* **ROUGE/BLEU/BLEURT + QA metrics** — для QG/QA pipelines комбинируют.\n",
    "\n",
    "---\n",
    "\n",
    "# 10. Factuality / Hallucination метрики\n",
    "\n",
    "* **QAGS / QuestEval / QuestEval-like** — задают вопросы по.reference/hypothesis и сравнивают ответы, либо используют QA-модели, чтобы оценить соответствие.\n",
    "* **FactCC / FEVER-style** — маркируют утверждения на согласованность с контекстом.\n",
    "* **Model-based verifiers**: запуск fact-checker (retrieval+verification) для каждого утверждения.\n",
    "  **Важно:** автоматические factuality-метрики часто дают много ложных срабатываний и требуют ручной валидации.\n",
    "\n",
    "---\n",
    "\n",
    "# 11. Human evaluation (неавтоматическое, но критичное)\n",
    "\n",
    "Стандартные схемы:\n",
    "\n",
    "* **Pairwise preference** (A vs B) — чаще всего надёжнее и проще для аннотаторов.\n",
    "* **Likert шкала** (1–5) по критериям: fluency, relevance, factuality, helpfulness.\n",
    "* **Best-worst scaling** — полезно для ранжирования множества систем.\n",
    "  **Совет:** проводить blind A/B, пояснять инструкции аннотаторам, репликивать на разных аннотаторах и вычислять меж-ранговую согласованность.\n",
    "\n",
    "---\n",
    "\n",
    "# 12. Практические рекомендации (чеклист)\n",
    "\n",
    "1. **Комбинируйте метрики.** Для summarization возьмите ROUGE + BERTScore + BLEURT/COMET + ручная проверка фактов.\n",
    "2. **Сравнивайте в рамках одного токенизатора/референса** (особенно для PPL).\n",
    "3. **Старайтесь измерять статистическую значимость** (bootstrap resampling, approximate randomization) при A/B.\n",
    "4. **Калибруйте learned-metrics** (BLEURT/COMET) под ваш домен, если возможно.\n",
    "5. **Не доверяйте одному числу**: проверьте качественные разборы ошибок (error analysis).\n",
    "6. **Для open-ended**: больше веса human evaluation + distributional metrics (MAUVE) + factuality checks.\n",
    "7. **Для code**: используйте pass\\@k + unit tests (functional).\n",
    "8. **Логи и мониторинг в продакшн:** track perplexity, toxicity scores, hallucination flags, average length, latency.\n",
    "\n",
    "---\n",
    "\n",
    "# 13. Как интерпретировать числа (коротко)\n",
    "\n",
    "* **BLEU/ROUGE:** абсолютное значение мало говорит; сравнивайте системы на одном датасете.\n",
    "* **BERTScore / BLEURT / COMET:** более интерпретируемы в относительном сравнении; положительная разница часто значима.\n",
    "* **Perplexity:** относительная величина по датасету; снижение PPL на 10% — обычно заметно, но не гарантирует улучшение downstream.\n",
    "* **Pass\\@k (code):** специалисты часто смотрят pass\\@1 и pass\\@10, потому что генерация нескольких кандидатов — обычная практика.\n",
    "\n",
    "---\n",
    "\n",
    "Если нужно, могу:\n",
    "\n",
    "* предложить **комбинацию метрик** для вашей конкретной задачи (скажите: перевод / суммаризация / диалог / код / QA),\n",
    "* подготовить **скрипт на Python** (HF + sacrebleu + bert\\_score + bleurt + mauve) для оценки ваших выводов,\n",
    "* или составить план **human evaluation** (инструкции для аннотаторов, шаблон анкеты, как анализировать результаты).\n",
    "\n",
    "Что из этого сделать дальше?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
