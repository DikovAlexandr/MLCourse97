{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 3: Деревья решений и ансамблевые методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Деревья решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Принцип работы\n",
    "\n",
    "1. **Корневой узел**: Всю выборку $D = \\{\\mathbf{x}, \\mathbf{y}\\}$ рассматривают как набор в корневом узле.\n",
    "2. **Разбиение**: На каждом шаге выбирается признак и порог, по которому выборка разделяется на два (или более) подмножеств $D_1$ и $D_2$. Критерий выбора разбиения основан на уменьшении некоторой величины, показывающей качество (чистоту) разбиения.\n",
    "3. **Остановка**: Процесс рекурсивного деления продолжается до достижения определённых условий (максимальная глубина, минимальное число объектов в узле, достижение чистого узла и т.д.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрики для разбиения\n",
    "\n",
    "##### Энтропия и информационный выигрыш\n",
    "\n",
    "Энтропия – это мера неопределённости или «хаотичности» распределения. Если в узле дерева классы распределены поровну, энтропия максимальна (мы практически не можем предсказать класс случайно выбранного объекта). Если же все объекты одного класса, энтропия равна нулю – неопределённость отсутствует. При выборе признака для разбиения мы ищем тот, который даёт наибольшее снижение энтропии (то есть увеличивает чистоту разбиения).\n",
    "\n",
    "Предположим, что в некотором узле дерева у нас есть объекты, принадлежащие K классам, и пусть $p_i$ – доля объектов, принадлежащих классу $i$ (при этом $\\sum_{i=1}^{K} p_i = 1$).\n",
    "\n",
    "Энтропия узла $D$ определяется как:\n",
    "$$\n",
    "H = - \\sum_{i=1}^{K} p_i \\log_2(p_i),\n",
    "$$\n",
    "где $p_k$ – доля объектов, принадлежащих классу $k$. \n",
    "\n",
    "При этом, если какая-то вероятность равна нулю, принято считать, что вклад $p_i \\log_2(p_i) = 0$. \n",
    "\n",
    "В случае бинарной классификации (K=2), если обозначить $p = p_1$ и $1-p = p_2$, формула принимает вид:\n",
    "$$\n",
    "H = - \\left[p \\log_2(p) + (1-p) \\log_2(1-p)\\right]\n",
    "$$\n",
    "\n",
    "Например, если $p = 0.5$, то:\n",
    "$$\n",
    "H = - \\left[0.5\\log_2(0.5) + 0.5\\log_2(0.5)\\right] = - \\left[0.5(-1) + 0.5(-1)\\right] = 1\n",
    "$$\n",
    "\n",
    "При разбиении узла информационный выигрыш вычисляется по формуле:\n",
    "$$\n",
    "\\Delta H = H(D) - \\sum_{v \\in \\text{Values}(A)} \\frac{|D_v|}{|D|} H(D_v),\n",
    "$$\n",
    "где $D_v$ – подмножество данных, для которых значение признака $A$ равно $v$.\n",
    "\n",
    "Например, если исходная неоднородность в родительском узле равна $H_{\\text{parent}}$, а после разбиения получаются два дочерних узла с неоднородностями $H_{\\text{left}}$ и $H_{\\text{right}}$, прирост информации определяется как:\n",
    "$$\n",
    "\\Delta H = H_{\\text{parent}} - \\left(\\frac{|D_{\\text{left}}|}{|D|} H_{\\text{left}} + \\frac{|D_{\\text{right}}|}{|D|} H_{\\text{right}}\\right),\n",
    "$$\n",
    "где $|D_{\\text{left}}$ и $|D_{\\text{right}}$ – число объектов в левых и правых узлах, а $|D|$ – общее число объектов в родительском узле.\n",
    "\n",
    "##### Индекс Джини\n",
    "\n",
    "Индекс Джини – ещё одна мера неоднородности. Он показывает вероятность того, что случайно выбранный объект будет неправильно классифицирован, если назначить ему класс по распределению в узле. Если в узле объекты принадлежат разным классам почти поровну, индекс Джини будет высоким, а если почти все объекты принадлежат к одному классу – низким. При выборе признака для разбиения выбирают тот, который приводит к наименьшему значению индекса Джини в дочерних узлах.\n",
    "\n",
    "Индекс Джини определяется как:\n",
    "$$\n",
    "G = 1 - \\sum_{i=1}^{K} p_i^2.\n",
    "$$\n",
    "\n",
    "Для бинарного случая получаем:\n",
    "$$\n",
    "G = 1 - \\left[p^2 + (1-p)^2\\right]\n",
    "$$\n",
    "\n",
    "Если $p = 0.5$, то:\n",
    "$$\n",
    "G = 1 - \\left[0.25 + 0.25\\right] = 0.5\n",
    "$$\n",
    "\n",
    "При выборе разбиения ищут признак, который даёт максимальное уменьшение индекса Джини:\n",
    "$$\n",
    "\\Delta G = G(D) - \\sum_{v \\in \\text{Values}(A)} \\frac{|D_v|}{|D|} G(D_v).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Особенности\n",
    "- **Жадный алгоритм**: На каждом шаге выбирается локально оптимальное разбиение, что не гарантирует глобально оптимального дерева.\n",
    "- **Переобучение**: Очень глубокие деревья могут переобучаться, поэтому применяются методы отсечения (pruning) для улучшения обобщающей способности модели.\n",
    "- **Интерпретируемость**: Деревья решений являются интуитивно понятными и наглядно демонстрируют процесс принятия решений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Более подробное описание, с алгоритмическими тонкостями и обоснованиями в [учебнике](https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/decision_tree.png\" alt=\"Дерево решений\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм построения решающего дерева:\n",
    "1. **Начальный этап (корневой узел)**:  \n",
    "   На вход подаётся весь набор данных. Для этого узла вычисляются пропорции классов $p_i$ (например, для бинарной классификации – это $p$ и $1-p$).\n",
    "2. **Выбор оптимального разбиения**:  \n",
    "   Для каждого признака и для каждого возможного порогового значения рассчитывается мера неоднородности узла.\n",
    "   Затем для каждого потенциального разбиения вычисляется прирост информации (или снижение неоднородности).\n",
    "3. **Разбиение узла**:  \n",
    "   Выбирается разбиение, при котором прирост информации максимален (или индекс Джини минимален). Данные разделяются на две группы - например, по условию «значение признака $\\leq$ порога» и «значение признака $>$ порога».\n",
    "4. **Рекурсивное повторение**:  \n",
    "   Применяется тот же алгоритм к каждому из полученных дочерних узлов. Алгоритм повторяет выбор оптимального разбиения для каждого узла, используя те же формулы, пока не выполнится одно из условий остановки:\n",
    "   - Узел достиг максимальной глубины.\n",
    "   - Количество объектов в узле меньше заданного минимума.\n",
    "   - Все объекты в узле принадлежат одному классу.\n",
    "5. **Окончательное решение (листовые узлы)**:  \n",
    "   После завершения рекурсии узлы, которые больше не делятся, становятся листовыми узлами.  \n",
    "   - Для задачи классификации в листе назначается класс, который чаще всего встречается среди объектов.\n",
    "   - Для задачи регрессии вычисляется среднее значение целевой переменной в узле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X, y = make_classification(n_samples=50,\n",
    "                            n_features=2,\n",
    "                            n_redundant=0,\n",
    "                            n_informative=2,\n",
    "                            n_clusters_per_class=1,\n",
    "                            flip_y=0.1,\n",
    "                            class_sep=1.5,\n",
    "                            random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = tree_clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(acc * 100))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plot_tree(tree_clf,\n",
    "            feature_names=[\"Feature 1\", \"Feature 2\"],\n",
    "            filled=True,\n",
    "            rounded=True,\n",
    "            fontsize=10)\n",
    "plt.title(\"Tree graph\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разложение ошибки на смещение и дисперсию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из центральных понятий в машинном обучении является **разложение ошибки на смещение и дисперсию (bias-variance decomposition)**. Оно позволяет понять, какие составляющие вносят вклад в общую ошибку модели и как можно управлять этим балансом при выборе и настройке моделей.\n",
    "\n",
    "> Более подробно в [учебнике](https://education.yandex.ru/handbook/ml/article/bias-variance-decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теория\n",
    "Пусть у нас имеется функция истинной зависимости $ y = f(x) + \\varepsilon $, где:\n",
    "- $ f(x) $ – истинная функция,\n",
    "- $\\varepsilon$ – случайный шум с дисперсией $\\sigma^2$ (неизбежная ошибка, относящаяся к шуму).\n",
    "\n",
    "Модель, обученная на случайной выборке, даёт предсказание $\\hat{f}(x)$, которое само по себе является случайной величиной (так как зависит от конкретной выборки).\n",
    "\n",
    "\n",
    "Для фиксированного $x$ нас интересует ожидаемая квадратичная ошибка:\n",
    "$$\n",
    "\\mathbb{E}[(y - \\hat{f}(x))^2] = \\mathbb{E}[(f(x) + \\epsilon - \\hat{f}(x))^2].\n",
    "$$\n",
    "\n",
    "Раскроем квадрат:\n",
    "$$\n",
    "\\mathbb{E}\\Big[(f(x)-\\hat{f}(x))^2\\Big] + 2\\mathbb{E}\\Big[(f(x)-\\hat{f}(x))\\epsilon\\Big] + \\mathbb{E}[\\epsilon^2].\n",
    "$$\n",
    "\n",
    "Поскольку $\\epsilon$ имеет нулевое среднее и независим от $\\hat{f}(x)$, второй член равен нулю, а $\\mathbb{E}[\\epsilon^2]=\\sigma^2$. \n",
    "\n",
    "Теперь для первого члена добавим и вычтем среднее значение предсказания $\\mathbb{E}[\\hat{f}(x)]$:\n",
    "$$\n",
    "f(x)-\\hat{f}(x)= \\Bigl(f(x)-\\mathbb{E}[\\hat{f}(x)]\\Bigr) + \\Bigl(\\mathbb{E}[\\hat{f}(x)]-\\hat{f}(x)\\Bigr).\n",
    "$$\n",
    "\n",
    "Тогда, раскрыв квадрат, получим:\n",
    "$$\n",
    "\\mathbb{E}[(f(x)-\\hat{f}(x))^2] = \\Bigl(f(x)-\\mathbb{E}[\\hat{f}(x)]\\Bigr)^2 + \\mathbb{E}\\Bigl[(\\hat{f}(x)-\\mathbb{E}[\\hat{f}(x)])^2\\Bigr] + 2\\,\\mathbb{E}\\Bigl[\\Bigl(f(x)-\\mathbb{E}[\\hat{f}(x)]\\Bigr)\\Bigl(\\mathbb{E}[\\hat{f}(x)]-\\hat{f}(x)\\Bigr)\\Bigr].\n",
    "$$\n",
    "\n",
    "При этом последний (поперечный) член равен нулю, поскольку $\\mathbb{E}[\\hat{f}(x)]- \\hat{f}(x)$ имеет нулевое математическое ожидание. Таким образом получаем:\n",
    "$$\n",
    "\\mathbb{E}\\left[(y - \\hat{f}(x))^2\\right] = \\underbrace{\\left( \\mathbb{E}[\\hat{f}(x)] - f(x) \\right)^2}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}\\left[\\left(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)]\\right)^2\\right]}_{\\text{Variance}} + \\sigma^2.\n",
    "$$\n",
    "\n",
    "- **Смещение (Bias)**: Показывает, насколько среднее предсказание модели отклоняется от истинной функции $ f(x) $. Высокое смещение может возникать, если модель слишком проста и не способна уловить сложность зависимости (так называемое недообучение).\n",
    "\n",
    "- **Дисперсия (Variance)**: Характеризует изменчивость предсказаний модели при изменении обучающей выборки. Высокая дисперсия свойственна сложным моделям, которые сильно адаптируются к обучающим данным (что может привести к переобучению).\n",
    "\n",
    "Баланс между bias и variance определяет качество обобщения модели. Идеальной моделью является такая, которая имеет как низкое смещение, так и низкую дисперсию, однако в реальных задачах часто наблюдается компромисс между ними."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сильные и слабые модели\n",
    "В контексте ансамблирования модели часто делят на **слабые** и **сильные**:\n",
    "\n",
    "- **Слабые модели** – это модели, которые лишь немного лучше случайного угадывания. В задачах классификации такие алгоритмы называют «слабым обучением». Пример: *решающее дерево с единичной глубиной* (decision stump) или в целом неглубокие деревья. Они, как правило, имеют высокое смещение (не могут уловить сложные зависимости), но обладают низкой дисперсией.\n",
    "\n",
    "- **Сильные модели** – это более сложные модели с хорошей способностью описывать данные (низкое смещение), но зачастую они обладают высокой дисперсией. Пример: полные, глубоко растущие деревья решений без отсечения. В ансамблировании, например, случайный лес использует именно такие сильные модели, и за счёт бутстреп-агрегации (bagging) достигается значительное снижение дисперсии.\n",
    "\n",
    "Различные методы ансамблирования используют именно эти особенности:\n",
    "\n",
    "- **Бэггинг (Bagging)**: Обычно применяется для сильных моделей с высоким разбросом (variance). Например, полные деревья решений могут сильно колебаться от разбиения к разбиению, и их усреднение снижает дисперсию предсказания.\n",
    "\n",
    "- **Бустинг (Boosting)**: Часто использует слабые модели с ограниченной глубиной (низкая дисперсия, высокое смещение), внося последовательные коррективы в модель. Каждая последующая слабая модель обучается на исправлении ошибок предыдущих, что позволяет снижать смещение итогового ансамбля.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/darts.png\" alt=\"Визуализация bias и variance\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/bias_vs_variance.png\" alt=\"Сложность модели и переобучение\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.evaluate import bias_variance_decomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1,\n",
    "                             max_depth = 100)\n",
    "avg_error, avg_bias, avg_var = bias_variance_decomp(clf, \n",
    "                                                    X_train, y_train, \n",
    "                                                    X_test, y_test,\n",
    "                                                    loss='0-1_loss', \n",
    "                                                    num_rounds=200, \n",
    "                                                    random_seed=1)\n",
    "\n",
    "print(\"Average Classification Error: {:.3f}\".format(avg_error))\n",
    "print(\"Average Bias: {:.3f}\".format(avg_bias))\n",
    "print(\"Average Variance: {:.3f}\".format(avg_var))\n",
    "\n",
    "labels = ['Error', 'Bias', 'Variance']\n",
    "values = [avg_error, avg_bias, avg_var]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = sns.color_palette(\"Set2\", n_colors=len(labels))\n",
    "bars = plt.bar(labels, values, color=colors)\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Bias-Variance for Decision Tree Classifier\")\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval, \n",
    "             f\"{yval:.3f}\", ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ансамблевые методы\n",
    "\n",
    "Ансамблевые методы объединяют несколько моделей для получения более точного и стабильного результата. Они используются для уменьшения дисперсии, смещения или общей ошибки модели.\n",
    "\n",
    "> [Учебник](https://education.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/ensemble_methods.png\" alt=\"Ансамблевые методы\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/ensemble_models.png\" alt=\"Ансамблевые модели\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_function(x):\n",
    "    \"\"\"\n",
    "    Истинная функция: если sin(x) > 0, то класс 1, иначе 0\n",
    "    \"\"\"\n",
    "    return (np.sin(x) > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(model, n_train, noise_std, x_grid, n_experiments):\n",
    "    \"\"\"\n",
    "    Запускает серию экспериментов для оценки предсказаний модели.\n",
    "\n",
    "    Аргументы:\n",
    "        model: экземпляр модели (например, DecisionTreeRegressor или RandomForestRegressor)\n",
    "        n_train (int): число обучающих образцов в каждом эксперименте.\n",
    "        noise_std (float): стандартное отклонение шума.\n",
    "        x_grid (np.ndarray): массив точек (форма (n_points, 1)) для вычисления предсказаний.\n",
    "        n_experiments (int): количество экспериментов.\n",
    "\n",
    "    Возвращает:\n",
    "        predictions (np.ndarray): массив предсказаний формы (n_experiments, n_points).\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for _ in range(n_experiments):\n",
    "        X_train = np.random.uniform(0, 2*np.pi, n_train).reshape(-1, 1)\n",
    "        noise = np.random.normal(0, noise_std, n_train)\n",
    "        y_train = true_function(X_train.ravel())\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(x_grid)\n",
    "        predictions.append(pred)\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variance_decomposition(predictions, y_true):\n",
    "    \"\"\"\n",
    "    Вычисляет bias² и variance для набора предсказаний по координатам.\n",
    "\n",
    "    Аргументы:\n",
    "        predictions (np.ndarray): матрица предсказаний формы (n_experiments, n_points).\n",
    "        y_true (np.ndarray): истинные значения функции для точек сетки (n_points,).\n",
    "\n",
    "    Возвращает:\n",
    "        global_bias (float): усреднённое значение bias² по всем точкам сетки.\n",
    "        global_variance (float): усреднённое значение дисперсии предсказаний.\n",
    "    \"\"\"\n",
    "    mean_pred = np.mean(predictions, axis=0)\n",
    "    bias_sq = (y_true - mean_pred)**2\n",
    "    variance = np.var(predictions, axis=0)\n",
    "    global_bias = np.mean(bias_sq)\n",
    "    global_variance = np.mean(variance)\n",
    "    return global_bias, global_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(model_names, bias_values, variance_values):\n",
    "    \"\"\"\n",
    "    Выводит результаты (bias² и variance для каждой модели) и строит группированный столбчатый график.\n",
    "\n",
    "    Аргументы:\n",
    "        model_names (list[str]): список имен моделей.\n",
    "        bias_values (list[float]): список значений bias² для каждой модели.\n",
    "        variance_values (list[float]): список значений variance для каждой модели.\n",
    "    \"\"\"\n",
    "    for name, bias, var in zip(model_names, bias_values, variance_values):\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Bias²: {bias:.3f}, Variance: {var:.3f}\\n\")\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(6, 4))\n",
    "    width = 0.35\n",
    "    indices = np.arange(len(model_names))\n",
    "\n",
    "    palette = sns.color_palette(\"Set2\", n_colors=2)\n",
    "    bars_bias = ax.bar(indices - width / 2, bias_values, width, \n",
    "                       label=\"Bias²\", color=palette[0])\n",
    "    bars_variance = ax.bar(indices + width / 2, variance_values, width, \n",
    "                           label=\"Variance\", color=palette[1])\n",
    "\n",
    "    ax.set_xlabel(\"Модель\")\n",
    "    ax.set_ylabel(\"Значение\")\n",
    "    ax.set_xticks(indices)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    ax.legend()\n",
    "\n",
    "    for bar in bars_bias + bars_variance:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f\"{height:.3f}\",\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bootstrap Aggregating (Bagging)** заключается в обучении множества моделей на различных бутстреп-выборках из исходного набора данных. Итоговый прогноз вычисляется как агрегированное (например, голосование для классификации) предсказание отдельных \n",
    "моделей.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "Бутстреп выборка имеет такой же размер, что и исходная (генерация с повторениями).\n",
    "</div> \n",
    "\n",
    "Пусть $ D $ – исходная выборка из $ N $ объектов. Из неё генерируются $ M $ бутстреп-выборок $ D^{(1)}, D^{(2)}, \\ldots, D^{(M)} $. Для каждой выборки $ D^{(m)} $ обучается модель $ f^{(m)}(x) $. Тогда итоговая модель имеет вид:\n",
    "  $$\n",
    "  f_{\\text{bag}}(x) = \\frac{1}{M}\\sum_{m=1}^{M} f^{(m)}(x).\n",
    "  $$\n",
    "  Для задач классификации итоговое решение часто определяется по принципу большинства голосов.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Особенность такого подхода в снижении дисперсии модели, стабилизации предсказаний и уменьшения риска переобучения.\n",
    "</div> \n",
    "\n",
    "**Пример:**\n",
    "    Random Forest – ансамбль деревьев решений, где каждая модель обучается на бутстреп-выборке, а при выборе разбиений учитывается случайное подмножество признаков.\n",
    "\n",
    "- Каждое дерево обучается на случайной бутстреп-выборке исходных данных.\n",
    "- При построении каждого дерева на каждом узле выбирается случайное подмножество признаков для разбиения, что снижает корреляцию между деревьями.\n",
    "\n",
    "Для $ M $ деревьев решений, где $ f^{(m)}(x) $ – предсказание $ m $-го дерева, итоговое предсказание определяется как:\n",
    "$$\n",
    "f_{\\text{RF}}(x) = \\frac{1}{M}\\sum_{m=1}^{M} f^{(m)}(x),\n",
    "$$\n",
    "при этом для задач классификации применяется голосование, а для регрессии – усреднение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/bagging_classifier.png\" alt=\"Bagging\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_train = 20\n",
    "noise_std = 0.3\n",
    "n_experiments = 50\n",
    "x_grid = np.linspace(0, 2*np.pi, 100).reshape(-1, 1)\n",
    "y_true = true_function(x_grid.ravel())\n",
    "\n",
    "tree_model = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "tree_predictions = run_experiments(tree_model, n_train, \n",
    "                                   noise_std, x_grid, n_experiments)\n",
    "tree_bias, tree_variance = bias_variance_decomposition(tree_predictions, y_true)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_predictions = run_experiments(rf_model, n_train, noise_std, x_grid, n_experiments)\n",
    "rf_bias, rf_variance = bias_variance_decomposition(rf_predictions, y_true)\n",
    "\n",
    "model_names = [\"Decision Tree\", \"Bagging\"]\n",
    "bias_values = [tree_bias, rf_bias]\n",
    "variance_values = [tree_variance, rf_variance]\n",
    "\n",
    "display_results(model_names, bias_values, variance_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бустинг (Boosting) – это семейство ансамблевых методов, в которых базовые модели (`weak learners`) обучаются **последовательно**. Основная идея в том, что каждая следующая модель стремится скорректировать ошибки предыдущих. В результате достигается высокое качество за счёт комбинирования многих слабых моделей в один сильный ансамбль."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AdaBoost\n",
    "\n",
    "**AdaBoost** – один из первых бустинговых алгоритмов для классификации. Основная идея – обучать последовательность слабых классификаторов $h_m$, каждый из которых уделяет больше внимания объектам, неправильно классифицированным предыдущими моделями.\n",
    "\n",
    "1. **Инициализация весов объектов**\n",
    "\n",
    "   $$\n",
    "   w_i^{(1)} = \\frac{1}{N}, \\quad i=1,\\dots,N.\n",
    "   $$\n",
    "\n",
    "2. **Обучение $m$-го слабого классификатора**\n",
    "   $h_m: \\mathcal X \\to \\{ -1, +1\\}$ обучается на выборке с весами $w_i^{(m)}$.\n",
    "\n",
    "3. **Вычисление ошибки модели**\n",
    "\n",
    "   $$\n",
    "   \\epsilon_m = \\sum_{i=1}^{N} w_i^{(m)} \\; \\mathbb{I}\\bigl(y_i \\neq h_m(x_i)\\bigr),\n",
    "   $$\n",
    "\n",
    "   где $y_i \\in \\{-1, +1\\}$.\n",
    "\n",
    "4. **Вес модели в ансамбле**\n",
    "\n",
    "   $$\n",
    "   \\alpha_m = \\frac{1}{2} \\ln\\frac{1 - \\epsilon_m}{\\epsilon_m}.\n",
    "   $$\n",
    "\n",
    "5. **Обновление весов объектов**\n",
    "\n",
    "   $$\n",
    "   w_i^{(m+1)} = \\frac{w_i^{(m)} \\exp\\bigl(-\\alpha_m y_i h_m(x_i)\\bigr)}{Z_m},\n",
    "   $$\n",
    "\n",
    "   где нормировочный множитель $Z_m$ обеспечивает $\\sum_i w_i^{(m+1)} = 1$.\n",
    "\n",
    "6. **Финальное решение**\n",
    "\n",
    "   $$\n",
    "   F(x) = \\operatorname{sign}\\bigl(\\sum_{m=1}^M \\alpha_m h_m(x)\\bigr).\n",
    "   $$\n",
    "\n",
    "- Тенденция к переобучению при большом числе итераций и слабых шумоустойчивых базовых моделях.\n",
    "- Чувствительность к выбросам из‑за экспоненциального обновления весов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/boosting_classifier.png\" alt=\"Boosting\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting\n",
    "\n",
    "**Gradient Boosting** – обобщает идею бустинга как решение задачи градиентного спуска в функциональном пространстве: ансамбль строится итеративно, минимизируя выбранную функцию потерь $L(y, F(x))$.\n",
    "\n",
    "1. **Инициализация ансамбля**\n",
    "\n",
    "   $$\n",
    "   F_0(x) = \\arg\\min_{\\gamma} \\sum_{i=1}^N L\\bigl(y_i, \\gamma\\bigr).\n",
    "   $$\n",
    "\n",
    "2. **Итерации $m=1 \\dots M$**:\n",
    "\n",
    "   1. **Вычисление псевдо-остатков**\n",
    "\n",
    "      $$\n",
    "      r_i^{(m)} = -\\left.\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right|_{F=F_{m-1}}.\n",
    "      $$\n",
    "\n",
    "      Если используется квадратичная ошибка (MSE): $L = \\tfrac12 (y - F)^2$ $\\implies r_i^{(m)} = y_i - F_{m-1}(x_i)$.\n",
    "        \n",
    "   2. **Обучение базового регрессора** $h_m(x)$ на данных $(x_i, r_i^{(m)})$.\n",
    "   3. **Поиск шагового коэффициента**\n",
    "\n",
    "      $$\n",
    "      \\gamma_m = \\arg\\min_{\\gamma} \\sum_{i=1}^N L\\bigl(y_i, F_{m-1}(x_i) + \\gamma\\,h_m(x_i)\\bigr).\n",
    "      $$\n",
    "   4. **Обновление ансамбля**\n",
    "\n",
    "      $$\n",
    "      F_m(x) = F_{m-1}(x) + \\eta\\,\\gamma_m\\,h_m(x),\n",
    "      $$\n",
    "\n",
    "      где $\\eta\\in (0,1]$ — темп обучения (learning rate).\n",
    "\n",
    "3. **Финальное предсказание**\n",
    "\n",
    "   * Для задач регрессии: $\\hat y = F_M(x)$.\n",
    "   * Для задач классификации: $\\hat y = \\operatorname{sign}\\bigl(F_M(x)\\bigr)$ или через сигмоиду/softmax для вероятностных оценок.\n",
    "\n",
    "\n",
    "\n",
    "**Регуляризация и гиперпараметры**\n",
    "\n",
    "Для предотвращения переобучения и повышения устойчивости применяются:\n",
    "\n",
    "- **Темп обучения** $\\eta$: маленькие значения (например, 0.01–0.1) замедляют обучение.\n",
    "- **Глубина деревьев**: ограничение максимальной глубины или числа листьев.\n",
    "- **Количество итераций** $M$: число деревьев.\n",
    "- **Подвыборка (subsampling)**: случайная выборка $\\alpha N$ объектов без замены при обучении каждого дерева.\n",
    "- **Регуляризация значений листьев**: L1/L2-регуляризация на выходные значения (например, в XGBoost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Современные реализации** градиентного бустинга значительно расширяют базовую идею функционального градиентного спуска, предлагая улучшения по скорости обучения, устойчивости к переобучению и работе с категориальными данными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "XGBoost – одна из наиболее популярных реализаций градиентного бустинга, отличающаяся высокой скоростью и качеством. Основные особенности:\n",
    "\n",
    "* **Второй порядок оптимизации**: в отличие от классического бустинга, XGBoost использует не только градиенты, но и гессианы (вторые производные функции потерь), что позволяет более точно выбирать шаги обновления.\n",
    "* **Регуляризация**: в функцию потерь добавляются L1- и L2-регуляризаторы, что повышает устойчивость к переобучению:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}^{(t)} = \\sum_{i=1}^N L(y_i, \\hat{y}_i^{(t)}) + \\sum_{k=1}^{t} \\Omega(h_k), \\quad \\Omega(h) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^{T} w_j^2\n",
    "  $$\n",
    "\n",
    "  где $T$ — число листьев, $w_j$ — вес узла.\n",
    "* **Пропущенные значения**: встроенные механизмы для автоматического определения направления разветвления при отсутствующих значениях.\n",
    "* **Параллелизм**: обучение и построение гистограмм для разбиения признаков выполняется параллельно.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "XGBoost строит деревья по принципу: «Строим дерево последовательно по уровням до достижения максимальной глубины». Отдельного ограничения на количество вершин нет, так как оно естественным образом получается из ограничения на глубину дерева. В XGBoost деревья «стремятся» быть симметричными по глубине, и в идеале получается полное бинарное дерево, если это не противоречит другим ограничениям (например, ограничению на минимальное количество объектов в листе). Такие деревья обычно являются более устойчивыми к переобучению.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://yastatic.net/s3/education-portal/media/tree_xgboost_e0caf19935_5029e855a6.webp\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LightGBM\n",
    "\n",
    "LightGBM – библиотека от Microsoft, ориентированная на масштабируемость и скорость при сохранении качества модели:\n",
    "\n",
    "* **Leaf-wise рост деревьев**: вместо уровня (level-wise) выбирается лист с наибольшим вкладом в уменьшение функции потерь, что ускоряет сходимость, но может привести к переобучению.\n",
    "* **Гистограммное разбиение**: признаки бинируются (дискретизируются) в фиксированное число корзин (bins), что сокращает потребление памяти и ускоряет поиск разбиений.\n",
    "* **Градиентное сэмплирование (GOSS)**: важные (большие) по градиенту примеры используются всегда, остальные — с вероятностью, что сохраняет информативность и ускоряет обучение.\n",
    "* **Экстремально эффективен на больших и разреженных данных**.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "LightGBM строит деревья по принципу: «На каждом шаге делим вершину с наилучшим скором», а основным критерием остановки выступает максимально допустимое количество вершин в дереве. Это приводит к тому, что деревья получаются несимметричными, то есть поддеревья могут иметь разную глубину – например, левое поддерево может иметь глубину 2, а правое может разрастись до глубины 15.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://yastatic.net/s3/education-portal/media/tree_lightgbm_b27000abd5_dcdf18005a.webp\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CatBoost\n",
    "\n",
    "CatBoost – библиотека от Яндекса, разработанная специально для эффективной обработки категориальных признаков:\n",
    "\n",
    "* **Порядковый бустинг** (ordered boosting): устранение таргет-шифта за счёт особой последовательности построения модели, при которой каждый базовый алгоритм обучается только на части данных, не содержащей целевой переменной для текущего объекта.\n",
    "* **Обработка категориальных признаков**: автоматически применяет методы target encoding и счётчиков с регуляризацией, устраняя необходимость в предварительной подготовке.\n",
    "* **Симметричные деревья**: все разветвления принимаются одновременно на всех уровнях, что ускоряет предсказание и делает модель более устойчивой.\n",
    "* **Поддержка мультиклассовой классификации, нерегулярных задач и использования CPU/GPU.**\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "CatBoost строит деревья по принципу: «Все вершины одного уровня имеют одинаковый предикат». Одинаковые сплиты во всех вершинах одного уровня позволяют избавиться от ветвлений (конструкций if-else) в коде инференса модели с помощью битовых операций и получить более эффективный код, который в разы ускоряет применение модели, в особенности в случае применения на батчах.\n",
    "</div>\n",
    "\n",
    "Кроме этого, такое ограничение на форму дерева выступает в качестве сильной регуляризации, что делает модель более устойчивой к переобучению. Основной критерий остановки, как и в случае XGBoost, – ограничение на глубину дерева. Однако, в отличие от XGBoost, в CatBoost всегда создаются полные бинарные деревья, несмотря на то, что в некоторые поддеревья может не попасть ни одного объекта из обучающей выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://yastatic.net/s3/education-portal/media/tree_catboost_d501cbfe52_d44e880b4e.webp\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_train = 20\n",
    "noise_std = 0.3\n",
    "n_experiments = 50\n",
    "x_grid = np.linspace(0, 2 * np.pi, 100).reshape(-1, 1)\n",
    "y_true = true_function(x_grid.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "tree_predictions = run_experiments(tree_model, n_train, \n",
    "                                   noise_std, x_grid, n_experiments)\n",
    "tree_bias, tree_variance = bias_variance_decomposition(tree_predictions, y_true)\n",
    "\n",
    "xgb_model = XGBClassifier(n_estimators=50,\n",
    "                          max_depth=10,\n",
    "                          subsample=0.8,\n",
    "                          random_state=42)\n",
    "xgb_predictions = run_experiments(xgb_model, n_train, \n",
    "                                  noise_std, x_grid, n_experiments)\n",
    "xgb_bias, xgb_variance = bias_variance_decomposition(xgb_predictions, y_true)\n",
    "\n",
    "model_names = [\"Decision Tree\", \"Boosting\"]\n",
    "bias_values = [tree_bias, xgb_bias]\n",
    "variance_values = [tree_variance, xgb_variance]\n",
    "\n",
    "display_results(model_names, bias_values, variance_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод стекинга объединяет прогнозы нескольких базовых моделей посредством мета-модели. Базовые модели (уровень-0) выдают предсказания, которые затем используются в качестве входных признаков для мета-модели (уровень-1). Итоговая модель может быть записана как:\n",
    "  $$\n",
    "  f_{\\text{stack}}(x) = g\\Bigl( h_1(x), h_2(x), \\dots, h_M(x) \\Bigr),\n",
    "  $$\n",
    "  где $ g $ – обучаемая мета-функция.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Позволяет объединить сильные стороны различных моделей, что зачастую приводит к улучшению качества предсказаний.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/stacking_model.jpg\" alt=\"Stacking\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_train = 20\n",
    "noise_std = 0.3\n",
    "n_experiments = 50\n",
    "x_grid = np.linspace(0, 2 * np.pi, 100).reshape(-1, 1)\n",
    "y_true = true_function(x_grid.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "tree_predictions = run_experiments(tree_model, n_train, noise_std, x_grid, n_experiments)\n",
    "tree_bias, tree_variance = bias_variance_decomposition(tree_predictions, y_true)\n",
    "\n",
    "estimators = [\n",
    "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stack_predictions = run_experiments(stack_model, n_train, noise_std, x_grid, n_experiments)\n",
    "stack_bias, stack_variance = bias_variance_decomposition(stack_predictions, y_true)\n",
    "\n",
    "model_names = [\"Decision Tree\", \"Stacking\"]\n",
    "bias_values = [tree_bias, stack_bias]\n",
    "variance_values = [tree_variance, stack_variance]\n",
    "\n",
    "display_results(model_names, bias_values, variance_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Блэндинг – это ансамблевый приём, который заключается в следующем:\n",
    "- обучаем несколько базовых моделей на тренировочных данных\n",
    "- выделяем небольшую часть данных (holdout) и получаем на ней предсказания базовых моделей\n",
    "- на этих предсказаниях обучаем **мета-модель** (обычно простая логистическая регрессия или линейная регрессия).\n",
    "\n",
    "На тесте комбинируем предсказания базовых моделей через обученную мета-модель.\n",
    "\n",
    "По задумке, мета-модель должна научится комбинировать слабые и разные по поведению базовые модели, но для неё используются **предсказания на holdout наборе**, а не out-of-fold (как при стэкинге). Из-за этого метод менее стабилен на небольших датасетах, ввиду необходимости иметь holdout набор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_rest, X_test, y_rest, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_blend, y_train, y_blend = train_test_split(X_rest, y_rest, test_size=0.25, random_state=42, stratify=y_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=200, random_state=1)),\n",
    "    (\"gb\", GradientBoostingClassifier(n_estimators=200, random_state=2))\n",
    "]\n",
    "\n",
    "for name, model in base_models:\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_preds = []\n",
    "for name, model in base_models:\n",
    "    p = model.predict_proba(X_blend)[:, 1]\n",
    "    blend_preds.append(p.reshape(-1, 1))\n",
    "X_blend_meta = np.hstack(blend_preds)\n",
    "\n",
    "meta = LogisticRegression()\n",
    "meta.fit(X_blend_meta, y_blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for name, model in base_models:\n",
    "    p = model.predict_proba(X_test)[:, 1]\n",
    "    test_preds.append(p.reshape(-1, 1))\n",
    "X_test_meta = np.hstack(test_preds)\n",
    "y_pred_meta = meta.predict_proba(X_test_meta)[:, 1]\n",
    "\n",
    "print(\"ROC AUC (meta):\", roc_auc_score(y_test, y_pred_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in base_models:\n",
    "    p = model.predict_proba(X_test)[:, 1]\n",
    "    print(f\"ROC AUC ({name}):\", roc_auc_score(y_test, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ансамблевые методы для задачи регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ансамблевые методы могут используются и в задачах регрессии. Однако, итоговая аппроксимация исходной зависимости имеет характер разбиения на регионы, внутри которых модель ведёт себя как некоторая простая функция – обычно постоянная или, при специальных модификациях, линейная.\n",
    "\n",
    "Классические регрессионные деревья (например, `DecisionTreeRegressor`) разделяют пространство признаков на непересекающиеся регионы $ R_1, R_2, \\dots, R_n $. В каждом регионе дерево присваивает константное значение, как правило, равное среднему значению целевой переменной для объектов, попавших в этот регион:\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^{n} c_i \\cdot \\mathbb{I}(x \\in R_i)\n",
    "$$\n",
    "где:\n",
    "- $ c_i $ – среднее значение отклика для $ R_i $,\n",
    "- $\\mathbb{I}(x \\in R_i)$ – индикатор, равный 1, если $x$ принадлежит $R_i$.\n",
    "\n",
    "Таким образом, **если базовой моделью является дерево решений с константным прогнозом в листьях**, восстановленная функция будет иметь вид кусочно постоянного приближения. Усредняя, можно искусственно сгладить эти переходы, но принцип регионального разбиения пространства сохраняется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_function(x):\n",
    "    \"\"\"\n",
    "    Истинная функция: sin(x)\n",
    "    \"\"\"\n",
    "    return np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 10\n",
    "X = np.random.uniform(0, 2*np.pi, n_samples)\n",
    "X = np.sort(X)\n",
    "y_true = true_function(X)\n",
    "noise = np.random.normal(0, 0.2, n_samples)\n",
    "y = y_true # + noise\n",
    "\n",
    "X = X.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=20, random_state=42)\n",
    "gb_model = XGBRegressor(n_estimators=20,\n",
    "                        random_state=42,\n",
    "                        objective=\"reg:squarederror\",\n",
    "                        verbosity=0)\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "y_test_pred_gb = gb_model.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
    "mse_gb = mean_squared_error(y_test, y_test_pred_gb)\n",
    "print(f\"MSE (Random Forest Regressor): {mse_rf:.2f}\")\n",
    "print(f\"MSE (XGBoost Regressor): {mse_gb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(0, 2*np.pi, 500).reshape(-1, 1)\n",
    "y_grid_true = true_function(x_grid.ravel())\n",
    "y_grid_rf = rf_model.predict(x_grid)\n",
    "y_grid_gb = gb_model.predict(x_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_grid, y_grid_true, color='black', lw=2, label='Grand Truth')\n",
    "plt.plot(x_grid, y_grid_rf, lw=2, label=f'Random Forest')\n",
    "plt.plot(x_grid, y_grid_gb, lw=2, label=f'XGBoost')\n",
    "plt.scatter(X, y, color='gray', alpha=0.6, label='Data')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Вы можете попрактиковаться на конкретных данных, выполнив задания из\n",
    "<a href=\"../../Workshops/week03_boosting.ipynb\" target=\"_blank\">ноутбука</a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
