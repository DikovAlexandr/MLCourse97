{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 1.2: Данные и их анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные – это важный, а иногда и самый важный, элемент построения моделей машинного обучения.\n",
    "\n",
    "Хотя данные могут быть из разных источников, иметь различные представления, быть сколь угодно разнообразными, важно помнить следующие постулаты:\n",
    "- `Garbage in – garbage out` – качество данных имеет первостепенное значение и может сделать бесполезными все дальнейшие шаги по построении модели.\n",
    "- Если в данных отсутствует закономерность, то модель не сможет её выучить. Именно поэтому, как мы далее изучим, для работы с разными типами данных и моделями мы дополнительно может требовать выполнение некоторых условий (часто формальное) или считаем состоятельными смелые предположения о природе данных.\n",
    "- Модель не обязательно выучит именно ту закономерность, которую предполагает исследователь. Если в данных есть лазейки – модель их обязательно найдет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные можно условно разделить на:\n",
    "- **Cтруктурированные**:табличные данные\n",
    "- **Неструктурированные**: текст, картинки, аудио\n",
    "\n",
    "> Работать с неструктурированными данными зачастую невозможно, сама природа компьютера и реализация вычислений в нем обязывает переходить к векторному представлению отдельного неструктурированного элемента, что сводит данные к матричному (табличному) представлению, а значит структурированному.\n",
    "\n",
    "Табличные данные описываются как числовая матрица X – матрица признаков (feature), где каждой строке соответствует объект, а столбцу – признак (измерение/атрибут объекта). Признаки бывают числовыми (дискретными и непрерывными) и категориальными (номинальные и порядковые)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/types_of_data.png\" alt=\"Типы данных\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чаще всего работа с данными состоит из типовых шагов:\n",
    "- Сбор и агрегация\n",
    "- Первичный анализ данных (EDA)\n",
    "- Предобработка и инженерия признаков\n",
    "- Разбиение на train/validation/test\n",
    "\n",
    "Рассмотрим каждый из них подробнее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сбор и агрегация\n",
    "\n",
    "Для учебных задач обычно используют готовые или даже синтетические датасеты:\n",
    "- [Google Dataset Search](https://datasetsearch.research.google.com)\n",
    "- [Re3Data](https://www.re3data.org)\n",
    "- [World Bank](https://datacatalog.worldbank.org/home)\n",
    "- [Kaggle](https://www.kaggle.com/datasets)\n",
    "- [HuggingFace](https://huggingface.co/datasets)\n",
    "- [GitHub](https://github.com/awesomedata/awesome-public-datasets)\n",
    "- [Humanitarian Data Exchange](https://data.humdata.org)\n",
    "- [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu)\n",
    "- [OpenML](https://www.openml.org/search?type=data)\n",
    "- [FiftyOne Dataset Zoo](https://docs.voxel51.com/dataset_zoo/datasets.html)\n",
    "\n",
    "По компьютерному зрению:\n",
    "- [Open Images](https://storage.googleapis.com/openimages/web/index.html)\n",
    "- [COCO](https://cocodataset.org)\n",
    "- [ImageNet](https://www.image-net.org)\n",
    "- [VisualData](https://visualdata.io)\n",
    "- [Roboflow](https://universe.roboflow.com)\n",
    "- [Visual Genome](https://homes.cs.washington.edu/~ranjay/visualgenome/index.html)\n",
    "\n",
    "По обработке естественного языка:\n",
    "- [Common Crawl](https://commoncrawl.org)\n",
    "- [C4](https://www.tensorflow.org/datasets/catalog/c4)\n",
    "- [The Pile](https://github.com/EleutherAI/the-pile)\n",
    "- [LAION](https://laion.ai)\n",
    "\n",
    "По работе с речью и аудио:\n",
    "- [LibriSpeech / LibriTTS (OpenSLR)](https://www.openslr.org/12)\n",
    "- [Mozilla Common Voice](https://commonvoice.mozilla.org/datasets)\n",
    "- [AudioSet](https://research.google.com/audioset)\n",
    "- [FSD50K](https://annotator.freesound.org/fsd/release/FSD50K)\n",
    "\n",
    "По временным рядам:\n",
    "- [UCR Time Series Archive](https://www.cs.ucr.edu/~eamonn/time_series_data)\n",
    "- [M-Competitions](https://nixtlaverse.nixtla.io/datasetsforecast/m4.html)\n",
    "- [Forecasting Data](https://forecastingdata.org)\n",
    "\n",
    "Кроме того, существуют наборы данных по медицине ([MIMIC](https://physionet.org/content/mimiciv), [MIMIC-CXR](https://physionet.org/content/mimic-cxr)), по геопространственным данным ([EuroSAT](https://www.tensorflow.org/datasets/catalog/eurosat), [BigEarthNet](https://bigearth.net)), по 3D графике ([ShapeNet](https://shapenet.org), [CARLA](https://carla.org/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реальных задачах, данные чаще приходится собрать данные самостоятельно. Реализовывать логику сборки обычно не приходится – если данные нужные, они и так сохраняются.\n",
    "\n",
    "Важно, однако, понимать, что в крупных сервисах данных может быть крайне много и все они должны как-то храниться. хранилища данных существуют.\n",
    "- **Data Lake** – это хранилище, которое позволяет собирать и хранить огромные объёмы сырых данных в их первоначальном формате, что даёт гибкость для дальнейшей обработки, анализа или обучения моделей машинного обучения. Возможность хранения данные в исходном виде и определение их структуры и назначения уже на этапе использования (schema-on-read), несмотря на все преимущества, без должного управления может приводить к сложностям поиска и интерпретируемости.\n",
    "\n",
    "- **DWH (Data WareHouse)** – это централизованное хранилище структурированных данных, оптимизированное для аналитических запросов и отчётности. Обычно строится по схеме \"звезда\" или \"снежинка\", что обеспечивает высокую производительность при выполнении сложных SQL-запросов. Подходит для бизнес-аналитики (BI), где важны согласованность данных, высокая скорость обработки и возможность генерации отчётов. \n",
    "\n",
    "- **DLH (Data LakeHouse)** – это гибридная архитектура, сочетающая лучшие черты Data Lake (возможность хранить сырые данные в любом формате) и Data Warehouse (поддержка транзакционной целостности и аналитических возможностей). В DLH данные могут храниться как в сыром виде, так и в структурированном, при этом поддерживаются ACID-транзакции и оптимизация для аналитических запросов.\n",
    "\n",
    "Кроме того, на практике разные данные оптимальнее хранить в разных местах, поскольку они имеют разные требования по доступности и задержке. Поэтому выделяют:\n",
    "- **Hot**: низкая задержка, высокая частота обращений, быстрый ввод/чтение – PostgreSQL, MySQL, Cassandra, Redis, ElasticSearch.\n",
    "- **Warm**: умеренная задержка и частота обращений – NoSQL, S3, Snowflake.\n",
    "- **Cold**: редкий доступ, упор на стоимость хранения – S3 Glacier.\n",
    "\n",
    "Уходить глубже в классификацию и устройство баз данных не будем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Первичный анализ данных\n",
    "\n",
    "**Exploratory Data Analisys (EDA)** – это термин для обозначения работы по изучению статистических закономерностей в данных. Обычно EDA предполагает расчет статистик и построение визуализаций для того, чтобы исследователю было легче сформулировать гипотезы относительно данных. \n",
    "\n",
    "Само исследование порой можно делать автоматизировано, для этого есть множество инструментов или инструкций. В конечном итоге, все сводится к тому, как агрегировать данные и как именно их лучше отобразить на графике. Есть проекты вроде [этого](https://www.data-to-viz.com/), которые посвящены различным способам визуального анализа данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практический пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ydata-profiling -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from ydata_profiling import ProfileReport\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset('penguins')\n",
    "print(\"Размер исходного датасета:\", penguins.shape)\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProfileReport(penguins, title=\"Profiling Report\").to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предобработка\n",
    "\n",
    "**Предобработка** – процесс, направленный на устранение несоответствий в представлении данных, шума и иных видов аномалий. Чем больше полезного сигнала будет оставлено в данных, тем лучше ML-модель сможет его уловить.\n",
    "\n",
    "Некоторые этапы предобработки:\n",
    "- Работа с пропущенными значениями\n",
    "- Удаление выбросов\n",
    "- Масштабирование признаков\n",
    "- Кодирование категориальных переменных\n",
    "- Кодирование числовых переменных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практический пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = penguins.copy()\n",
    "\n",
    "for col in ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "for col in ['species', 'island', 'sex']:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df, columns):\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "df = remove_outliers_iqr(df, numeric_features)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разбиение на train-val-test\n",
    "\n",
    "Чтобы понять, что модель машинного обучения определила некие закономерности в данных, у нас должна быть возможность оценить это метрически.\n",
    "\n",
    "Для оценки необходимо иметь тестовые данные – они называются **отложенная выборка** - именно они позволяют оценить качество модели путём расчёта метрик на этих данных. Ключевым требованием является то, что модель ни в коем случае не должна обучаться на тестовых данных.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Подробнее узнать про метрики можно в следующем\n",
    "<a href=\"../../Workshops/week01_metrcs.ipynb\" target=\"_blank\">ноутбуке</a>\n",
    "</div>\n",
    "\n",
    "Как мы уже ранее разобрали, разделение данных преследует строго определённые цели:\n",
    "- обучающая выборка используется для обучения модели.\n",
    "- валидационная выборка для подбора гиперпараметров и выбора наилучшей модели в рамках заданного семейства алгоритмов.\n",
    "- тестовая выборка служит для финальной, объективной оценки качества модели или сравнения лучших моделей из различных семейств алгоритмов.\n",
    "\n",
    "> На практике часто используется соотношение 70/10/20 для train, validation и test соответственно, однако это соотношение не является догмой и может варьироваться в зависимости от специфики задачи и объёма доступных данных.\n",
    "\n",
    "От свойств данных и характера задачи выделяют различные стратегии разбиения:\n",
    "- Случайное разбиение (Random Split)\n",
    "- Стратифицированное разбиение (Stratified Split) – гарантирует, что каждая подвыборка будет иметь примерно такое же распределение по целевому признаку, как и исходный набор данных\n",
    "- Временное разбиение (Temporal Split)\n",
    "- Кросс-валидация (Cross-Validation, KFold, StratifiedKFold) – данные разбиваются на $k$ равных частей и модель обучается $k$ раз, каждый раз используя $k-1$ фолдов для обучения и 1 фолд для валидации. фолда.\n",
    "\n",
    "Даже при корректном разбиении данных можно столкнуться с рядом серьезных проблем, среди главных можно выделить:\n",
    "- Дисбаланс классов: самая серьезная из проблем задачи классификации. Есть следующие методы борьбы:\n",
    "    - смещение порога классификации\n",
    "    - ансамблирование\n",
    "    - сэмплирование (undersampling – удаление экземпляров мажорного класса/oversampling – генерация экземпляров миноритарного класса)\n",
    "- Утечка данных\n",
    "- Сдвиг распределения\n",
    "- Недостаточный объем данных"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
