{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 4.1: Аппроксимация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аппроксимация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи\n",
    "Задача аппроксимации формулируется следующим образом: дано множество обучающих данных  \n",
    "$$\n",
    "\\{(x_i, y_i)\\}_{i=1}^{N}, \\quad x_i \\in \\mathbb{R}^d, \\; y_i \\in \\mathbb{R},\n",
    "$$\n",
    "требуется найти функцию $ f: \\mathbb{R}^d \\to \\mathbb{R} $, которая удовлетворяет условию:\n",
    "$$\n",
    "y_i \\approx f(x_i), \\quad \\forall i = 1, \\dots, N.\n",
    "$$\n",
    "\n",
    "Обычно задача решается через минимизацию функции потерь, например, суммы квадратов ошибок:\n",
    "$$\n",
    "\\min_{f \\in \\mathcal{F}} \\; \\sum_{i=1}^{N} \\left( y_i - f(x_i) \\right)^2,\n",
    "$$\n",
    "где $\\mathcal{F}$ – множество рассматриваемых моделей. Такой подход позволяет не только \"подогнать\" модель под данные, но и контролировать обобщающую способность модели через понятия смещения (bias) и дисперсии (variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Классические методы\n",
    "\n",
    "Например, можно построить интерполяционный полином Лагранжа для набора точек $\\{(x_i, y_i)\\}_{i=1}^{N}$ определяется как:\n",
    "$$\n",
    "L(x) = \\sum_{i=0}^{n} y_i \\cdot l_i(x)\n",
    "$$\n",
    "\n",
    "где базисные полиномы $ l_i(x) $ определяются как:\n",
    "$$\n",
    "l_i(x) = \\prod_{\\substack{0 \\le j \\le n \\\\ j \\neq i}} \\frac{x - x_j}{x_i - x_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: float):\n",
    "    return np.sin(x) * x/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagrange_interpolation(x_points, \n",
    "                           y_points, \n",
    "                           x):\n",
    "    \"\"\"\n",
    "    Интерполяция Лагранжа\n",
    "\n",
    "    Аргументы:\n",
    "        x_points (list[float]): Список значений x\n",
    "        y_points (list[float]): Список значений y\n",
    "        x (float): Значение x для интерполяции\n",
    "\n",
    "    Возвращает:\n",
    "        float: Значение интерполяции\n",
    "    \"\"\"\n",
    "    def basis_polynomial(i, x):\n",
    "        \"\"\"\n",
    "        Базисный полином Лагранжа\n",
    "\n",
    "        Аргументы:\n",
    "            i (int): Индекс базисного полинома\n",
    "            x (float): Значение x для интерполяции\n",
    "\n",
    "        Возвращает:\n",
    "            float: Значение базисного полинома\n",
    "        \"\"\"\n",
    "        terms = [\n",
    "            (x - x_points[j]) / (x_points[i] - x_points[j])\n",
    "            for j in range(len(x_points)) if j != i\n",
    "        ]\n",
    "        return np.prod(terms, axis=0)\n",
    "\n",
    "    return (sum(y_points[i] * basis_polynomial(i, x) \n",
    "                for i in range(len(x_points))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "x_points = np.linspace(0, 5, N)\n",
    "y_points = f(x_points)\n",
    "\n",
    "x_new = np.linspace(0, 5, 100)\n",
    "y_lagrange = lagrange_interpolation(x_points, \n",
    "                                    y_points, \n",
    "                                    x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(x_points, y_points, 'bo', label='Data')\n",
    "plt.plot(x_new, y_lagrange, 'r-', label='Interpolation')\n",
    "plt.plot(x_new, f(x_new), 'g--', label='True function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Lagrange interpolation')\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Универсальная аппроксимационная теорема\n",
    "\n",
    "Универсальная аппроксимационная теорема утверждает следующее:\n",
    "\n",
    "Пусть $ f: \\mathbb{R}^n \\to \\mathbb{R} $ – непрерывная функция, определённая на компактном подмножестве $\\mathbb{R}^n$. Тогда для любого $\\epsilon > 0$ существует нейронная сеть с одним скрытым слоем, использующая нелинейную активационную функцию $\\sigma$, такая, что:\n",
    "\n",
    "$$\n",
    "\\left| f(x) - \\sum_{i=1}^{N} a_i \\sigma(\\langle w_i, x \\rangle + b_i) \\right| < \\epsilon\n",
    "$$\n",
    "\n",
    "для всех $ x $ из этого компактного подмножества, где:\n",
    "- $ N $ – количество нейронов в скрытом слое,\n",
    "- $ a_i, w_i, b_i $ – параметры сети (веса и смещения),\n",
    "- $\\sigma$ – нелинейная активационная функция, например, сигмоидальная функция.\n",
    "\n",
    "> Она еще встретится далее в курсе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 16)\n",
    "        self.linear2 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "x_points = np.linspace(0, 5, N)\n",
    "y_points = f(x_points)\n",
    "\n",
    "x_train = torch.tensor(x_points, \n",
    "                       dtype=torch.float32).view(-1, 1)\n",
    "y_train = torch.tensor(y_points, \n",
    "                       dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.tensor(x_new, dtype=torch.float32).view(-1, 1)\n",
    "y_pred_nn = model(x_test).detach().numpy()\n",
    "\n",
    "mse_lagrange = np.mean((y_lagrange - f(x_new))**2)\n",
    "mse_nn = np.mean((y_pred_nn.flatten() - f(x_new))**2)\n",
    "\n",
    "print(f\"MSE Lagrange: {mse_lagrange:.4f}\")\n",
    "print(f\"MSE NN: {mse_nn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(x_points, y_points, 'bo', label='Data')\n",
    "plt.plot(x_test.numpy(), y_pred_nn, 'r-', label='NN')\n",
    "plt.plot(x_test.numpy(), f(x_test.numpy()), 'g--', label='Original function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('NN interpolation')\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейные и нелинейные методы аппроксимации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Линейные методы\n",
    "В линейном случае предполагается, что зависимость между $ x $ и $ y $ может быть описана линейным соотношением:\n",
    "$$\n",
    "f(x) = \\langle \\mathbf{w}, x \\rangle + b,\n",
    "$$\n",
    "где:\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^d$ – вектор коэффициентов,\n",
    "- $b \\in \\mathbb{R}$ – смещение.\n",
    "\n",
    "Обучение модели (например, с использованием метода наименьших квадратов) сводится к решению задачи:\n",
    "$$\n",
    "\\min_{\\mathbf{w}, \\, b} \\; \\sum_{i=1}^{N} \\left( y_i - (\\langle \\mathbf{w}, x_i \\rangle + b) \\right)^2.\n",
    "$$\n",
    "Преимущества линейных моделей:\n",
    "- Простота и хорошая интерпретируемость;\n",
    "- Низкая вычислительная сложность.\n",
    "\n",
    "Ограничение заключается в том, что они не способны адекватно аппроксимировать сложные нелинейные зависимости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нелинейные методы\n",
    "Чтобы учитывать нелинейности, можно использовать преобразование исходных признаков. Одним из подходов является введение нового отображения $\\phi: \\mathbb{R}^d \\to \\mathbb{R}^{d'}$, приводящего к модели:\n",
    "$$\n",
    "f(x) = \\langle \\mathbf{w}, \\phi(x) \\rangle + b.\n",
    "$$\n",
    "\n",
    "Примером является полиномиальная регрессия, где функция $\\phi(x)$ включает полиномы входных признаков:\n",
    "$$\n",
    "\\phi(x) = [1, x, x^2, \\dots, x^p],\n",
    "$$\n",
    "и модель становится:\n",
    "$$\n",
    "f(x) = w_0 + w_1 x + w_2 x^2 + \\dots + w_p x^p.\n",
    "$$\n",
    "Преимущества нелинейных методов:\n",
    "- Гибкость в аппроксимации сложных зависимостей;\n",
    "- Возможность выбора степени нелинейности через параметр $p$.\n",
    "\n",
    "Недостатки:\n",
    "- Рост числа параметров может привести к переобучению;\n",
    "- Снижение интерпретируемости модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleNeuron(nn.Module):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points = np.linspace(-10, 10, 100, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "activations = {\n",
    "    'ReLU': torch.relu,\n",
    "    'Sigmoid': torch.sigmoid,\n",
    "    'Tanh': torch.tanh\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 4))\n",
    "\n",
    "for ax, (name, activation) in zip(axes, activations.items()):\n",
    "    model = SingleNeuron(activation)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(torch.tensor(x_points)).numpy()\n",
    "    \n",
    "    ax.plot(x_points, y_pred, label=f'Activation: {name}')\n",
    "    ax.set_title(f'Activation: {name}')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleNeuron(torch.relu)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}, {param.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим влияние функций активации на полноценной нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, activation, num_neurons):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, num_neurons)\n",
    "        self.activation = activation\n",
    "        self.linear2 = nn.Linear(num_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sin(x) * x / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points = np.linspace(0, 5, 100, dtype=np.float32)\n",
    "y_points = f(x_points)\n",
    "\n",
    "x_train = torch.tensor(x_points, dtype=torch.float32).view(-1, 1)\n",
    "y_train = torch.tensor(y_points, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_plot(activation, num_neurons, ax, title):\n",
    "    model = SimpleNN(activation, num_neurons)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 5000\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    y_pred = model(x_train).detach().numpy()\n",
    "\n",
    "    ax.plot(x_points, y_points, 'g--', label='Оригинальная функция')\n",
    "    ax.plot(x_points, y_pred, 'r-', label='Аппроксимация')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    'ReLU': torch.relu,\n",
    "    'Sigmoid': torch.sigmoid,\n",
    "    'Tanh': torch.tanh\n",
    "}\n",
    "\n",
    "num_neurons_list = [1, 5, 10, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(num_neurons_list), \n",
    "                         ncols=3, figsize=(15, 12))\n",
    "\n",
    "for col, (name, activation) in enumerate(activations.items()):\n",
    "    for row, num_neurons in enumerate(num_neurons_list):\n",
    "        train_and_plot(activation, num_neurons, axes[row, col], \n",
    "                       f'{name} с {num_neurons} neurons')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
