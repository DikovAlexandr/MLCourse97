{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 4.2: Ядерные методы и метод опорных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ядерные методы\n",
    "Ядерные методы опираются на идею явного или неявного переноса данных в пространство более высокой размерности, где зависимость становится линейной. Пусть имеется отображение $\\phi: \\mathbb{R}^d \\to \\mathcal{H}$, тогда модель выглядит как:\n",
    "$$\n",
    "f(x) = \\langle \\mathbf{w}, \\phi(x) \\rangle + b.\n",
    "$$\n",
    "При этом скалярное произведение в пространстве $\\mathcal{H}$ вычисляется с помощью функции ядра $K(x, z)$ по правилу:\n",
    "$$\n",
    "K(x, z) = \\langle \\phi(x), \\phi(z) \\rangle.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Основные типы ядер\n",
    "\n",
    "1. **Линейное ядро**:\n",
    "   $$\n",
    "   K(x, z) = x^T z\n",
    "   $$\n",
    "   - Используется, когда данные линейно разделимы.\n",
    "\n",
    "2. **Полиномиальное ядро**:\n",
    "   $$\n",
    "   K(x, z) = (\\gamma x^T z + r)^d\n",
    "   $$\n",
    "   - $\\gamma$ – коэффициент, $r$ – смещение, $d$ – степень полинома.\n",
    "   - Позволяет моделировать полиномиальные зависимости.\n",
    "\n",
    "3. **Радиальная базисная функция (RBF) или Гауссово ядро**:\n",
    "   $$\n",
    "   K(x, z) = \\exp\\left(-\\frac{\\|x - z\\|^2}{2\\sigma^2}\\right)\n",
    "   $$\n",
    "   - $\\sigma$ – параметр, определяющий ширину гауссиана.\n",
    "   - Хорошо подходит для нелинейно разделимых данных.\n",
    "\n",
    "4. **Сигмоидное ядро**:\n",
    "   $$\n",
    "   K(x, z) = \\tanh(\\gamma x^T z + r)\n",
    "   $$\n",
    "   - Похоже на активационную функцию нейронных сетей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод опорных векторов (SVM)\n",
    "\n",
    "[Подробно](http://www.machinelearning.ru/wiki/images/archive/4/43/20150402092440!Voron-ML-regress-non-slides.pdf)\n",
    "\n",
    "#### Основная идея\n",
    "Для линейно разделимых данных SVM ищет гиперплоскость, которая разделяет данные с максимальным зазором. Гиперплоскость в $ n $-мерном пространстве определяется уравнением:\n",
    "$$\n",
    "\\mathbf{w} \\cdot \\mathbf{x} + b = 0\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $\\mathbf{w}$ – вектор весов,\n",
    "- $b$ – смещение,\n",
    "- $\\mathbf{x}$ – вектор признаков.\n",
    "\n",
    "#### Максимизация зазора\n",
    "SVM стремится максимизировать зазор между двумя классами. Границы зазора определяются уравнениями:\n",
    "$$\n",
    "\\mathbf{w} \\cdot \\mathbf{x} + b = 1\n",
    "$$\n",
    "$$\n",
    "\\mathbf{w} \\cdot \\mathbf{x} + b = -1\n",
    "$$\n",
    "\n",
    "Расстояние между этими двумя гиперплоскостями равно $ \\frac{2}{\\|\\mathbf{w}\\|} $. Таким образом, задача SVM сводится к минимизации $\\|\\mathbf{w}\\|$ при условии, что все точки классифицируются правильно:\n",
    "\n",
    "$$\n",
    "y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i\n",
    "$$\n",
    "\n",
    "где $ y_i $ – метка класса для точки $\\mathbf{x}_i$.\n",
    "\n",
    "\n",
    "#### Отступ (margin)\n",
    "Определяется как:\n",
    "$$\n",
    "M = y_i (\\mathbf{w} \\cdot \\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "Положительный отступ указывает на правильную классификацию, отрицательный – на ошибку.\n",
    "\n",
    "\n",
    "#### Оптимизация\n",
    "Для минимизации ошибок классификации используются различные функции потерь, такие как Hinge Loss:\n",
    "$$\n",
    "L(w, x, y) = \\lambda \\|\\mathbf{w}\\|^2 + \\sum_{i} \\max(0, 1 - y_i (\\mathbf{w} \\cdot \\mathbf{x}_i))\n",
    "$$\n",
    "\n",
    "#### Ядровый трюк\n",
    "\n",
    "Для нелинейно разделимых данных SVM использует ядровую функцию $ K(\\mathbf{x}_i, \\mathbf{x}_j) $, чтобы неявно преобразовать данные в более высокоразмерное пространство, где они могут быть линейно разделимы. Ядровая функция заменяет скалярное произведение $\\mathbf{x}_i \\cdot \\mathbf{x}_j$ в двойственной задаче.\n",
    "\n",
    "Примеры ядер:\n",
    "- Линейное: $ K(\\mathbf{x}, \\mathbf{z}) = \\mathbf{x}^T \\mathbf{z} $\n",
    "- Полиномиальное: $ K(\\mathbf{x}, \\mathbf{z}) = (\\gamma \\mathbf{x}^T \\mathbf{z} + r)^d $\n",
    "- RBF (Гауссово): $ K(\\mathbf{x}, \\mathbf{z}) = \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{z}\\|^2}{2\\sigma^2}\\right) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.linspace(-2, 2, 400)\n",
    "\n",
    "def error(M):\n",
    "    return np.maximum(0, np.sign(-M))\n",
    "\n",
    "def hinge_loss(M):\n",
    "    return np.maximum(0, 1 - M)\n",
    "\n",
    "def perceptron_loss(M):\n",
    "    return np.maximum(0, -M)\n",
    "\n",
    "def squared_hinge_loss(M):\n",
    "    return np.maximum(0, 1 - M)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(M, error(M), label='Error')\n",
    "plt.plot(M, hinge_loss(M), label='Hinge Loss')\n",
    "plt.plot(M, perceptron_loss(M), label='Perceptron Loss')\n",
    "plt.plot(M, squared_hinge_loss(M), label='Squared Hinge Loss')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.axvline(0, color='black', linestyle='--')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Margin')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=100, factor=0.3, noise=0.1, random_state=42)\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, kernel in zip(axes.ravel(), kernels):\n",
    "    clf = svm.SVC(kernel=kernel, gamma='auto')\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(-1.5, 1.5, 100), \n",
    "                         np.linspace(-1.5, 1.5, 100))\n",
    "    \n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "    ax.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')\n",
    "    ax.set_title(f'Kernel: {kernel}')\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод опорных векторов (SVR)\n",
    "Метод опорных векторов для задачи регрессии, называемый Support Vector Regression (SVR), решает следующую задачу оптимизации:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min_{w,b,\\xi_i,\\xi_i^*}\\quad \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{N} (\\xi_i + \\xi_i^*) \\\\\n",
    "&\\text{при условиях:} \\\\\n",
    "& y_i - \\langle w, \\phi(x_i) \\rangle - b \\le \\epsilon + \\xi_i, \\\\\n",
    "& \\langle w, \\phi(x_i) \\rangle + b - y_i \\le \\epsilon + \\xi_i^*, \\\\\n",
    "& \\xi_i, \\xi_i^* \\ge 0, \\quad i = 1,\\dots,N.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Здесь:\n",
    "- $C$ – параметр, регулирующий штраф за ошибки,\n",
    "- $\\epsilon$ задаёт допустимый уровень неточности,\n",
    "- $\\xi_i, \\xi_i^*$ – переменные, вводимые для учета нарушений допустимого отклонения.\n",
    "\n",
    "В SVR цель – минимизировать ошибку предсказания, сохраняя простоту модели. Это достигается путем введения $\\epsilon$ полосы (трубы) вокруг функции, в пределах которой ошибки не учитываются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
    "y = np.sin(X).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=epsilon)\n",
    "svr_poly = SVR(kernel='poly', C=100, degree=3, epsilon=epsilon)\n",
    "svr_linear = SVR(kernel='linear', C=100, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rbf = svr_rbf.fit(X, y).predict(X)\n",
    "y_poly = svr_poly.fit(X, y).predict(X)\n",
    "y_linear = svr_linear.fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 8))\n",
    "plt.scatter(X, y, color='darkorange', \n",
    "            label='Data', edgecolors='k')\n",
    "\n",
    "plt.plot(X, y_rbf, color='navy', lw=2, label='RBF model')\n",
    "plt.fill_between(X.ravel(), \n",
    "                 y_rbf - epsilon, \n",
    "                 y_rbf + epsilon, \n",
    "                 color='navy', alpha=0.2)\n",
    "\n",
    "plt.plot(X, y_poly, color='c', lw=2, label='Polynomial model')\n",
    "plt.fill_between(X.ravel(), \n",
    "                 y_poly - epsilon,\n",
    "                 y_poly + epsilon, \n",
    "                 color='c', alpha=0.2)\n",
    "\n",
    "plt.plot(X, y_linear, color='cornflowerblue', lw=2, label='Linear model')\n",
    "plt.fill_between(X.ravel(), \n",
    "                 y_linear - epsilon, \n",
    "                 y_linear + epsilon, \n",
    "                 color='cornflowerblue', alpha=0.2)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Support Vector Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гауссовские процессы\n",
    "\n",
    "Гауссовские процессы (Gaussian Processes, GP) относятся к непараметрическим байесовским методам аппроксимации. Идея заключается в том, что функция $ f(x) $ рассматривается как случайный процесс, при котором любые конечные наборы значений функции имеют совместное мультиномальное нормальное распределение:\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}\\big(m(x),\\, k(x, x') \\big),\n",
    "$$\n",
    "где:\n",
    "- $m(x)$ – функция среднего (обычно принимается равной нулю: $m(x)=0$),\n",
    "- $k(x, x')$ – ковариационная функция (ядро), задающая степень схожести между точками $x$ и $x'$.\n",
    "\n",
    "Одним из популярных ядер является **радиальная базисная функция (RBF)**, или Гауссово ядро:\n",
    "$$\n",
    "k(x, x') = \\sigma_f^2 \\exp\\Big(-\\frac{\\|x - x'\\|^2}{2l^2}\\Big),\n",
    "$$\n",
    "где:\n",
    "- $\\sigma_f^2$ – дисперсия входных данных,\n",
    "- $l$ – длина масштабирования.\n",
    "\n",
    "**Преимущества Гауссовских процессов:**\n",
    "- Непараметрический характер позволяет гибко адаптироваться к данным.\n",
    "- Возможность получения не только точечного предсказания, но и оценки неопределенности (дисперсии) регрессионной функции.\n",
    "\n",
    "**Ограничения:**\n",
    "- Вычислительная сложность $ \\mathcal{O}(N^3) $ при обучении, что затрудняет применение для больших наборов данных.\n",
    "- Необходимость выбора ядра и его гиперпараметров, что может существенно влиять на качество аппроксимации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sin(x) + 0.1 * np.random.randn(*x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1], [3], [5], [6], [8]])\n",
    "y_train = f(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = (C(1.0, (1e-3, 1e3)) \n",
    "          * RBF(length_scale=1.0, \n",
    "                length_scale_bounds=(1e-2, 1e2)))\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel, \n",
    "                              n_restarts_optimizer=10)\n",
    "\n",
    "gp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "\n",
    "y_pred, sigma = gp.predict(X_test, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(X_test, y_pred, 'r-', label='GP')\n",
    "plt.fill_between(X_test.ravel(), \n",
    "                 y_pred - 1.96 * sigma, \n",
    "                 y_pred + 1.96 * sigma, \n",
    "                 color='lightblue', alpha=0.5, label='95% confidence')\n",
    "plt.scatter(X_train, y_train, color='darkorange', label='Data')\n",
    "plt.title('Gaussian Process Regression')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Интерактивная визуализация гауссовских процессов](https://distill.pub/2019/visual-exploration-gaussian-processes/)\n",
    "\n",
    "[Другой вариант визуализации](https://www.infinitecuriosity.org/vizgp/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
