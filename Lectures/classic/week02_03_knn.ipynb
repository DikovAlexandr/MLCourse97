{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 2.3: Метрические алгоритмы и наивный байесовский классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод ближайших соседей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно с метрическими алгоритмами знакомятся на примере частного случая – **kNN (k ближайших соседей)**. Суть данного метода заключается в том, что на основании известных признаков для объекта находятся соседние, располагающиеся как можно ближе к исходному объекту в некотором пространстве признаков. Затем по этим объектам вычисляется значение недостающего (обычно целевого) признака.\n",
    "\n",
    "Возникает два вопроса:\n",
    "- Каким образом представить объекты в пространстве и что это должно быть за пространство?\n",
    "- Как вычислять расстояние между ними?\n",
    "\n",
    "Ответ на оба этих вопроса зачастую зависит от задачи. Объект обычно проще всего представить как вектор признаков, а для подсчета расстояния используются, например:\n",
    "- евклидово расстояние или расстояние Минковского\n",
    "- косинусное расстояние\n",
    "- манхэттенское расстояние\n",
    "- расстояние Чебышёва\n",
    "\n",
    "> Какую именно метрику использовать в конкретной задаче, обычно можно определить только экспериментально\n",
    "\n",
    "> На практике бывает полезно использовать не сам kNN, а некоторые его приближенные альтернативы. Их реализация представлена, например, в библиотеке векторной базы [FAISS](https://github.com/facebookresearch/faiss).\n",
    "\n",
    "Опишем задачу формально. Пусть задана обучающая выборка пар:\n",
    "$$X^m = \\{(x_1,y_1),\\dots,(x_m,y_m)\\}$$\n",
    "\n",
    "Пусть на множестве объектов задана функция расстояния $\\rho(x,x')$. Чем больше значение этой функции, тем менее схожими являются два объекта $x, x'$.\n",
    "\n",
    "В простейшем случае используется евклидова метрика:\n",
    "$$\\rho(x,x') = \\sqrt {\\sum _{i=1}^{n}(x_{i}-x'_{i})^{2}}$$\n",
    "\n",
    "Для произвольного объекта $u$ расположим объекты обучающей выборки $x_i$ в порядке возрастания расстояний до $u$:\n",
    "$$\\rho(u,x_{1; u}) \\leq  \\rho(u,x_{2; u}) \\leq \\cdots \\leq \\rho(u,x_{m; u})$$\n",
    "\n",
    "где через $x_{i; u}$ обозначается тот объект обучающей выборки, который является $i$-м соседом объекта $u$.\n",
    "\n",
    "Аналогичное обозначение введём и для ответа на $i$-м соседе: $y_{i; u}$.\n",
    "\n",
    "Таким образом, произвольный объект $u$ порождает свою перестановку выборки. В наиболее общем виде алгоритм ближайших соседей есть:\n",
    "$$a(u) = \\mathrm{arg}\\max_{y\\in Y} \\sum_{i=1}^k \\bigl[ y_{i; u}=y \\bigr] w(i,u),$$\n",
    "\n",
    "где $w(i,u)$ – заданная весовая функция, которая оценивает степень важности $i$-го соседа для классификации объекта $u$.\n",
    "\n",
    "Естественно полагать, что эта функция не отрицательна и не возрастает по $i$ (поскольку чем дальше объект, тем меньший вклад он должен вносить в пользу своего класса).\n",
    "\n",
    "По-разному задавая весовую функцию, можно получать различные варианты метода ближайших соседей:\n",
    "- $w(i,u) = [i=1]$ – простейший метод ближайшего соседа;\n",
    "- $w(i,u) = [i\\leq k]$ – метод $k$ ближайших соседей;\n",
    "- $w(i,u) = [i\\leq k] q^i$ – метод $k$ экспоненциально взвешенных ближайших соседей, где предполагается константа $q < 1$\n",
    "\n",
    "Для избежания неоднозначности ответа при классификации в качестве весовой функции $w(i, u)$ используют функцию ядра сглаживания $K(r)$\n",
    "\n",
    "Примеры ядер:\n",
    "- Треугольное: \\quad ${\\displaystyle K(r) = 1 - |r|}$\n",
    "- Параболическое: \\quad ${\\displaystyle K(r) = \\frac{3}{4}(1 - r^2)}$\n",
    "- Трёхкубическое: \\quad ${\\displaystyle K(r) = \\frac{70}{81}(1 - |r|^3)^3}$\n",
    "\n",
    "Алгоритм $k$ ближайших соседей можно обобщить с помощью функции ядра. Рассмотрим два способа, которыми это можно сделать.\n",
    "\n",
    "Метод парзеновского окна фиксированной ширины  $h$:\n",
    "$$w(i,u) = K\\biggl(\\frac{\\rho(u,x_{i; u})}{h}\\biggr)$$\n",
    "\n",
    "Метод парзеновского окна переменной ширины:\n",
    "$$w(i,u) = K\\biggl(\\frac{\\rho(u,x_{i; u})}{\\rho(u,x_{k+1; u})}\\biggr)$$\n",
    "\n",
    "Таким образом классификаторы, полученные при использовании этих методов, можно записать в следующем виде\n",
    "\n",
    "Фиксированной ширины: $$a_h = \\mathrm{arg}\\max_{y\\in Y} \\sum_{i=1}^k \\bigl[ y_{i; u}=y \\bigr] K\\biggl(\\frac{\\rho(u,x_{i; u})}{h}\\biggr)$$\n",
    "\n",
    "Переменной ширины (фиксированное число соседей):\n",
    "$$a_k = \\mathrm{arg}\\max_{y\\in Y} \\sum_{i=1}^k \\bigl[ y_{i; u}=y \\bigr] K\\biggl(\\frac{\\rho(u,x_{i; u})}{\\rho(u,x_{k+1; u})}\\biggr)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практический пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=200, centers=2, cluster_std=10, random_state=42)\n",
    "\n",
    "k = 3\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(X, y)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.colors.ListedColormap(['#FFDD2D', '#428BF9']))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.colors.ListedColormap(['#FFDD2D', '#428BF9']),\n",
    "            s=50, edgecolors='white', linewidth=0.5)\n",
    "plt.title(f'kNN (k={k})')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наивный байесовский классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим алгоритм наивного байесовского классификатора – это простой, но мощный метод классификации, основанный на **теореме Байеса** в предположении о независимости признаков при заданном классе.\n",
    "\n",
    "Вспомним, что теорема Байеса позволяет вычислить апостериорную вероятность класса $y$ при наличии признаков $X_1, X_2, ..., X_p$:\n",
    "$$P(y|X_1, X_2, ..., X_p) = \\frac{P(X_1, X_2, ..., X_p|y) \\cdot P(y)}{P(X_1, X_2, ..., X_p)}$$\n",
    "\n",
    "- $P(y|X_1, X_2, ..., X_p)$ – вероятность того, что объект принадлежит классу \\( y \\) при данных признаках.\n",
    "- $P(X_1, X_2, ..., X_p|y)$ – вероятность наблюдать данные признаки при условии, что объект принадлежит классу $y$.\n",
    "- $P(y)$ – априорная вероятность класса $y$.\n",
    "- $P(X_1, X_2, ..., X_p)$ – вероятность наблюдать данные признаки (нормировочная константа).\n",
    "\n",
    "Классификатор называется **наивным**, потому что делает сильное (и такое еще ни раз встретится) предположение о независимости признаков при заданном классе:\n",
    "$$P(X_1, X_2, ..., X_p|y) = \\prod_{i=1}^p P(X_i|y)$$\n",
    "\n",
    "Это предположение упрощает вычисления, но редко выполняется в реальных данных. Тем не менее, на практике алгоритм часто работает хорошо даже при его нарушении.\n",
    "\n",
    "Для используется для упрощения вычислений, особенно при работе с малыми вероятностями и перехода от перемножения вероятностей, которые могут быть очень маленькими, к их сложению используется логарифмирование (этот переход тоже еще ни раз встретится):\n",
    "$$\\ln P(y|X_1, X_2, ..., X_p) \\propto \\ln P(y) + \\sum_{i=1}^p \\ln P(X_i|y)$$\n",
    "\n",
    "Если в обучающей выборке не встречается какое-то значение признака для класса, то $P(X_i|y) = 0$, и произведение вероятностей станет нулевым, что приведёт к неверной классификации. Для решения этой проблемы используется **сглаживание** (например, сглаживание Лапласа или Лидстоуна). Оно добавляет небольшое значение $\\alpha$ к каждому счёту:\n",
    "$$P(X_i|y) = \\frac{\\text{количество объектов с } X_i \\text{ в классе } y + \\alpha}{\\text{общее количество объектов в классе } y + \\alpha \\cdot n}$$\n",
    "\n",
    "где $n$ — количество возможных значений признака $X_i$.\n",
    "\n",
    "В итоге, алгоритм наивного байесовского классификатора можно записать следующим образом:\n",
    "1. Обучение:\n",
    "   - Вычислить априорные вероятности классов $P(y)$.\n",
    "   - Вычислить условные вероятности признаков для каждого класса $P(X_i|y)$.\n",
    "2. Предсказание:\n",
    "   - Для нового объекта с признаками $X_1, X_2, ..., X_p$ вычислить для каждого класса $y$: $F(y) = \\ln P(y) + \\sum_{i=1}^p \\ln P(X_i|y).$\n",
    "   - Выбрать класс $y^*$, для которого $F(y)$ максимально: $y^* = \\arg\\max_{y \\in Y} F(y).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практический пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'rec.autos',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "    'rec.sport.baseball'\n",
    "]\n",
    "\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    CountVectorizer(\n",
    "        max_features=5000,\n",
    "        stop_words='english',\n",
    "        min_df=2,\n",
    "        max_df=0.8\n",
    "    ),\n",
    "    MultinomialNB(alpha=1.0)\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=newsgroups.target_names,\n",
    "            yticklabels=newsgroups.target_names)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
