{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 4: Аппроксимация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аппроксимация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи\n",
    "Задача аппроксимации формулируется следующим образом: дано множество обучающих данных  \n",
    "$$\n",
    "\\{(x_i, y_i)\\}_{i=1}^{N}, \\quad x_i \\in \\mathbb{R}^d, \\; y_i \\in \\mathbb{R},\n",
    "$$\n",
    "требуется найти функцию $ f: \\mathbb{R}^d \\to \\mathbb{R} $, которая удовлетворяет условию:\n",
    "$$\n",
    "y_i \\approx f(x_i), \\quad \\forall i = 1, \\dots, N.\n",
    "$$\n",
    "\n",
    "Обычно задача решается через минимизацию функции потерь, например, суммы квадратов ошибок:\n",
    "$$\n",
    "\\min_{f \\in \\mathcal{F}} \\; \\sum_{i=1}^{N} \\left( y_i - f(x_i) \\right)^2,\n",
    "$$\n",
    "где $\\mathcal{F}$ — множество рассматриваемых моделей. Такой подход позволяет не только \"подогнать\" модель под данные, но и контролировать обобщающую способность модели через понятия смещения (bias) и дисперсии (variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Классические методы\n",
    "\n",
    "Например, можно построить интерполяционный полином Лагранжа для набора точек $\\{(x_i, y_i)\\}_{i=1}^{N}$ определяется как:\n",
    "$$\n",
    "L(x) = \\sum_{i=0}^{n} y_i \\cdot l_i(x)\n",
    "$$\n",
    "\n",
    "где базисные полиномы $ l_i(x) $ определяются как:\n",
    "$$\n",
    "l_i(x) = \\prod_{\\substack{0 \\le j \\le n \\\\ j \\neq i}} \\frac{x - x_j}{x_i - x_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def f(x: float):\n",
    "    return np.sin(x) * x/5\n",
    "\n",
    "def lagrange_interpolation(x_points, \n",
    "                           y_points, \n",
    "                           x):\n",
    "    \"\"\"\n",
    "    Интерполяция Лагранжа\n",
    "\n",
    "    Аргументы:\n",
    "        x_points (list[float]): Список значений x\n",
    "        y_points (list[float]): Список значений y\n",
    "        x (float): Значение x для интерполяции\n",
    "\n",
    "    Возвращает:\n",
    "        float: Значение интерполяции\n",
    "    \"\"\"\n",
    "    def basis_polynomial(i, x):\n",
    "        \"\"\"\n",
    "        Базисный полином Лагранжа\n",
    "\n",
    "        Аргументы:\n",
    "            i (int): Индекс базисного полинома\n",
    "            x (float): Значение x для интерполяции\n",
    "\n",
    "        Возвращает:\n",
    "            float: Значение базисного полинома\n",
    "        \"\"\"\n",
    "        terms = [\n",
    "            (x - x_points[j]) / (x_points[i] - x_points[j])\n",
    "            for j in range(len(x_points)) if j != i\n",
    "        ]\n",
    "        return np.prod(terms, axis=0)\n",
    "\n",
    "    return (sum(y_points[i] * basis_polynomial(i, x) \n",
    "                for i in range(len(x_points))))\n",
    "\n",
    "N = 10\n",
    "x_points = np.linspace(0, 5, N)\n",
    "y_points = f(x_points)\n",
    "\n",
    "x_new = np.linspace(0, 5, 100)\n",
    "y_lagrange = lagrange_interpolation(x_points, \n",
    "                                    y_points, \n",
    "                                    x_new)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(x_points, y_points, 'bo', label='Исходные точки')\n",
    "plt.plot(x_new, y_lagrange, 'r-', label='Интерполяция Лагранжа')\n",
    "plt.plot(x_new, f(x_new), 'g--', label='Оригинальная функция')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Интерполяция Лагранжа')\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Универсальная аппроксимационная теорема\n",
    "\n",
    "Универсальная аппроксимационная теорема утверждает следующее:\n",
    "\n",
    "Пусть $ f: \\mathbb{R}^n \\to \\mathbb{R} $ — непрерывная функция, определённая на компактном подмножестве $\\mathbb{R}^n$. Тогда для любого $\\epsilon > 0$ существует нейронная сеть с одним скрытым слоем, использующая нелинейную активационную функцию $\\sigma$, такая, что:\n",
    "\n",
    "$$\n",
    "\\left| f(x) - \\sum_{i=1}^{N} a_i \\sigma(\\langle w_i, x \\rangle + b_i) \\right| < \\epsilon\n",
    "$$\n",
    "\n",
    "для всех $ x $ из этого компактного подмножества, где:\n",
    "- $ N $ — количество нейронов в скрытом слое,\n",
    "- $ a_i, w_i, b_i $ — параметры сети (веса и смещения),\n",
    "- $\\sigma$ — нелинейная активационная функция, например, сигмоидальная функция."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 16)\n",
    "        self.linear2 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "N = 10\n",
    "x_points = np.linspace(0, 5, N)\n",
    "y_points = f(x_points)\n",
    "\n",
    "x_train = torch.tensor(x_points, \n",
    "                       dtype=torch.float32).view(-1, 1)\n",
    "y_train = torch.tensor(y_points, \n",
    "                       dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "x_test = torch.tensor(x_new, dtype=torch.float32).view(-1, 1)\n",
    "y_pred_nn = model(x_test).detach().numpy()\n",
    "\n",
    "mse_lagrange = np.mean((y_lagrange - f(x_new))**2)\n",
    "mse_nn = np.mean((y_pred_nn.flatten() - f(x_new))**2)\n",
    "\n",
    "print(f\"MSE Лагранжа: {mse_lagrange:.4f}\")\n",
    "print(f\"MSE Нейронной сети: {mse_nn:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(x_points, y_points, 'bo', label='Исходные точки')\n",
    "plt.plot(x_test.numpy(), y_pred_nn, 'r-', label='Нейронная сеть')\n",
    "plt.plot(x_test.numpy(), np.sin(x_test.numpy()) * x_test.numpy() / 5, \n",
    "         'g--', label='Оригинальная функция sin(x)')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Сравнение методов аппроксимации')\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейные и нелинейные методы аппроксимации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Линейные методы\n",
    "В линейном случае предполагается, что зависимость между $ x $ и $ y $ может быть описана линейным соотношением:\n",
    "$$\n",
    "f(x) = \\langle \\mathbf{w}, x \\rangle + b,\n",
    "$$\n",
    "где:\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^d$ — вектор коэффициентов,\n",
    "- $b \\in \\mathbb{R}$ — смещение.\n",
    "\n",
    "Обучение модели (например, с использованием метода наименьших квадратов) сводится к решению задачи:\n",
    "$$\n",
    "\\min_{\\mathbf{w}, \\, b} \\; \\sum_{i=1}^{N} \\left( y_i - (\\langle \\mathbf{w}, x_i \\rangle + b) \\right)^2.\n",
    "$$\n",
    "Преимущества линейных моделей:\n",
    "- Простота и хорошая интерпретируемость;\n",
    "- Низкая вычислительная сложность.\n",
    "\n",
    "Ограничение заключается в том, что они не способны адекватно аппроксимировать сложные нелинейные зависимости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нелинейные методы\n",
    "Чтобы учитывать нелинейности, можно использовать преобразование исходных признаков. Одним из подходов является введение нового отображения $\\phi: \\mathbb{R}^d \\to \\mathbb{R}^{d'}$, приводящего к модели:\n",
    "$$\n",
    "f(x) = \\langle \\mathbf{w}, \\phi(x) \\rangle + b.\n",
    "$$\n",
    "\n",
    "Примером является полиномиальная регрессия, где функция $\\phi(x)$ включает полиномы входных признаков:\n",
    "$$\n",
    "\\phi(x) = [1, x, x^2, \\dots, x^p],\n",
    "$$\n",
    "и модель становится:\n",
    "$$\n",
    "f(x) = w_0 + w_1 x + w_2 x^2 + \\dots + w_p x^p.\n",
    "$$\n",
    "Преимущества нелинейных методов:\n",
    "- Гибкость в аппроксимации сложных зависимостей;\n",
    "- Возможность выбора степени нелинейности через параметр $p$.\n",
    "\n",
    "Недостатки:\n",
    "- Рост числа параметров может привести к переобучению;\n",
    "- Снижение интерпретируемости модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SingleNeuron(nn.Module):\n",
    "    def __init__(self, activation):\n",
    "        super(SingleNeuron, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "x_points = np.linspace(-10, 10, 100, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "activations = {\n",
    "    'ReLU': torch.relu,\n",
    "    'Sigmoid': torch.sigmoid,\n",
    "    'Tanh': torch.tanh\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 4))\n",
    "\n",
    "for ax, (name, activation) in zip(axes, activations.items()):\n",
    "    model = SingleNeuron(activation)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(torch.tensor(x_points)).numpy()\n",
    "    \n",
    "    ax.plot(x_points, y_pred, label=f'Активация: {name}')\n",
    "    ax.set_title(f'Функция активации: {name}')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleNeuron(torch.relu)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}, {param.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, activation, num_neurons):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, num_neurons)\n",
    "        self.activation = activation\n",
    "        self.linear2 = nn.Linear(num_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x) * x / 5\n",
    "\n",
    "\n",
    "x_points = np.linspace(0, 5, 100, dtype=np.float32)\n",
    "y_points = f(x_points)\n",
    "\n",
    "x_train = torch.tensor(x_points, dtype=torch.float32).view(-1, 1)\n",
    "y_train = torch.tensor(y_points, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "def train_and_plot(activation, num_neurons, ax, title):\n",
    "    model = SimpleNN(activation, num_neurons)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 5000\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    y_pred = model(x_train).detach().numpy()\n",
    "\n",
    "    ax.plot(x_points, y_points, 'g--', label='Оригинальная функция')\n",
    "    ax.plot(x_points, y_pred, 'r-', label='Аппроксимация')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "\n",
    "activations = {\n",
    "    'ReLU': torch.relu,\n",
    "    'Sigmoid': torch.sigmoid,\n",
    "    'Tanh': torch.tanh\n",
    "}\n",
    "\n",
    "num_neurons_list = [1, 5, 10, 50]\n",
    "fig, axes = plt.subplots(nrows=len(num_neurons_list), \n",
    "                         ncols=3, figsize=(15, 12))\n",
    "\n",
    "for col, (name, activation) in enumerate(activations.items()):\n",
    "    for row, num_neurons in enumerate(num_neurons_list):\n",
    "        train_and_plot(activation, num_neurons, axes[row, col], \n",
    "                       f'{name} с {num_neurons} нейронами')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ядерные методы и метод опорных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ядерные методы\n",
    "Ядерные методы опираются на идею явного или неявного переноса данных в пространство более высокой размерности, где зависимость становится линейной. Пусть имеется отображение $\\phi: \\mathbb{R}^d \\to \\mathcal{H}$, тогда модель выглядит как:\n",
    "$$\n",
    "f(x) = \\langle \\mathbf{w}, \\phi(x) \\rangle + b.\n",
    "$$\n",
    "При этом скалярное произведение в пространстве $\\mathcal{H}$ вычисляется с помощью функции ядра $K(x, z)$ по правилу:\n",
    "$$\n",
    "K(x, z) = \\langle \\phi(x), \\phi(z) \\rangle.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Основные типы ядер\n",
    "\n",
    "1. **Линейное ядро**:\n",
    "   $$\n",
    "   K(x, z) = x^T z\n",
    "   $$\n",
    "   - Используется, когда данные линейно разделимы.\n",
    "\n",
    "2. **Полиномиальное ядро**:\n",
    "   $$\n",
    "   K(x, z) = (\\gamma x^T z + r)^d\n",
    "   $$\n",
    "   - $\\gamma$ — коэффициент, $r$ — смещение, $d$ — степень полинома.\n",
    "   - Позволяет моделировать полиномиальные зависимости.\n",
    "\n",
    "3. **Радиальная базисная функция (RBF) или Гауссово ядро**:\n",
    "   $$\n",
    "   K(x, z) = \\exp\\left(-\\frac{\\|x - z\\|^2}{2\\sigma^2}\\right)\n",
    "   $$\n",
    "   - $\\sigma$ — параметр, определяющий ширину гауссиана.\n",
    "   - Хорошо подходит для нелинейно разделимых данных.\n",
    "\n",
    "4. **Сигмоидное ядро**:\n",
    "   $$\n",
    "   K(x, z) = \\tanh(\\gamma x^T z + r)\n",
    "   $$\n",
    "   - Похоже на активационную функцию нейронных сетей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод опорных векторов (SVM)\n",
    "\n",
    "[Подробно](http://www.machinelearning.ru/wiki/images/archive/4/43/20150402092440!Voron-ML-regress-non-slides.pdf)\n",
    "\n",
    "#### Основная идея\n",
    "Для линейно разделимых данных SVM ищет гиперплоскость, которая разделяет данные с максимальным зазором. Гиперплоскость в $ n $-мерном пространстве определяется уравнением:\n",
    "$$\n",
    "\\mathbf{w} \\cdot \\mathbf{x} + b = 0\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $\\mathbf{w}$ — вектор весов,\n",
    "- $b$ — смещение,\n",
    "- $\\mathbf{x}$ — вектор признаков.\n",
    "\n",
    "#### Максимизация зазора\n",
    "SVM стремится максимизировать зазор между двумя классами. Границы зазора определяются уравнениями:\n",
    "$$\n",
    "\\mathbf{w} \\cdot \\mathbf{x} + b = 1\n",
    "$$\n",
    "$$\n",
    "\\mathbf{w} \\cdot \\mathbf{x} + b = -1\n",
    "$$\n",
    "\n",
    "Расстояние между этими двумя гиперплоскостями равно $ \\frac{2}{\\|\\mathbf{w}\\|} $. Таким образом, задача SVM сводится к минимизации $\\|\\mathbf{w}\\|$ при условии, что все точки классифицируются правильно:\n",
    "\n",
    "$$\n",
    "y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i\n",
    "$$\n",
    "\n",
    "где $ y_i $ — метка класса для точки $\\mathbf{x}_i$.\n",
    "\n",
    "\n",
    "#### Отступ (margin)\n",
    "Определяется как:\n",
    "$$\n",
    "M = y_i (\\mathbf{w} \\cdot \\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "Положительный отступ указывает на правильную классификацию, отрицательный — на ошибку.\n",
    "\n",
    "\n",
    "#### Оптимизация\n",
    "Для минимизации ошибок классификации используются различные функции потерь, такие как Hinge Loss:\n",
    "$$\n",
    "L(w, x, y) = \\lambda \\|\\mathbf{w}\\|^2 + \\sum_{i} \\max(0, 1 - y_i (\\mathbf{w} \\cdot \\mathbf{x}_i))\n",
    "$$\n",
    "\n",
    "#### Ядровый трюк\n",
    "\n",
    "Для нелинейно разделимых данных SVM использует ядровую функцию $ K(\\mathbf{x}_i, \\mathbf{x}_j) $, чтобы неявно преобразовать данные в более высокоразмерное пространство, где они могут быть линейно разделимы. Ядровая функция заменяет скалярное произведение $\\mathbf{x}_i \\cdot \\mathbf{x}_j$ в двойственной задаче.\n",
    "\n",
    "Примеры ядер:\n",
    "- Линейное: $ K(\\mathbf{x}, \\mathbf{z}) = \\mathbf{x}^T \\mathbf{z} $\n",
    "- Полиномиальное: $ K(\\mathbf{x}, \\mathbf{z}) = (\\gamma \\mathbf{x}^T \\mathbf{z} + r)^d $\n",
    "- RBF (Гауссово): $ K(\\mathbf{x}, \\mathbf{z}) = \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{z}\\|^2}{2\\sigma^2}\\right) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "M = np.linspace(-2, 2, 400)\n",
    "\n",
    "def error(M):\n",
    "    return np.maximum(0, np.sign(-M))\n",
    "\n",
    "def hinge_loss(M):\n",
    "    return np.maximum(0, 1 - M)\n",
    "\n",
    "def perceptron_loss(M):\n",
    "    return np.maximum(0, -M)\n",
    "\n",
    "def squared_hinge_loss(M):\n",
    "    return np.maximum(0, 1 - M)**2\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(M, error(M), label='Error')\n",
    "plt.plot(M, hinge_loss(M), label='Hinge Loss')\n",
    "plt.plot(M, perceptron_loss(M), label='Perceptron Loss')\n",
    "plt.plot(M, squared_hinge_loss(M), label='Squared Hinge Loss')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.axvline(0, color='black', linestyle='--')\n",
    "plt.title('Функции потерь')\n",
    "plt.xlabel('Отступ (M)')\n",
    "plt.ylabel('Значение функции потерь')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "\n",
    "X, y = make_circles(n_samples=100, factor=0.3, noise=0.1, random_state=42)\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, kernel in zip(axes.ravel(), kernels):\n",
    "    clf = svm.SVC(kernel=kernel, gamma='auto')\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(-1.5, 1.5, 100), \n",
    "                         np.linspace(-1.5, 1.5, 100))\n",
    "    \n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "    ax.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')\n",
    "    ax.set_title(f'Ядро: {kernel}')\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод опорных векторов (SVR)\n",
    "Метод опорных векторов для задачи регрессии, называемый Support Vector Regression (SVR), решает следующую задачу оптимизации:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min_{w,b,\\xi_i,\\xi_i^*}\\quad \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{N} (\\xi_i + \\xi_i^*) \\\\\n",
    "&\\text{при условиях:} \\\\\n",
    "& y_i - \\langle w, \\phi(x_i) \\rangle - b \\le \\epsilon + \\xi_i, \\\\\n",
    "& \\langle w, \\phi(x_i) \\rangle + b - y_i \\le \\epsilon + \\xi_i^*, \\\\\n",
    "& \\xi_i, \\xi_i^* \\ge 0, \\quad i = 1,\\dots,N.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Здесь:\n",
    "- $C$ — параметр, регулирующий штраф за ошибки,\n",
    "- $\\epsilon$ задаёт допустимый уровень неточности,\n",
    "- $\\xi_i, \\xi_i^*$ — переменные, вводимые для учета нарушений допустимого отклонения.\n",
    "\n",
    "В SVR цель — минимизировать ошибку предсказания, сохраняя простоту модели. Это достигается путем введения $\\epsilon$ полосы (трубы) вокруг функции, в пределах которой ошибки не учитываются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "epsilon = 0.1\n",
    "svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=epsilon)\n",
    "svr_poly = SVR(kernel='poly', C=100, degree=3, epsilon=epsilon)\n",
    "svr_linear = SVR(kernel='linear', C=100, epsilon=epsilon)\n",
    "\n",
    "y_rbf = svr_rbf.fit(X, y).predict(X)\n",
    "y_poly = svr_poly.fit(X, y).predict(X)\n",
    "y_linear = svr_linear.fit(X, y).predict(X)\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "plt.scatter(X, y, color='darkorange', \n",
    "            label='Данные', edgecolors='k')\n",
    "\n",
    "plt.plot(X, y_rbf, color='navy', lw=2, label='RBF модель')\n",
    "plt.fill_between(X.ravel(), \n",
    "                 y_rbf - epsilon, \n",
    "                 y_rbf + epsilon, \n",
    "                 color='navy', alpha=0.2)\n",
    "\n",
    "plt.plot(X, y_poly, color='c', lw=2, label='Полиномиальная модель')\n",
    "plt.fill_between(X.ravel(), \n",
    "                 y_poly - epsilon,\n",
    "                 y_poly + epsilon, \n",
    "                 color='c', alpha=0.2)\n",
    "\n",
    "plt.plot(X, y_linear, color='cornflowerblue', lw=2, label='Линейная модель')\n",
    "plt.fill_between(X.ravel(), \n",
    "                 y_linear - epsilon, \n",
    "                 y_linear + epsilon, \n",
    "                 color='cornflowerblue', alpha=0.2)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Support Vector Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гауссовские процессы\n",
    "\n",
    "Гауссовские процессы (Gaussian Processes, GP) относятся к непараметрическим байесовским методам аппроксимации. Идея заключается в том, что функция $ f(x) $ рассматривается как случайный процесс, при котором любые конечные наборы значений функции имеют совместное мультиномальное нормальное распределение:\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}\\big(m(x),\\, k(x, x') \\big),\n",
    "$$\n",
    "где:\n",
    "- $m(x)$ — функция среднего (обычно принимается равной нулю: $m(x)=0$),\n",
    "- $k(x, x')$ — ковариационная функция (ядро), задающая степень схожести между точками $x$ и $x'$.\n",
    "\n",
    "Одним из популярных ядер является **радиальная базисная функция (RBF)**, или Гауссово ядро:\n",
    "$$\n",
    "k(x, x') = \\sigma_f^2 \\exp\\Big(-\\frac{\\|x - x'\\|^2}{2l^2}\\Big),\n",
    "$$\n",
    "где:\n",
    "- $\\sigma_f^2$ — дисперсия входных данных,\n",
    "- $l$ — длина масштабирования.\n",
    "\n",
    "**Преимущества Гауссовских процессов:**\n",
    "- Непараметрический характер позволяет гибко адаптироваться к данным.\n",
    "- Возможность получения не только точечного предсказания, но и оценки неопределенности (дисперсии) регрессионной функции.\n",
    "\n",
    "**Ограничения:**\n",
    "- Вычислительная сложность $ \\mathcal{O}(N^3) $ при обучении, что затрудняет применение для больших наборов данных.\n",
    "- Необходимость выбора ядра и его гиперпараметров, что может существенно влиять на качество аппроксимации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x) + 0.1 * np.random.randn(*x.shape)\n",
    "\n",
    "X_train = np.array([[1], [3], [5], [6], [8]])\n",
    "y_train = f(X_train)\n",
    "\n",
    "kernel = (C(1.0, (1e-3, 1e3)) \n",
    "          * RBF(length_scale=1.0, \n",
    "                length_scale_bounds=(1e-2, 1e2)))\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel, \n",
    "                              n_restarts_optimizer=10)\n",
    "\n",
    "gp.fit(X_train, y_train)\n",
    "\n",
    "X_test = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "\n",
    "y_pred, sigma = gp.predict(X_test, return_std=True)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(X_test, y_pred, 'r-', label='Предсказание')\n",
    "plt.fill_between(X_test.ravel(), \n",
    "                 y_pred - 1.96 * sigma, \n",
    "                 y_pred + 1.96 * sigma, \n",
    "                 color='lightblue', alpha=0.5, label='Доверительный интервал')\n",
    "plt.scatter(X_train, y_train, color='darkorange', label='Обучающие данные')\n",
    "plt.title('Гауссовский процесс для регрессии')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Интерактивная визуализация](https://distill.pub/2019/visual-exploration-gaussian-processes/)\n",
    "\n",
    "[Интерактивная визуализация](https://www.infinitecuriosity.org/vizgp/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
