{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics-Informed Machine Learning (PIML) и Scientific Machine Learning (SciML) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классический подход решения математических задач широко известен и изучен, он включает решение уравнений численными методами (FEM, FDM, FVM), требует сеточной дискретизации, может быть ресурсоёмким.\n",
    "\n",
    "Как было ранее выяснено, модель машинного обучения $f_\\theta(x)$ обучается на данных $\\{(x_i, y_i)\\}$, то есть пусть они будут размеченными, однако без учёта физики - модель не имеет представлений о мире и никак не \"предрасположена\" к пониманию фундаментальных заново физики, а значит может их нарушать. Однако важным свойством является то, что модель хорошо аппроксимирует распределение данных.\n",
    "\n",
    "**Physics-Informed Machine Learning (PIML)**, или **Scientific Machine Learning (SciML)**, объединяет методы машинного обучения с физическими, химическими и инженерными моделями. Цель — получать гибридные модели, которые одновременно:\n",
    "1. Используют **вычислительные преимущества** нейронных сетей и статистических методов.\n",
    "2. Интегрируют **известные физические законы**, отсюда и области применения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics-Informed подход\n",
    "\n",
    "Обучение модели $u_\\theta(\\mathbf{x})$ (например, нейронной сети) с учётом физики через доп. слагаемое в функции потерь.\n",
    "\n",
    "Пусть мы хотим решить уравнение в частных производных (PDE):\n",
    "\n",
    "$$\n",
    "\\mathcal{N}[u(\\mathbf{x})] = 0, \\quad \\mathbf{x}\\in\\Omega,\\quad\n",
    "\\begin{cases}\n",
    "u(\\mathbf{x}) = g(\\mathbf{x}), & \\mathbf{x}\\in\\partial\\Omega_D,\\\\\n",
    "\\mathcal{B}[u] = h(\\mathbf{x}), & \\mathbf{x}\\in\\partial\\Omega_N.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Обучаем нейронную сеть $u_\\theta(\\mathbf{x})$, минимизируя комбинированную функцию потерь:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta)\n",
    "= \\underbrace{\\frac1{N_u}\\sum_{i=1}^{N_u} \\bigl|u_\\theta(\\mathbf{x}_i) - y_i \\bigr|^2}_{\\displaystyle\\text{Data Loss}}\n",
    "+ \\underbrace{\\frac\\lambda{N_r}\\sum_{j=1}^{N_r} \\bigl|\\mathcal{N}[u_\\theta](\\mathbf{x}_j)\\bigr|^2}_{\\displaystyle\\text{Physics Loss}}\n",
    "+ \\underbrace{\\frac\\mu{N_b}\\sum_{k=1}^{N_b} \\bigl|\\mathcal{B}[u_\\theta](\\mathbf{x}_k) - h(\\mathbf{x}_k)\\bigr|^2}_{\\displaystyle\\text{Boundary Loss}},\n",
    "$$\n",
    "\n",
    "где:\n",
    "\n",
    "* $\\{\\mathbf{x}_i,y_i\\}$ — обучающая выборка (если есть наблюдения);\n",
    "* $\\{\\mathbf{x}_j\\}$ — точки, где проверяется PDE;\n",
    "* $\\{\\mathbf{x}_k\\}$ — граничные точки для граничных условий;\n",
    "* $\\lambda,\\mu$ — весовые коэффициенты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Универсальная теорема аппроксимации\n",
    "\n",
    "[Цыбенко, 1989](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%A6%D1%8B%D0%B1%D0%B5%D0%BD%D0%BA%D0%BE)\n",
    "\n",
    "Пусть $ \\sigma\\colon\\mathbb{R}\\to\\mathbb{R} $ - непрерывная сигмоидная функция (то есть $ \\lim_{t\\to-\\infty}\\sigma(t)=0 $, $ \\lim_{t\\to+\\infty}\\sigma(t)=1 $, например $ 1/(1+e^t) $). \n",
    "Тогда для любой непрерывной функции $ f\\in C([0,1]^d) $ и любого $ \\varepsilon>0 $ существует параметризованная функция вида:  \n",
    "$$\n",
    "F(\\mathbf{x})\n",
    "=\\sum_{j=1}^N \\alpha_j\\,\\sigma(\\mathbf{w}_j^\\top \\mathbf{x} + b_j),\n",
    "$$\n",
    "где $N\\in\\mathbb{N}$, $\\alpha_j\\in\\mathbb{R}$, $\\mathbf{w}_j\\in\\mathbb{R}^d$, $b_j\\in\\mathbb{R}$, такая что  \n",
    "$$\n",
    "\\sup_{\\mathbf{x}\\in[0,1]^d}\\bigl|F(\\mathbf{x}) - f(\\mathbf{x})\\bigr| < \\varepsilon.\n",
    "$$\n",
    "\n",
    "> Теорема утверждает, что искусственная нейронная сеть с одним скрытым слоем может аппроксимировать любую непрерывную функцию многих переменных с любой наперед заданной точностью. Условиями являются достаточное количество нейронов скрытого слоя, \"удачный\" подбор параметров этого слоя. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теорема суперпозиции Колмогорова–Арнольда\n",
    "\n",
    "[Колмогоров, Арнольд, 1957](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%9A%D0%BE%D0%BB%D0%BC%D0%BE%D0%B3%D0%BE%D1%80%D0%BE%D0%B2%D0%B0_%E2%80%94_%D0%90%D1%80%D0%BD%D0%BE%D0%BB%D1%8C%D0%B4%D0%B0)\n",
    "\n",
    "Любая непрерывная функция $f\\colon[0,1]^d\\to\\mathbb{R}$ может быть представлена в виде  \n",
    "$$\n",
    "f(\\mathbf{x})\n",
    "=\\sum_{q=0}^{2d}\\Phi_q\\!\\Bigl(\\sum_{p=1}^d\\psi_{p,q}(x_p)\\Bigr),\n",
    "$$\n",
    "где $\\{\\psi_{p,q}\\}$ и $\\{\\Phi_q\\}$ - непрерывные функции одной переменной.\n",
    "\n",
    "> Теорема утверждает, что каждая многомерная непрерывная функция может быть представлена в виде суперпозиции непрерывных функций одной переменной. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Характеризация функций активации\n",
    "\n",
    "[Пинкуc, 1999](https://pinkus.net.technion.ac.il/files/2021/02/acta.pdf)\n",
    "\n",
    "Пусть $K\\subset\\mathbb{R}^d$ - компакт, и рассмотрим класс функций\n",
    "\n",
    "$$\n",
    "\\mathcal{H}_\\sigma \\;=\\;\n",
    "\\Bigl\\{\\,h(\\mathbf{x}) = \\sum_{j=1}^N \\alpha_j\\,\\sigma(\\mathbf{w}_j^\\top\\mathbf{x} + b_j)\\;\\Big|\\; \n",
    "N\\in\\mathbb{N},\\;\\alpha_j\\in\\mathbb{R},\\;\\mathbf{w}_j\\in\\mathbb{R}^d,\\;b_j\\in\\mathbb{R}\n",
    "\\Bigr\\}.\n",
    "$$\n",
    "\n",
    "Обозначим через $C(K)$ пространство всех непрерывных функций на $K$ с нормой $\\|f\\|_\\infty = \\sup_{\\mathbf{x}\\in K}|f(\\mathbf{x})|$.\n",
    "\n",
    "Пусть $\\sigma\\colon\\mathbb{R}\\to\\mathbb{R}$ - непрерывная функция. Тогда следующие утверждения эквивалентны:\n",
    "1. $\\sigma$ **не совпадает ни с каким полиномом** на каком-либо открытом интервале $\\mathcal{I}\\subset\\mathbb{R}$.\n",
    "2. Класс $\\mathcal{H}_\\sigma$ **плотен** в $C(K)$ для любого компакта $K\\subset\\mathbb{R}^d$; то есть для любой $f\\in C(K)$ и любого $\\varepsilon>0$ существует $h\\in\\mathcal{H}_\\sigma$ с\n",
    "$$\n",
    "\\|f - h\\|_\\infty < \\varepsilon.\n",
    "$$\n",
    "\n",
    "\n",
    "> Если $\\sigma$ на некотором интервале точно совпадает с полиномом степени $m$, то все выражения $ \\sigma(\\mathbf{w}^\\top\\mathbf{x}+b) $ являются полиномиальными функциями переменной $ \\mathbf{w}^\\top\\mathbf{x}+b $. Линейные комбинации таких полиномов не могут аппроксимировать, скажем, непрерывные функции, не лежащие в пространстве полиномов степени $m$.\n",
    "\n",
    "> Если же $\\sigma$ нигде не полиномиальная, то с помощью классических конструкций теории аппроксимации можно показать, что $\\mathcal{H}_\\sigma$ содержит достаточно функций, чтобы покрыть всё $C(K)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того:\n",
    "\n",
    "- [Теорема Вейерштрасса-Стоуна](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%92%D0%B5%D0%B9%D0%B5%D1%80%D1%88%D1%82%D1%80%D0%B0%D1%81%D1%81%D0%B0_%E2%80%94_%D0%A1%D1%82%D0%BE%D1%83%D0%BD%D0%B0)\n",
    "- [Теормеа Хорника](https://www.cs.cmu.edu/~epxing/Class/10715/reading/Kornick_et_al.pdf)\n",
    "- [Теорема Яроцкого](https://arxiv.org/abs/1610.01145)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теорема сходимости PINN\n",
    "\n",
    "[Mishra & Molinaro, 2022](https://arxiv.org/abs/2006.16144)\n",
    "\n",
    "Рассмотрим PINN $u_\\theta$, обучаемую для решения корректной задачи, при наличии достаточного числа точек коллокации и нейронная сеть удовлетворяет условиям универсальной теоремы аппроксимации. При регулярности истинного решения $u$ существует набор весов $\\theta$, при котором общий для функции потерь справедливо:  \n",
    "$$\n",
    "\\mathcal{L}(\\theta)\n",
    "=\\frac1{N_u}\\sum_i|u_\\theta(x_i)-u(x_i)|^2\n",
    "+\\frac{\\lambda}{N_r}\\sum_j|\\mathcal{N}[u_\\theta](x_j)|^2\n",
    "$$\n",
    "стремится к нулю при $N_r\\to\\infty$, и при этом $\\|u_\\theta - u\\|_{L^2}$ можно связать с величиной $\\mathcal{L}(\\theta)$ и размером сети.\n",
    "\n",
    "> То есть, при достаточной обобщающей и аппроксимирующей возможностях сети (архитектура), малости потерь (оптимизатор) и корректной постановке задачи (само решение), PINN сходится к решению дифференциального уравнения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно отметить, что теорема, помимо прочего, вводит источники ошибок полученной аппроксимации конкретного решения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/errors.jpg\" alt=\"Компоненты ошибок\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Основные модели\n",
    "- **Physics-Informed Neural Networks (PINNs)**\n",
    "- **Neural Operators**\n",
    "- **DeepONet**\n",
    "\n",
    "##### Примеры\n",
    "\n",
    "- **Ускорение численных симуляций:** замена тяжелых итераций ML-аппроксимацией локальных шагов решения.\n",
    "- **Нейронные операторы и генерализация:** обучение сразу на целый класс задач.\n",
    "- **Неопределённость и верификация:** учет неопределенности данных и физической модели.\n",
    "- **Мультиизмерные и многофизичные задачи:** совмещение различных областей физики.\n",
    "- **Интеграция с экспериментальными данными:** адаптивное обучение при поступлении новых измерений.\n",
    "\n",
    "##### Области применения\n",
    "\n",
    "- **Гидродинамика и аэродинамика:** моделирование течений, турбулентности, оптимизация крыльев.\n",
    "- **Метеорология и климатология:** прогнозирование погоды, оценка риска экстремальных явлений.\n",
    "- **Биомедицинское моделирование:** распространение заболеваний, динамика лекарств, биомеханика тканей.\n",
    "- **Материаловедение:** предсказание свойств новых сплавов и композитов, фазовые переходы.\n",
    "- **Электроника и энергетика:** оптимизация ячеек топливных элементов, управление сетью.\n",
    "\n",
    "> Подробнее можно о конкретных примера можно узнать в [видео](https://www.youtube.com/watch?v=O09lu-lsLhU&ab_channel=MSU_AI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решение ОДУ с помощью нейронной сети\n",
    "\n",
    "Рассмотрим простейшее ОДУ первого порядка с начальным условием:\n",
    "$$\n",
    "\\begin{cases}\n",
    "u'(x) + u(x) = 0, & x \\in [0, X_{\\max}],\\\\\n",
    "u(0) = 1.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Аналитическое решение  \n",
    "$$\n",
    "u(x) = e^{-x}.\n",
    "$$\n",
    "\n",
    "Общая функция потерь состоит из двух слагаемых:\n",
    "1. **Physics loss** — MSE по коллокационным точкам \\(\\{x_i\\}\\):\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{phys}}(\\theta)\n",
    "= \\frac{1}{N_f} \\sum_{i=1}^{N_f} \\bigl(u_\\theta'(x_i) + u_\\theta(x_i)\\bigr)^2.\n",
    "$$\n",
    "2. **Initial condition loss** — MSE по начальному условию:\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{IC}}(\\theta)\n",
    "= \\bigl(u_\\theta(0) - 1\\bigr)^2.\n",
    "$$\n",
    "\n",
    "Итоговая функция:\n",
    "$$\n",
    "\\mathcal{L}(\\theta)\n",
    "= \\mathcal{L}_{\\mathrm{phys}}(\\theta)\n",
    "+ \\lambda \\,\\mathcal{L}_{\\mathrm{IC}}(\\theta),\n",
    "$$\n",
    "где $\\lambda$ — взвешивающий коэффициент (например, $\\lambda=1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.integrate import solve_ivp\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    \"\"\"\n",
    "    Физически информированная нейронная сеть для приближенного решения ОДУ.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(PINN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(1, 20)\n",
    "        self.hidden2 = nn.Linear(20, 20)\n",
    "        self.hidden3 = nn.Linear(20, 20)\n",
    "        self.out = nn.Linear(20, 1)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden1(x))\n",
    "        x = self.activation(self.hidden2(x))\n",
    "        x = self.activation(self.hidden3(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, x_colloc, x_ic):\n",
    "    \"\"\"\n",
    "    Вычисляет функцию потерь для PINN.\n",
    "\n",
    "    Аргументы:\n",
    "        model (nn.Module): обучаемая модель PINN.\n",
    "        x_colloc (torch.Tensor): тензор коллокационных точек.\n",
    "        x_ic (torch.Tensor): тензор для начального условия.\n",
    "\n",
    "    Возвращает:\n",
    "        loss (torch.Tensor): суммарное значение функции потерь.\n",
    "    \"\"\"\n",
    "    x_colloc.requires_grad = True\n",
    "    u_pred = model(x_colloc)\n",
    "\n",
    "    grad_outputs = torch.ones_like(u_pred)\n",
    "    du_dx = torch.autograd.grad(\n",
    "        u_pred, x_colloc,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # f(x) = u'(x) + u(x)\n",
    "    f = du_dx + u_pred\n",
    "    mse_pde = torch.mean(f ** 2)\n",
    "\n",
    "    # u(0) = 1\n",
    "    u_ic_pred = model(x_ic)\n",
    "    mse_ic = torch.mean((u_ic_pred - 1.0) ** 2)\n",
    "\n",
    "    loss = mse_pde + mse_ic\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn, \n",
    "          optimizer: optim, \n",
    "          num_epochs: int, \n",
    "          x_colloc: torch.Tensor, \n",
    "          x_ic: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Обучает модель PINN.\n",
    "\n",
    "    Аргументы:\n",
    "        model (nn.Module): обучаемая модель PINN.\n",
    "        optimizer (torch.optim.Optimizer): оптимизатор.\n",
    "        num_epochs (int): количество эпох обучения.\n",
    "        x_colloc (torch.Tensor): коллокационные точки.\n",
    "        x_ic (torch.Tensor): точка начального условия.\n",
    "    \"\"\"\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training epochs\"):\n",
    "        optimizer.zero_grad()\n",
    "        loss = compute_loss(model, x_colloc, x_ic)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch:5d}, Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PINN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "batch_size = 100\n",
    "\n",
    "x_colloc = torch.linspace(0, 1, batch_size).view(-1, 1)\n",
    "x_ic = torch.tensor([[0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, num_epochs, x_colloc, x_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(0, 1, 100).view(-1, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_pred = model(x_test).cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_np = x_test.cpu().numpy().flatten()\n",
    "u_true = np.exp(-x_test_np)\n",
    "\n",
    "sol = solve_ivp(lambda t, u: -u, [0, 1], [1.0], t_eval=x_test_np)\n",
    "u_numerical = sol.y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_test_np, u_true, 'b-', label='Analytic')\n",
    "plt.plot(x_test_np, u_pred, 'r--', label='PINN')\n",
    "plt.plot(x_test_np, u_numerical, 'g-.', label='SciPy')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(x)')\n",
    "plt.title(\"u'(x) + u(x) = 0\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение PINN для решения уравнения теплопроводности\n",
    "\n",
    "Решим задачу одномерной теплопроводности с граничными условиями вида первого рода. Рассматриваем линейное уравнение теплопроводности и постановку задачи в целом:\n",
    "$$\n",
    "\\begin{cases}\n",
    "u_t(x,t) - \\alpha\\,u_{xx}(x,t) = 0, & (x,t) \\in (0,1)\\times(0,T],\\\\\n",
    "u(x,0) = \\sin(\\pi x), & x \\in [0,1],\\\\\n",
    "u(0,t) = u(1,t) = 0, & t \\in [0,T],\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "где $\\alpha = 0.1$, $T = 2$.\n",
    "\n",
    "Аналитическое решение этой задачи:\n",
    "$$\n",
    "u(x,t) = \\sin(\\pi x)\\,\\exp\\bigl(-\\alpha\\,\\pi^2\\,t\\bigr).\n",
    "$$\n",
    "\n",
    "Функция потерь для такой задачи будет иметь одно дополнительное слагаемое:\n",
    "$$\n",
    "\\mathcal{L}(\\theta)\n",
    "= \\underbrace{\\frac{1}{N_f}\\sum_{i=1}^{N_f} \\bigl(u_{\\theta,t}\n",
    "- \\alpha\\,u_{\\theta,xx}\\bigr)^2}_{\\displaystyle\\text{Physics loss}}\n",
    "+ \\underbrace{\\frac{1}{N_b}\\sum_{j=1}^{N_b} \\bigl(u_\\theta(x_j, t_j)\\bigr)^2}_{\\displaystyle\\text{Boundary loss}}\n",
    "+ \\underbrace{\\frac{1}{N_0}\\sum_{k=1}^{N_0} \\bigl(u_\\theta(x_k,0) - \\sin(\\pi x_k)\\bigr)^2}_{\\displaystyle\\text{Initial condition loss}}.\n",
    "$$\n",
    "\n",
    "Для примера воспользуется библиотекой [DeepXDE](https://deepxde.readthedocs.io/en/latest/), одной из [первых](https://arxiv.org/abs/1907.04502) библиотек поддерживающих работу с PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepxde -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepxde as dde\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_eq(x, u):\n",
    "    du_t = dde.grad.jacobian(u, x, i=0, j=1) # du/dt\n",
    "    du_xx = dde.grad.hessian(u, x, i=0, j=0) # du/dx\n",
    "    return du_t - 0.1 * du_xx # α = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x ∈ [0, 1], t ∈ [0, 2]\n",
    "geom = dde.geometry.Interval(0, 1)\n",
    "timedomain = dde.geometry.TimeDomain(0, 1)\n",
    "geomtime = dde.geometry.GeometryXTime(geom, timedomain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boundary condition: u = 0 на границе области (при любом t)\n",
    "bc = dde.icbc.DirichletBC(geomtime, \n",
    "                          lambda x: 0, lambda _, on_boundary: on_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial condition: u(x,0) = sin(πx)\n",
    "ic = dde.icbc.IC(\n",
    "    geomtime,\n",
    "    lambda x: np.sin(np.pi * x[:, 0:1]),\n",
    "    lambda _, on_initial: on_initial,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras style\n",
    "net = dde.nn.FNN([2, 20, 20, 1], \"tanh\", \"Glorot normal\")\n",
    "data = dde.data.TimePDE(geomtime, heat_eq, [bc, ic], \n",
    "                        num_domain=1000, \n",
    "                        num_boundary=100, \n",
    "                        num_initial=100, \n",
    "                        num_test=1000)\n",
    "model = dde.Model(data, net)\n",
    "\n",
    "model.compile(\"adam\", lr=0.001)\n",
    "losshistory, train_state = model.train(epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 101)\n",
    "t = np.linspace(0, 2, 101)\n",
    "X, T = np.meshgrid(x, t)\n",
    "XT = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "\n",
    "u_pred = model.predict(XT)\n",
    "u_pred = u_pred.reshape(X.shape)\n",
    "\n",
    "u_exact = np.sin(np.pi * X) * np.exp(-0.1 * (np.pi**2) * T)\n",
    "\n",
    "error = np.abs(u_exact - u_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "contour1 = ax1.contourf(X, T, u_pred, 100, cmap=\"viridis\")\n",
    "ax1.set_title(\"PINN\")\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"t\")\n",
    "fig.colorbar(contour1, ax=ax1)\n",
    "\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "contour2 = ax2.contourf(X, T, u_exact, 100, cmap=\"viridis\")\n",
    "ax2.set_title(\"Analytic\")\n",
    "ax2.set_xlabel(\"x\")\n",
    "ax2.set_ylabel(\"t\")\n",
    "fig.colorbar(contour2, ax=ax2)\n",
    "\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "contour3 = ax3.contourf(X, T, error, 100, cmap=\"hot\")\n",
    "ax3.set_title(\"Absolute error\")\n",
    "ax3.set_xlabel(\"x\")\n",
    "ax3.set_ylabel(\"t\")\n",
    "fig.colorbar(contour3, ax=ax3)\n",
    "\n",
    "plt.suptitle(\"PINN\", \n",
    "             fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
