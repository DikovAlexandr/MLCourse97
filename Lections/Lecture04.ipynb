{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 4: Аппроксимация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аппроксимация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи\n",
    "Задача аппроксимации формулируется следующим образом: дано множество обучающих данных  \n",
    "$$\n",
    "\\{(x_i, y_i)\\}_{i=1}^{N}, \\quad x_i \\in \\mathbb{R}^d, \\; y_i \\in \\mathbb{R},\n",
    "$$\n",
    "требуется найти функцию $ f: \\mathbb{R}^d \\to \\mathbb{R} $, которая удовлетворяет условию:\n",
    "$$\n",
    "y_i \\approx f(x_i), \\quad \\forall i = 1, \\dots, N.\n",
    "$$\n",
    "\n",
    "Обычно задача решается через минимизацию функции потерь, например, суммы квадратов ошибок:\n",
    "$$\n",
    "\\min_{f \\in \\mathcal{F}} \\; \\sum_{i=1}^{N} \\left( y_i - f(x_i) \\right)^2,\n",
    "$$\n",
    "где $\\mathcal{F}$ — множество рассматриваемых моделей. Такой подход позволяет не только \"подогнать\" модель под данные, но и контролировать обобщающую способность модели через понятия смещения (bias) и дисперсии (variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейные и нелинейные методы аппроксимации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Линейные методы\n",
    "В линейном случае предполагается, что зависимость между $ x $ и $ y $ может быть описана линейным соотношением:\n",
    "$$\n",
    "f(x) = \\langle \\mathbf{w}, x \\rangle + b,\n",
    "$$\n",
    "где:\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^d$ — вектор коэффициентов,\n",
    "- $b \\in \\mathbb{R}$ — смещение.\n",
    "\n",
    "Обучение модели (например, с использованием метода наименьших квадратов) сводится к решению задачи:\n",
    "$$\n",
    "\\min_{\\mathbf{w}, \\, b} \\; \\sum_{i=1}^{N} \\left( y_i - (\\langle \\mathbf{w}, x_i \\rangle + b) \\right)^2.\n",
    "$$\n",
    "Преимущества линейных моделей:\n",
    "- Простота и хорошая интерпретируемость;\n",
    "- Низкая вычислительная сложность.\n",
    "\n",
    "Ограничение заключается в том, что они не способны адекватно аппроксимировать сложные нелинейные зависимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нелинейные методы\n",
    "Чтобы учитывать нелинейности, можно использовать преобразование исходных признаков. Одним из подходов является введение нового отображения $\\phi: \\mathbb{R}^d \\to \\mathbb{R}^{d'}$, приводящего к модели:\n",
    "$$\n",
    "f(x) = \\langle \\mathbf{w}, \\phi(x) \\rangle + b.\n",
    "$$\n",
    "\n",
    "Примером является полиномиальная регрессия, где функция $\\phi(x)$ включает полиномы входных признаков:\n",
    "$$\n",
    "\\phi(x) = [1, x, x^2, \\dots, x^p],\n",
    "$$\n",
    "и модель становится:\n",
    "$$\n",
    "f(x) = w_0 + w_1 x + w_2 x^2 + \\dots + w_p x^p.\n",
    "$$\n",
    "Преимущества нелинейных методов:\n",
    "- Гибкость в аппроксимации сложных зависимостей;\n",
    "- Возможность выбора степени нелинейности через параметр $p$.\n",
    "\n",
    "Недостатки:\n",
    "- Рост числа параметров может привести к переобучению;\n",
    "- Снижение интерпретируемости модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ядерные методы и метод опорных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ядерные методы\n",
    "Ядерные методы опираются на идею явного или неявного переноса данных в пространство более высокой размерности, где зависимость становится линейной. Пусть имеется отображение $\\phi: \\mathbb{R}^d \\to \\mathcal{H}$, тогда модель выглядит как:\n",
    "$$\n",
    "f(x) = \\langle \\mathbf{w}, \\phi(x) \\rangle + b.\n",
    "$$\n",
    "При этом скалярное произведение в высокоразмерном пространстве $\\mathcal{H}$ вычисляется с помощью функции ядра $K(x, z)$ по правилу:\n",
    "$$\n",
    "K(x, z) = \\langle \\phi(x), \\phi(z) \\rangle.\n",
    "$$\n",
    "Пример метода — Kernel Ridge Regression, решающий задачу:\n",
    "$$\n",
    "\\min_{\\boldsymbol\\alpha} \\; \\| \\mathbf{y} - K \\boldsymbol\\alpha \\|^2 + \\lambda\\, \\boldsymbol\\alpha^T K \\boldsymbol\\alpha,\n",
    "$$\n",
    "где $K$ — матрица ядра, а $\\lambda$ — коэффициент регуляризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод опорных векторов (SVR)\n",
    "Метод опорных векторов для задачи регрессии, называемый Support Vector Regression (SVR), решает следующую задачу оптимизации:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min_{w,b,\\xi_i,\\xi_i^*}\\quad \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{N} (\\xi_i + \\xi_i^*) \\\\\n",
    "&\\text{при условиях:} \\\\\n",
    "& y_i - \\langle w, \\phi(x_i) \\rangle - b \\le \\epsilon + \\xi_i, \\\\\n",
    "& \\langle w, \\phi(x_i) \\rangle + b - y_i \\le \\epsilon + \\xi_i^*, \\\\\n",
    "& \\xi_i, \\xi_i^* \\ge 0, \\quad i = 1,\\dots,N.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Здесь:\n",
    "- $C$ — параметр, регулирующий штраф за ошибки,\n",
    "- $\\epsilon$ задаёт допустимый уровень неточности,\n",
    "- $\\xi_i, \\xi_i^*$ — переменные, вводимые для учета нарушений допустимого отклонения.\n",
    "\n",
    "Решение задачи дает функцию регрессии в виде:\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^{N} (\\alpha_i - \\alpha_i^*)\\, K(x_i, x) + b,\n",
    "$$\n",
    "где $\\alpha_i, \\alpha_i^*$ — двойственные переменные оптимизации. Использование ядрового трюка позволяет эффективно работать в высокоразмерном пространстве без явного вычисления $\\phi(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гауссовские процессы\n",
    "\n",
    "Гауссовские процессы (Gaussian Processes, GP) относятся к непараметрическим байесовским методам аппроксимации. Идея заключается в том, что функция $ f(x) $ рассматривается как случайный процесс, при котором любые конечные наборы значений функции имеют совместное мультиномальное нормальное распределение:\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}\\big(m(x),\\, k(x, x') \\big),\n",
    "$$\n",
    "где:\n",
    "- $m(x)$ — функция среднего (обычно принимается равной нулю: $m(x)=0$),\n",
    "- $k(x, x')$ — ковариационная функция (ядро), задающая степень схожести между точками $x$ и $x'$.\n",
    "\n",
    "Одним из популярных ядер является **радиальная базисная функция (RBF)**, или Гауссово ядро:\n",
    "$$\n",
    "k(x, x') = \\sigma_f^2 \\exp\\Big(-\\frac{\\|x - x'\\|^2}{2l^2}\\Big),\n",
    "$$\n",
    "где:\n",
    "- $\\sigma_f^2$ — дисперсия сигнала,\n",
    "- $l$ — длина масштабирования.\n",
    "\n",
    "При наличии обучающих данных $ (X, y) $ предсказание для новой точки $ x_* $ производится посредством вычисления апостериорного распределения:\n",
    "- **Среднее предсказание:**\n",
    "  $$\n",
    "  \\bar{f}_* = k(x_*, X)\\left[ K(X,X) + \\sigma_n^2 I \\right]^{-1} y,\n",
    "  $$\n",
    "- **Дисперсия предсказания:**\n",
    "  $$\n",
    "  \\text{Var}(f_*) = k(x_*, x_*) - k(x_*, X)\\left[ K(X,X) + \\sigma_n^2 I \\right]^{-1} k(X, x_*),\n",
    "  $$\n",
    "где $ \\sigma_n^2 $ — дисперсия шума, а $ K(X, X) $ — ковариационная матрица, составленная из $ k(x_i, x_j) $ для обучающих точек.\n",
    "\n",
    "**Преимущества Гауссовских процессов:**\n",
    "- Непараметрический характер позволяет гибко адаптироваться к данным.\n",
    "- Возможность получения не только точечного предсказания, но и оценки неопределенности (дисперсии) регрессионной функции.\n",
    "\n",
    "**Ограничения:**\n",
    "- Вычислительная сложность $ \\mathcal{O}(N^3) $ при обучении, что затрудняет применение для больших наборов данных.\n",
    "- Необходимость выбора ядра и его гиперпараметров, что может существенно влиять на качество аппроксимации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
