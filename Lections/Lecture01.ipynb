{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 1: Введение в машинное обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка основных типов задач в машинном обучении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Машинное обучение (ML) — это область искусственного интеллекта, которая разрабатывает алгоритмы и модели, позволяющие программам изучать закономерности в данных и принимать решения на основе полученного опыта. \n",
    "\n",
    "В основе ML лежит идея: вместо того чтобы задавать каждому шагу алгоритма **точную инструкцию**, мы предоставляем системе данные, и она самостоятельно находит **универсальные правила** для решения поставленных задач.\n",
    "\n",
    "Если рассматривать задачи с точки зрения процесса обучения, методы могут быть разделены на три категории:\n",
    "- **Обучение с учителем (Supervised Learning):** при наличии разметки – каждому входному примеру сопоставляется метка или целевое значение.\n",
    "- **Обучение без учителя (Unsupervised Learning):** данные не размечены, и задача заключается в выявлении скрытой структуры или закономерностей в данных.\n",
    "- **Обучение с подкреплением (Reinforcement Learning, RL):** Добавлен механизм влияния на процесс обучения модели из \"окружающего мира\".\n",
    "\n",
    "Если рассматривать задачи с точки зрения конечной цели, можно выделить две основные категории:\n",
    "- **Дискриминативные задачи:** направлены на разделение данных, предсказание меток или регрессию. Примерами являются классификация и регрессия. Моделируют зависимость вида $ p(y \\mid x) $. Цель таких моделей – предсказывать метки или значения на основе входных признаков, не пытаясь смоделировать сам процесс генерации данных.\n",
    "- **Генеративные задачи:** целятся в изучение и моделирование полного распределения данных, что позволяет генерировать новые, «реалистичные» примеры, аналогичные исходным. Моделируют совместное распределение $ p(x, y) $ (или просто $ p(x) $ в случае отсутствия меток) и, таким образом, способны генерировать новые данные, похожие на исходные, а также могут использоваться для решения задач классификации через вычисление условных вероятностей. Примеры — генерация текста, изображений, музыки и т.д.\n",
    "\n",
    "Рассмотрим постановку основных типов задач машинного обучения, а именно: **классификацию**, **регрессию** и **кластеризацию**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Ameer-Kwekha-Rashid-2/publication/351793676/figure/fig1/AS:1026865322016771@1621835179569/Overview-of-machinelearning-types-and-tasks.png\" alt=\"Описание изображения\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация\n",
    "\n",
    "#### Суть задачи:\n",
    "Классификация применяется, когда необходимо распределить объекты по заранее определённым категориям. В обучающей выборке каждый объект сопровождается меткой класса, что позволяет модели изучить характерные особенности каждого класса и впоследствии использовать их для распознавания новых данных.\n",
    "\n",
    "#### Постановка задачи:\n",
    "Допустим, у нас имеется обучающая выборка:\n",
    "$$\n",
    "\\mathcal{D} = \\{ (\\mathbf{x}_i, y_i) \\}_{i=1}^N,\n",
    "$$\n",
    "где $\\mathbf{x}_i \\in \\mathbb{R}^d$ — вектор признаков, а $y_i \\in \\{1, 2, \\dots, K\\}$ — метка класса.\n",
    "\n",
    "#### Примеры:\n",
    "- **Распознавание рукописных цифр:** например, набор данных MNIST, где изображенные цифры нужно отнести к соответствующим числам.\n",
    "- **Фильтрация спама:** классификация электронных писем на \"спам\" и \"не спам\".\n",
    "- **Распознавание образов:** определение типа объекта на изображении (кот, собака, автомобиль и т.д.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdEee4iKUeq_HmcuU79jWcMxMd0p9G_EejMTT7r6zU0aLg0FtunGvxUh7emGC1YuaFNvFF6H6fZBjqCmv_4jLxYzFZkzzNlmLAKWtcnwvKNvLYQJhw9E0qc1h0HNufo7dHnSHMZtHRTe1RavB-IIMJt7gNx?key=IbqRKL5SySsVffR6LRm6IA\" alt=\"Описание изображения\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регрессия\n",
    "\n",
    "#### Суть задачи:\n",
    "Регрессия направлена на предсказание непрерывной величины. Здесь модель изучает зависимость между входными признаками и непрерывным выходом, пытаясь минимизировать ошибку между предсказанными и истинными значениями.\n",
    "\n",
    "#### Постановка задачи:\n",
    "Рассмотрим обучающую выборку вида:\n",
    "$$\n",
    "\\mathcal{D} = \\{ (\\mathbf{x}_i, y_i) \\}_{i=1}^N,\n",
    "$$\n",
    "где $\\mathbf{x}_i \\in \\mathbb{R}^d$ — вектор признаков, а $y_i \\in \\mathbb{R}$ — непрерывное значение, которое требуется предсказать (тоже может быть многомерным).\n",
    "\n",
    "#### Примеры:\n",
    "- **Прогнозирование цен:** например, оценка рыночной стоимости недвижимости или прогнозирование цен акций.\n",
    "- **Физические прогнозы:** определение физических параметров системы по различным показателям.\n",
    "- **Временные ряды:** прогнозирование на основе исторических данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://images.javatpoint.com/tutorial/machine-learning/images/types-of-regression2.png\" alt=\"Описание изображения\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация\n",
    "\n",
    "#### Суть задачи:\n",
    "Кластеризация относится к методам обучения без учителя. Здесь задача состоит в группировке объектов на основании их сходства, при этом данные не размечены заранее. Кластеризация позволяет автоматически выявлять скрытые структуры и паттерны в данных.\n",
    "\n",
    "#### Постановка задачи:\n",
    "Пусть имеется набор объектов без меток:\n",
    "$$\n",
    "\\mathcal{D} = \\{ \\mathbf{x}_i \\}_{i=1}^N, \\quad \\mathbf{x}_i \\in \\mathbb{R}^d.\n",
    "$$\n",
    "Задача кластеризации заключается в разделении объектов на $K$ групп, таких что объекты внутри одной группы максимально похожи друг на друга, а объекты разных групп — существенно различаются. При этом в части методов $K$ не задается наперед.\n",
    "\n",
    "#### Примеры:\n",
    "- **Сегментация клиентов:** выделение групп потребителей с похожим поведением или интересами.\n",
    "- **Группировка документов:** автоматическое распределение текстов по темам.\n",
    "- **Обнаружение аномалий:** обнаружение выбросов или аномальных объектов в наборе данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/merge3cluster.jpg\" alt=\"Описание изображения\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target\n",
    "\n",
    "classifier = LogisticRegression(multi_class='ovr', \n",
    "                                solver='lbfgs', \n",
    "                                max_iter=200)\n",
    "classifier.fit(X, y)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                        np.arange(y_min, y_max, 0.01))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = classifier.predict(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, \n",
    "             alpha=0.3, cmap=plt.cm.Set1)\n",
    "plt.scatter(X[:, 0], X[:, 1], \n",
    "            c=y, edgecolors='k', cmap=plt.cm.Set1)\n",
    "plt.xlabel('Длина лепестка')\n",
    "plt.ylabel('Ширина лепестка')\n",
    "plt.title('Классификация ирисов с помощью Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание\n",
    "1. Используйте `sklearn.datasets.load_wine()` вместо `load_iris`.\n",
    "2. Измените классификатор на `LogisticRegression()`.\n",
    "3. Оцените качество модели с помощью `precision`, `recall`, `f1-score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "print(f\"MSE: {mean_squared_error(y, y_pred):.2f}\")\n",
    "\n",
    "plt.scatter(X, y, label='Исходные данные')\n",
    "sorted_idx = X[:, 0].argsort()\n",
    "plt.plot(X[sorted_idx], y_pred[sorted_idx],\n",
    "            color='red', linewidth=2, label='Линия регрессии')\n",
    "plt.xlabel('Признак')\n",
    "plt.ylabel('Целевая переменная')\n",
    "plt.title('Регрессия с использованием Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание\n",
    "1. Используйте `sklearn.datasets.load_diabetes()`.\n",
    "2. Попробуйте `RandomForestRegressor()`.\n",
    "3. Сравните MSE и MAE для обеих моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=1.0,\n",
    "                       random_state=42)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis',\n",
    "            label='Кластеры')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, marker='X',\n",
    "            label='Центроиды', edgecolor='k')\n",
    "plt.xlabel('Признак 1')\n",
    "plt.ylabel('Признак 2')\n",
    "plt.title('Кластеризация с использованием KMeans')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание\n",
    "1. Измените количество кластеров и проанализируйте качество, измеряя его Silhouette Score.\n",
    "2. Используйте `AgglomerativeClustering()` вместо `KMeans`.\n",
    "3. Визуализируйте кластеры (например, с помощью `matplotlib`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scientific Machine Learning (SciML) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scientific Machine Learning (SciML)** — это область, объединяющая традиционные методы численного моделирования и машинного обучения (ML) для решения сложных физических задач.  \n",
    "\n",
    "### Основные направления SciML  \n",
    "1. **Моделирование физических процессов**:  \n",
    "   - Решение дифференциальных уравнений (ODE, PDE) с помощью ML.  \n",
    "   - Ускорение традиционных численных методов.  \n",
    "2. **Предсказание параметров процесса**:  \n",
    "   - Вывод физических параметров из данных.  \n",
    "   - Заменение дорогостоящих экспериментов моделями ML (суррогатное моделирование).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практическое задание: Решение ОДУ с помощью нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Пример решения обыкновенного дифференциального уравнения (ОДУ) с помощью\n",
    "физически информированной нейронной сети (PINN) на основе PyTorch с последующим\n",
    "сравнением с численным решением из SciPy.\n",
    "\n",
    "Рассматривается уравнение:\n",
    "    u'(x) + u(x) = 0,   u(0) = 1.\n",
    "Аналитическое решение:\n",
    "    u(x) = exp(-x).\n",
    "\n",
    "Функция потерь состоит из двух частей:\n",
    "    1. Потерь по остатку уравнения (MSE(u'(x) + u(x))).\n",
    "    2. Потерь по начальному условию (MSE(u(0) - 1)).\n",
    "\n",
    "Оптимизация проводится с использованием алгоритма Adam.\n",
    "После обучения производится визуализация:\n",
    "    - предсказанного решения PINN,\n",
    "    - аналитического решения,\n",
    "    - численного решения, полученного с помощью scipy.integrate.solve_ivp.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.integrate import solve_ivp\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    \"\"\"\n",
    "    Физически информированная нейронная сеть для приближенного решения ОДУ.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(PINN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(1, 20)\n",
    "        self.hidden2 = nn.Linear(20, 20)\n",
    "        self.hidden3 = nn.Linear(20, 20)\n",
    "        self.out = nn.Linear(20, 1)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden1(x))\n",
    "        x = self.activation(self.hidden2(x))\n",
    "        x = self.activation(self.hidden3(x))\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "def compute_loss(model, x_colloc, x_ic):\n",
    "    \"\"\"\n",
    "    Вычисляет функцию потерь для PINN.\n",
    "\n",
    "    Аргументы:\n",
    "        model (nn.Module): обучаемая модель PINN.\n",
    "        x_colloc (torch.Tensor): тензор коллокационных точек.\n",
    "        x_ic (torch.Tensor): тензор для начального условия.\n",
    "\n",
    "    Возвращает:\n",
    "        loss (torch.Tensor): суммарное значение функции потерь.\n",
    "    \"\"\"\n",
    "    x_colloc.requires_grad = True\n",
    "    u_pred = model(x_colloc)\n",
    "\n",
    "    grad_outputs = torch.ones_like(u_pred)\n",
    "    du_dx = torch.autograd.grad(\n",
    "        u_pred, x_colloc,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # f(x) = u'(x) + u(x)\n",
    "    f = du_dx + u_pred\n",
    "    mse_pde = torch.mean(f ** 2)\n",
    "\n",
    "    # u(0) = 1\n",
    "    u_ic_pred = model(x_ic)\n",
    "    mse_ic = torch.mean((u_ic_pred - 1.0) ** 2)\n",
    "\n",
    "    loss = mse_pde + mse_ic\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(model: nn, \n",
    "          optimizer: optim, \n",
    "          num_epochs: int, \n",
    "          x_colloc: torch.Tensor, \n",
    "          x_ic: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Обучает модель PINN.\n",
    "\n",
    "    Аргументы:\n",
    "        model (nn.Module): обучаемая модель PINN.\n",
    "        optimizer (torch.optim.Optimizer): оптимизатор.\n",
    "        num_epochs (int): количество эпох обучения.\n",
    "        x_colloc (torch.Tensor): коллокационные точки.\n",
    "        x_ic (torch.Tensor): точка начального условия.\n",
    "    \"\"\"\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training epochs\"):\n",
    "        optimizer.zero_grad()\n",
    "        loss = compute_loss(model, x_colloc, x_ic)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch:5d}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "\n",
    "model = PINN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5000\n",
    "batch_size = 100\n",
    "\n",
    "x_colloc = torch.linspace(0, 1, batch_size).view(-1, 1)\n",
    "x_ic = torch.tensor([[0.0]])\n",
    "\n",
    "train(model, optimizer, num_epochs, x_colloc, x_ic)\n",
    "\n",
    "x_test = torch.linspace(0, 1, 100).view(-1, 1)\n",
    "with torch.no_grad():\n",
    "    u_pred = model(x_test).cpu().numpy().flatten()\n",
    "x_test_np = x_test.cpu().numpy().flatten()\n",
    "u_true = np.exp(-x_test_np)\n",
    "\n",
    "sol = solve_ivp(lambda t, u: -u, [0, 1], [1.0], t_eval=x_test_np)\n",
    "u_numerical = sol.y[0]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_test_np, u_true, 'b-', label='Аналитическое решение')\n",
    "plt.plot(x_test_np, u_pred, 'r--', label='Предсказание PINN')\n",
    "plt.plot(x_test_np, u_numerical, 'g-.', label='Численное решение (solve_ivp)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(x)')\n",
    "plt.title(\"Решение ОДУ: u'(x) + u(x) = 0\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дополнительное задание**: попробуйте изменить архитектуру сети и сравнить результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практическое задание: предсказание параметра маятника"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Пример оценки гравитационного ускорения g по данным колебаний маятника с\n",
    "использованием нейронной сети на PyTorch. Здесь мы генерируем синтетические\n",
    "данные для углов маятника в заданные моменты времени по формуле:\n",
    "\n",
    "    θ(t) = θ₀ · cos(√g · t)\n",
    "\n",
    "с добавлением гауссовского шума. Целевая задача — восстановить значение g,\n",
    "используя в качестве входных данных вектор измеренных значений угла в 50 точках.\n",
    "Для повышения надежности обучения генерируется набор обучающих примеров.\n",
    "\n",
    "Архитектура модели — простая полносвязная сеть (MLP) с последовательными слоями,\n",
    "использующими функцию активации Tanh.\n",
    "\n",
    "Основные шаги:\n",
    "    1. Генерация обучающего набора (несколько траекторий маятника с шумом);\n",
    "    2. Определение модели GravityNet, параметризуемой по размерности входа;\n",
    "    3. Обучение модели с использованием оптимизатора Adam и функции потерь MSELoss;\n",
    "    4. Визуализация графика зависимости функции потерь от эпох обучения;\n",
    "    5. Тестирование модели на чистых и шумовых данных.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def pendulum(t, theta0, g):\n",
    "    \"\"\"\n",
    "    Функция моделирует угловое отклонение маятника в момент времени t при начальном\n",
    "    отклонении theta0 и гравитационном ускорении g.\n",
    "\n",
    "    Аргументы:\n",
    "        t (np.ndarray): Моменты времени.\n",
    "        theta0 (float): Начальное отклонение.\n",
    "        g (float): Гравитационное ускорение.\n",
    "\n",
    "    Возвращает:\n",
    "        np.ndarray: Вектор углов маятника по времени.\n",
    "    \"\"\"\n",
    "    return theta0 * np.cos(np.sqrt(g) * t)\n",
    "\n",
    "\n",
    "def generate_dataset(n_samples, t_data, theta0, g, noise_std=0.05):\n",
    "    \"\"\"\n",
    "    Генерирует набор обучающих примеров. Каждый пример представляет собой вектор\n",
    "    измеренных значений угла маятника с добавлением гауссовского шума.\n",
    "\n",
    "    Аргументы:\n",
    "        n_samples (int): Число примеров в обучающем наборе.\n",
    "        t_data (np.ndarray): Массив моментов времени.\n",
    "        theta0 (float): Начальное отклонение маятника.\n",
    "        g (float): Истинное гравитационное ускорение.\n",
    "        noise_std (float): Стандартное отклонение гауссовского шума.\n",
    "\n",
    "    Возвращает:\n",
    "        tuple: Кортеж из numpy массивов (X, y), где X имеет форму (n_samples, len(t_data))\n",
    "               и y — массив с постоянным значением g.\n",
    "    \"\"\"\n",
    "    X_data, y_data = [], []\n",
    "    for _ in range(n_samples):\n",
    "        theta = pendulum(t_data, theta0, g) + np.random.normal(0, noise_std, len(t_data))\n",
    "        X_data.append(theta)\n",
    "        y_data.append(g)\n",
    "    X = np.array(X_data)\n",
    "    y = np.array(y_data).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Простая полносвязная нейронная сеть для оценки гравитационного ускорения g по\n",
    "    вектору измеренных углов маятника.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        Аргументы:\n",
    "            input_dim (int): Размерность входного вектора (например, 50).\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Прямой проход модели: x -> g_pred\n",
    "\n",
    "        Аргументы:\n",
    "            x (torch.Tensor): Входной тензор формы (batch_size, input_dim).\n",
    "\n",
    "        Возвращает:\n",
    "            torch.Tensor: Предсказанное значение g для каждого примера.\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, loss_fn, X_train, y_train, epochs=1000):\n",
    "    \"\"\"\n",
    "    Обучает модель нейронной сети на заданном наборе данных.\n",
    "\n",
    "    Аргументы:\n",
    "        model (nn.Module): Обучаемая модель.\n",
    "        optimizer (torch.optim.Optimizer): Оптимизатор (например, Adam).\n",
    "        loss_fn (nn.Module): Функция потерь (например, MSELoss).\n",
    "        X_train (torch.Tensor): Входные данные: тензор формы (n_samples, input_dim).\n",
    "        y_train (torch.Tensor): Целевая переменная: тензор формы (n_samples, 1).\n",
    "        epochs (int): Количество эпох обучения.\n",
    "\n",
    "    Возвращает:\n",
    "        list: Список значений функции потерь на каждой эпохе.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training epochs\"):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_train)\n",
    "        loss = loss_fn(predictions, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch:4d}, Loss: {loss.item():.6f}\")\n",
    "    return losses\n",
    "\n",
    "\n",
    "\n",
    "theta0 = 1.0 # Начальное отклонение\n",
    "true_g = 9.8 # Истинное значение гравитационного ускорения\n",
    "t_data = np.linspace(0, 10, 50)  # 50 моментов времени от 0 до 10 секунд\n",
    "\n",
    "n_samples = 100\n",
    "X_np, y_np = generate_dataset(n_samples, t_data, theta0, true_g, noise_std=0.05)\n",
    "\n",
    "X_train = torch.tensor(X_np, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_np, dtype=torch.float32)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = MLP(input_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "epochs = 1000\n",
    "losses = train_model(model, optimizer, loss_fn, X_train, y_train, epochs=epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(losses, label=\"Loss\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"Значение функции потерь\")\n",
    "plt.title(\"График сходимости обучения\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "theta_test = pendulum(t_data, theta0, true_g)\n",
    "X_test = torch.tensor(theta_test.reshape(1, -1), dtype=torch.float32)\n",
    "predicted_g = model(X_test).item()\n",
    "print(\"Предсказанное значение g (без шума):\", predicted_g)\n",
    "\n",
    "theta_test_noisy = (pendulum(t_data, theta0, true_g) \n",
    "                    + np.random.normal(0, 0.1, len(t_data)))\n",
    "X_test_noisy = torch.tensor(theta_test_noisy.reshape(1, -1), \n",
    "                            dtype=torch.float32)\n",
    "predicted_g_noisy = model(X_test_noisy).item()\n",
    "print(\"Предсказанное значение g (с шумом):\", predicted_g_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практическое задание: обучение PINN для уравнения теплопроводности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepxde -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Пример решения уравнения теплопроводности с помощью PINN (DeepXDE)\n",
    "с расширенной и качественной визуализацией результатов.\n",
    "\n",
    "Уравнение имеет вид:\n",
    "    u_t - α u_xx = 0,\n",
    "где α = 0.1, область по x ∈ [0,1] и t ∈ [0,2].\n",
    "\n",
    "Граничные условия:\n",
    "    u(x,t) = 0  на границе (при x = 0 и x = 1),\n",
    "начальное условие:\n",
    "    u(x,0) = sin(π x).\n",
    "\n",
    "Аналитическое решение:\n",
    "    u(x,t) = sin(π x) * exp( -α π² t ).\n",
    "После обучения модели выводится визуализация:\n",
    "  - Контурная карта предсказанного решения,\n",
    "  - Контурная карта аналитического решения,\n",
    "  - Контурная карта абсолютной ошибки между аналитическим решением и предсказанием.\n",
    "\"\"\"\n",
    "\n",
    "import deepxde as dde\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def heat_eq(x, u):\n",
    "    du_t = dde.grad.jacobian(u, x, i=0, j=1)  # du/dt\n",
    "    du_xx = dde.grad.hessian(u, x, i=0, j=0)    # du/dx\n",
    "    return du_t - 0.1 * du_xx  # α = 0.1\n",
    "\n",
    "# x ∈ [0, 1], t ∈ [0, 2]\n",
    "geom = dde.geometry.Interval(0, 1)\n",
    "timedomain = dde.geometry.TimeDomain(0, 1)\n",
    "geomtime = dde.geometry.GeometryXTime(geom, timedomain)\n",
    "\n",
    "# Boundary condition: u = 0 на границе области (при любом t)\n",
    "bc = dde.icbc.DirichletBC(geomtime, \n",
    "                          lambda x: 0, lambda _, on_boundary: on_boundary)\n",
    "\n",
    "# Initial condition: u(x,0) = sin(πx)\n",
    "ic = dde.icbc.IC(\n",
    "    geomtime,\n",
    "    lambda x: np.sin(np.pi * x[:, 0:1]),\n",
    "    lambda _, on_initial: on_initial,\n",
    ")\n",
    "\n",
    "net = dde.nn.FNN([2, 20, 20, 1], \"tanh\", \"Glorot normal\")\n",
    "data = dde.data.TimePDE(geomtime, heat_eq, [bc, ic], \n",
    "                        num_domain=1000, \n",
    "                        num_boundary=100, \n",
    "                        num_initial=100, \n",
    "                        num_test=1000)\n",
    "model = dde.Model(data, net)\n",
    "\n",
    "model.compile(\"adam\", lr=0.001)\n",
    "losshistory, train_state = model.train(epochs=5000)\n",
    "\n",
    "x = np.linspace(0, 1, 101)\n",
    "t = np.linspace(0, 2, 101)\n",
    "X, T = np.meshgrid(x, t)\n",
    "XT = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "\n",
    "u_pred = model.predict(XT)\n",
    "u_pred = u_pred.reshape(X.shape)\n",
    "\n",
    "u_exact = np.sin(np.pi * X) * np.exp(-0.1 * (np.pi**2) * T)\n",
    "\n",
    "error = np.abs(u_exact - u_pred)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "contour1 = ax1.contourf(X, T, u_pred, 100, cmap=\"viridis\")\n",
    "ax1.set_title(\"Предсказанное решение PINN\")\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"t\")\n",
    "fig.colorbar(contour1, ax=ax1)\n",
    "\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "contour2 = ax2.contourf(X, T, u_exact, 100, cmap=\"viridis\")\n",
    "ax2.set_title(\"Аналитическое решение\")\n",
    "ax2.set_xlabel(\"x\")\n",
    "ax2.set_ylabel(\"t\")\n",
    "fig.colorbar(contour2, ax=ax2)\n",
    "\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "contour3 = ax3.contourf(X, T, error, 100, cmap=\"hot\")\n",
    "ax3.set_title(\"Абсолютная ошибка |u_exact - u_pred|\")\n",
    "ax3.set_xlabel(\"x\")\n",
    "ax3.set_ylabel(\"t\")\n",
    "fig.colorbar(contour3, ax=ax3)\n",
    "\n",
    "plt.suptitle(\"Результаты решения уравнения теплопроводности методом PINN\", \n",
    "             fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
