{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 3: Классификация и ансамблевые методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача классификации заключается в том, чтобы по входным данным $\\mathbf{x} \\in \\mathbb{R}^d$ предсказать метку класса $y$, которая принадлежит конечному множеству категорий, например,\n",
    "$$\n",
    "y \\in \\{0, 1\\} \\quad \\text{(бинарная классификация)},\n",
    "$$\n",
    "или\n",
    "$$\n",
    "y \\in \\{1, 2, \\dots, K\\} \\quad \\text{(многоклассовая классификация)}.\n",
    "$$\n",
    "\n",
    "Цель обучения заключается в построении модели $f: \\mathbb{R}^d \\to \\text{Labels}$, которая минимизирует некоторую функцию потерь между истинными метками и предсказаниями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Постановка задачи\n",
    "\n",
    "Пусть $y \\in \\{0, 1\\}$, а вектор признаков $\\mathbf{x} \\in \\mathbb{R}^d$. Модель логистической регрессии задаётся следующим образом:\n",
    "$$\n",
    "P(y=1 \\mid \\mathbf{x}) = \\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad \\text{где } z = \\mathbf{w}^\\top \\mathbf{x} + b.\n",
    "$$\n",
    "Соответственно,\n",
    "$$\n",
    "P(y=0 \\mid \\mathbf{x}) = 1 - \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b).\n",
    "$$\n",
    "\n",
    "#### Функция правдоподобия и оптимизация\n",
    "\n",
    "Для определения параметров $\\mathbf{w}$ и $b$ используется метод максимального правдоподобия. Правдоподобие всей выборки:\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}, b) = \\prod_{i=1}^N \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b)^{y_i} \\left[1 - \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\right]^{1-y_i}.\n",
    "$$\n",
    "Переходя к логарифмическому правдоподобию, получаем:\n",
    "$$\n",
    "\\ell(\\mathbf{w}, b) = \\sum_{i=1}^N \\left[ y_i \\ln \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) + (1-y_i) \\ln \\left(1-\\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\right) \\right].\n",
    "$$\n",
    "Оптимизационная задача сводится к поиску параметров:\n",
    "$$\n",
    "\\max_{\\mathbf{w},\\,b} \\; \\ell(\\mathbf{w}, b),\n",
    "$$\n",
    "или эквивалентно – к минимизации отрицательного логарифмического правдоподобия (кросс-энтропийной функции потерь).\n",
    "\n",
    "#### Особенности\n",
    "\n",
    "- **Линейная граница разделения:** Модель определяет гиперплоскость, которая делит пространство на две области.\n",
    "- **Использование градиентного спуска:** В аналитическом виде задача не решается, поэтому применяются итерационные методы оптимизации (градиентный спуск, Ньютон и др.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcHuZ2ujpLJBtvr6h_R3W_yR10Ao4K-SDGEWcs00BX9nriJD7RJisOPIWyOzbLaaIDSTvqhwZaSm-krhGfdlsby-DWyFTNrPzUpU1M0g4-FluB83uNt5e9iBBAuz80pUmBP_hMwTlraiFhOLlhxmamxqHB0?key=3-PO9Abu6TW8hu9LQuqfMg\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_decision_boundary(model, X, y, ax=None, cmap=\"coolwarm\", h=0.02):\n",
    "    \"\"\"\n",
    "    Строит график разделяющей границы для модели классификации.\n",
    "\n",
    "    Аргументы:\n",
    "        model (sklearn-модель): обученная модель классификации.\n",
    "        X (np.ndarray): массив признаков, размер (n_samples, 2).\n",
    "        y (np.ndarray): массив целевых меток.\n",
    "        ax (matplotlib.axes.Axes, optional): ось для построения графика.\n",
    "        cmap (str): название цветовой карты.\n",
    "        h (float): шаг для создания сетки.\n",
    "        \n",
    "    Возвращает:\n",
    "        ax (matplotlib.axes.Axes): ось с графиком разделяющей границы.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=cmap)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\"k\", cmap=cmap)\n",
    "    \n",
    "    ax.set_xlabel(\"Признак 1\")\n",
    "    ax.set_ylabel(\"Признак 2\")\n",
    "    ax.set_title(\"Разделяющая граница логистической регрессии\")\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             confusion_matrix, \n",
    "                             classification_report)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(n_samples=50,\n",
    "                            n_features=2,\n",
    "                            n_redundant=0,\n",
    "                            n_informative=2,\n",
    "                            n_clusters_per_class=1,\n",
    "                            flip_y=0.1,\n",
    "                            class_sep=1.5,\n",
    "                            random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\")\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Матрица ошибок:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Отчет по классификации:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_boundary(log_reg, X, y)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Деревья решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Принцип работы\n",
    "\n",
    "1. **Корневой узел:** Всю выборку $D$ рассматривают как набор в корневом узле.\n",
    "2. **Разбиение:** На каждом шаге выбирается признак и порог, по которому выборка разделяется на два (или более) подмножеств $D_1$ и $D_2$. Критерий выбора разбиения основан на уменьшении нечистоты узла.\n",
    "3. **Остановка:** Процесс рекурсивного деления продолжается до достижения определённых условий (максимальная глубина, минимальное число объектов в узле, достижение чистого узла и т.д.).\n",
    "\n",
    "#### Метрики для разбиения\n",
    "\n",
    "##### Энтропия и информационный выигрыш\n",
    "\n",
    "Энтропия узла $D$ определяется как:\n",
    "$$\n",
    "H(D) = -\\sum_{k} p_k \\log_2 p_k,\n",
    "$$\n",
    "где $p_k$ — доля объектов, принадлежащих классу $k$. При разбиении узла информационный выигрыш ($IG$) вычисляется по формуле:\n",
    "$$\n",
    "IG(D, A) = H(D) - \\sum_{v \\in \\text{Values}(A)} \\frac{|D_v|}{|D|} H(D_v),\n",
    "$$\n",
    "где $D_v$ — подмножество данных, для которых значение признака $A$ равно $v$.\n",
    "\n",
    "##### Индекс Джини\n",
    "\n",
    "Другой распространённый критерий нечистоты – индекс Джини:\n",
    "$$\n",
    "G(D) = 1 - \\sum_{k} p_k^2.\n",
    "$$\n",
    "При выборе разбиения ищут признак, который даёт максимальное уменьшение индекса Джини:\n",
    "$$\n",
    "\\Delta G = G(D) - \\sum_{v \\in \\text{Values}(A)} \\frac{|D_v|}{|D|} G(D_v).\n",
    "$$\n",
    "\n",
    "#### Особенности\n",
    "\n",
    "- **Жадный алгоритм:** На каждом шаге выбирается локально оптимальное разбиение, что не гарантирует глобально оптимального дерева.\n",
    "- **Переобучение:** Очень глубокие деревья могут переобучаться, поэтому применяются методы отсечения (pruning) для улучшения обобщающей способности модели.\n",
    "- **Интерпретируемость:** Деревья решений являются интуитивно понятными и наглядно демонстрируют процесс принятия решений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://help.pyramidanalytics.com/Content/Root/MainClient/apps/Model/Model%20Pro/Data%20Flow/ML/Images/DecisionTrees-MachineLearining-01.png\" alt=\"Описание изображения\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(n_samples=50,\n",
    "                            n_features=2,\n",
    "                            n_redundant=0,\n",
    "                            n_informative=2,\n",
    "                            n_clusters_per_class=1,\n",
    "                            flip_y=0.1,\n",
    "                            class_sep=1.5,\n",
    "                            random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = tree_clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(acc * 100))\n",
    "print(\"Матрица ошибок:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Отчет по классификации:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plot_tree(tree_clf,\n",
    "            feature_names=[\"Признак 1\", \"Признак 2\"],\n",
    "            filled=True,\n",
    "            rounded=True,\n",
    "            fontsize=10)\n",
    "plt.title(\"Графическое представление дерева решений\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias variance decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из центральных понятий в машинном обучении является **разложение ошибки на смещение и дисперсию (bias-variance decomposition)**. Оно позволяет понять, какие составляющие вносят вклад в общую ошибку модели и как можно управлять этим балансом при выборе и настройке моделей.\n",
    "\n",
    "#### Теория\n",
    "Пусть у нас имеется функция истинной зависимости $ y = f(x) + \\varepsilon $, где:\n",
    "- $ f(x) $ – истинная функция,\n",
    "- $\\varepsilon$ – случайный шум с дисперсией $\\sigma^2$ (неизбежная ошибка, относящаяся к шуму).\n",
    "\n",
    "Общая квадратная ошибка модели $\\hat{f}(x)$ для фиксированной точки $x$ определяется как:\n",
    "$$\n",
    "\\mathbb{E}\\left[(y - \\hat{f}(x))^2\\right] = \\underbrace{\\left( \\mathbb{E}[\\hat{f}(x)] - f(x) \\right)^2}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}\\left[\\left(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)]\\right)^2\\right]}_{\\text{Variance}} + \\sigma^2.\n",
    "$$\n",
    "\n",
    "- **Смещение (Bias):** Показывает, насколько среднее предсказание модели отклоняется от истинной функции $ f(x) $. Высокое смещение может возникать, если модель слишком проста и не способна уловить сложность зависимости (так называемое недообучение).\n",
    "\n",
    "- **Дисперсия (Variance):** Характеризует изменчивость предсказаний модели при изменении обучающей выборки. Высокая дисперсия свойственна сложным моделям, которые сильно адаптируются к обучающим данным (что может привести к переобучению).\n",
    "\n",
    "Баланс между bias и variance определяет качество обобщения модели. Идеальной моделью является такая, которая имеет как низкое смещение, так и низкую дисперсию, однако в реальных задачах часто наблюдается компромисс между ними.\n",
    "\n",
    "#### Сильные и слабые модели\n",
    "В контексте ансамблирования модели часто делят на **слабые** и **сильные**:\n",
    "\n",
    "- **Слабые модели** – это модели, которые лишь немного лучше случайного угадывания. В задачах классификации такие алгоритмы называют «слабым обучением». Пример: *решающее дерево с одной глубиной* (decision stump) или в целом неглубокие деревья. Они, как правило, имеют высокое смещение (не могут уловить сложные зависимости), но обладают низкой дисперсией.\n",
    "\n",
    "- **Сильные модели** – это более сложные модели с хорошей способностью описывать данные (низкое смещение), но зачастую они обладают высокой дисперсией. Пример: полные, глубоко растущие деревья решений без отсечения. В ансамблировании, например, случайный лес использует именно такие сильные модели, и за счёт бутстреп-агрегации (bagging) достигается значительное снижение дисперсии.\n",
    "\n",
    "Различные методы ансамблирования используют именно эти особенности:\n",
    "\n",
    "- **Бэггинг (Bagging):**  \n",
    "  Обычно применяется для сильных моделей с высоким разбросом (variance). Например, полные деревья решений могут сильно колебаться от разбиения к разбиению, и их усреднение снижает дисперсию предсказания.\n",
    "\n",
    "- **Бустинг (Boosting):**\n",
    "  Часто использует слабые модели с ограниченной глубиной (низкая дисперсия, высокое смещение), внося последовательные коррективы в модель. Каждая последующая слабая модель обучается на исправлении ошибок предыдущих, что позволяет снижать смещение итогового ансамбля.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/images/bias_variance/bullseye.png\" alt=\"Описание изображения\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2024/07/eba93f5a75070f0fbb9d86bec8a009e9.webp\" alt=\"Описание изображения\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "\n",
    "np.random.seed(42)\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=1)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "avg_error, avg_bias, avg_var = bias_variance_decomp(clf, \n",
    "                                                    X_train, y_train, \n",
    "                                                    X_test, y_test,\n",
    "                                                    loss='0-1_loss', \n",
    "                                                    num_rounds=200, \n",
    "                                                    random_seed=1)\n",
    "\n",
    "print(\"Average Classification Error: {:.3f}\".format(avg_error))\n",
    "print(\"Average Bias: {:.3f}\".format(avg_bias))\n",
    "print(\"Average Variance: {:.3f}\".format(avg_var))\n",
    "\n",
    "labels = ['Error', 'Bias', 'Variance']\n",
    "values = [avg_error, avg_bias, avg_var]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = sns.color_palette(\"Set2\", n_colors=len(labels))\n",
    "bars = plt.bar(labels, values, color=colors)\n",
    "plt.ylabel(\"Значение\")\n",
    "plt.title(\"Разложение ошибки (Bias-Variance) для Decision Tree Classifier\")\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval, \n",
    "             f\"{yval:.3f}\", ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ансамблевые методы\n",
    "\n",
    "Ансамблевые методы объединяют несколько моделей для получения более точного и стабильного результата. Они используются для уменьшения дисперсии, смещения или общей ошибки модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.corporatefinanceinstitute.com/assets/ensemble-methods.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://intuitivetutorial.com/wp-content/uploads/2023/05/ensemble_models.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def true_function(x):\n",
    "    \"\"\"\n",
    "    Истинная функция: sin(x)\n",
    "    \"\"\"\n",
    "    return np.sin(x)\n",
    "\n",
    "\n",
    "def run_experiments(model, n_train, noise_std, x_grid, n_experiments):\n",
    "    \"\"\"\n",
    "    Запускает серию экспериментов для оценки предсказаний модели.\n",
    "\n",
    "    Аргументы:\n",
    "        model: экземпляр модели (например, DecisionTreeRegressor или RandomForestRegressor)\n",
    "        n_train (int): число обучающих образцов в каждом эксперименте.\n",
    "        noise_std (float): стандартное отклонение шума.\n",
    "        x_grid (np.ndarray): массив точек (форма (n_points, 1)) для вычисления предсказаний.\n",
    "        n_experiments (int): количество экспериментов.\n",
    "\n",
    "    Возвращает:\n",
    "        predictions (np.ndarray): массив предсказаний формы (n_experiments, n_points).\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for _ in range(n_experiments):\n",
    "        X_train = np.random.uniform(0, 2*np.pi, n_train).reshape(-1, 1)\n",
    "        noise = np.random.normal(0, noise_std, n_train)\n",
    "        y_train = true_function(X_train.ravel()) + noise\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(x_grid)\n",
    "        predictions.append(pred)\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "def bias_variance_decomposition(predictions, y_true):\n",
    "    \"\"\"\n",
    "    Вычисляет bias² и variance для набора предсказаний по координатам.\n",
    "\n",
    "    Аргументы:\n",
    "        predictions (np.ndarray): матрица предсказаний формы (n_experiments, n_points).\n",
    "        y_true (np.ndarray): истинные значения функции для точек сетки (n_points,).\n",
    "\n",
    "    Возвращает:\n",
    "        global_bias (float): усреднённое значение bias² по всем точкам сетки.\n",
    "        global_variance (float): усреднённое значение дисперсии предсказаний.\n",
    "    \"\"\"\n",
    "    mean_pred = np.mean(predictions, axis=0)\n",
    "    bias_sq = (y_true - mean_pred)**2\n",
    "    variance = np.var(predictions, axis=0)\n",
    "    global_bias = np.mean(bias_sq)\n",
    "    global_variance = np.mean(variance)\n",
    "    return global_bias, global_variance\n",
    "\n",
    "\n",
    "def display_results(model_names, bias_values, variance_values):\n",
    "    \"\"\"\n",
    "    Выводит результаты (bias² и variance для каждой модели) и строит группированный столбчатый график.\n",
    "\n",
    "    Аргументы:\n",
    "        model_names (list[str]): список имен моделей.\n",
    "        bias_values (list[float]): список значений bias² для каждой модели.\n",
    "        variance_values (list[float]): список значений variance для каждой модели.\n",
    "    \"\"\"\n",
    "    for name, bias, var in zip(model_names, bias_values, variance_values):\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Bias²: {bias:.3f}, Variance: {var:.3f}\\n\")\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(6, 4))\n",
    "    width = 0.35\n",
    "    indices = np.arange(len(model_names))\n",
    "\n",
    "    palette = sns.color_palette(\"Set2\", n_colors=2)\n",
    "    bars_bias = ax.bar(indices - width / 2, bias_values, width, \n",
    "                       label=\"Bias²\", color=palette[0])\n",
    "    bars_variance = ax.bar(indices + width / 2, variance_values, width, \n",
    "                           label=\"Variance\", color=palette[1])\n",
    "\n",
    "    ax.set_xlabel(\"Модель\")\n",
    "    ax.set_ylabel(\"Значение\")\n",
    "    ax.set_xticks(indices)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    ax.legend()\n",
    "\n",
    "    for bar in bars_bias + bars_variance:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f\"{height:.3f}\",\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bootstrap Aggregating (Bagging)** заключается в обучении множества моделей на различных бутстреп-выборках из исходного набора данных. Итоговый прогноз вычисляется как агрегированное (например, голосование для классификации) предсказание отдельных \n",
    "моделей.\n",
    "\n",
    "**Важно!** утстрепная выборка имеет такой же размер, что и исходная (генерация с повторениями)\n",
    "\n",
    "Пусть $ D $ — исходная выборка из $ N $ объектов. Из неё генерируются $ M $ бутстреп-выборок $ D^{(1)}, D^{(2)}, \\ldots, D^{(M)} $. Для каждой выборки $ D^{(m)} $ обучается модель $ f^{(m)}(x) $. Тогда итоговая модель имеет вид:\n",
    "  $$\n",
    "  f_{\\text{bag}}(x) = \\frac{1}{M}\\sum_{m=1}^{M} f^{(m)}(x).\n",
    "  $$\n",
    "  Для задач классификации итоговое решение часто определяется по принципу большинства голосов.\n",
    "\n",
    "**Особенность:** Снижает дисперсию модели, стабилизирует предсказания и уменьшает риск переобучения.\n",
    "\n",
    "**Пример:**\n",
    "    Random Forest – ансамбль деревьев решений, где каждая модель обучается на бутстреп-выборке, а при выборе разбиений учитывается случайное подмножество признаков.\n",
    "\n",
    "- Каждое дерево обучается на случайной бутстреп-выборке исходных данных.\n",
    "- При построении каждого дерева на каждом узле выбирается случайное подмножество признаков для разбиения, что снижает корреляцию между деревьями.\n",
    "\n",
    "Для $ M $ деревьев решений, где $ f^{(m)}(x) $ – предсказание $ m $-го дерева, итоговое предсказание определяется как:\n",
    "$$\n",
    "f_{\\text{RF}}(x) = \\frac{1}{M}\\sum_{m=1}^{M} f^{(m)}(x),\n",
    "$$\n",
    "при этом для задач классификации применяется голосование, а для регрессии – усреднение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20230731175958/Bagging-classifier.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "n_train = 20\n",
    "noise_std = 0.3\n",
    "n_experiments = 100\n",
    "x_grid = np.linspace(0, 2*np.pi, 100).reshape(-1, 1)\n",
    "y_true = true_function(x_grid.ravel())\n",
    "\n",
    "tree_model = DecisionTreeRegressor()\n",
    "tree_predictions = run_experiments(tree_model, n_train, \n",
    "                                   noise_std, x_grid, n_experiments)\n",
    "tree_bias, tree_variance = bias_variance_decomposition(tree_predictions, y_true)\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100)\n",
    "rf_predictions = run_experiments(rf_model, n_train, \n",
    "                                 noise_std, x_grid, n_experiments)\n",
    "rf_bias, rf_variance = bias_variance_decomposition(rf_predictions, y_true)\n",
    "\n",
    "model_names = [\"Decision Tree\", \"Bagging\"]\n",
    "bias_values = [tree_bias, rf_bias]\n",
    "variance_values = [tree_variance, rf_variance]\n",
    "\n",
    "display_results(model_names, bias_values, variance_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting – метод, в котором модели обучаются последовательно, каждая следующая модель сосредотачивается на ошибках предыдущих.\n",
    "\n",
    "**Основная идея:**\n",
    "    1. Изначально каждому объекту присваивается вес:  \n",
    "        $$\n",
    "        w_i^{(1)} = \\frac{1}{N}, \\quad i=1,\\dots,N.\n",
    "        $$\n",
    "    2. На каждом шаге $ m $ обучается слабый классификатор $ h_m(x) $ согласно текущей взвешенной выборке. Ошибка классификации определяется как:\n",
    "        $$\n",
    "        \\epsilon_m = \\sum_{i=1}^{N} w_i^{(m)} \\mathbb{I}\\left( y_i \\neq h_m(x_i) \\right).\n",
    "        $$\n",
    "    3. Вычисляется вес модели:\n",
    "        $$\n",
    "        \\gamma_m = \\ln\\frac{1-\\epsilon_m}{\\epsilon_m}.\n",
    "        $$\n",
    "    4. Обновление весов объектов производится по формуле:\n",
    "        $$\n",
    "        w_i^{(m+1)} = w_i^{(m)} \\exp\\left( \\gamma_m \\mathbb{I}\\left( y_i \\neq h_m(x_i) \\right) \\right),\n",
    "        $$\n",
    "        с последующей нормировкой так, чтобы $\\sum_i w_i^{(m+1)} = 1$.\n",
    "\n",
    "**Особенность:** Снижает смещение модели, позволяет добиться высокой точности за счет концентрации на сложных для классификации объектах.\n",
    "\n",
    "**Пример:** \n",
    "**Gradient Boosting**, где последовательно обучаются модели для минимизации остаточной ошибки с использованием градиентного спуска.\n",
    "Каждое новое базовое дерево обучается на остатках (или псевдопроизводных) функции потерь предыдущего ансамбля.\n",
    "\n",
    "Итоговая модель представлена как:\n",
    "$$\n",
    "f(x) = \\sum_{m=1}^{M} \\gamma_m h_m(x).\n",
    "$$\n",
    "На шаге $ m $ новая модель $ h_m(x) $ решает задачу аппроксимации негативного градиента:\n",
    "$$\n",
    "r_i^{(m)} = -\\left.\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\right|_{f(x) = f_{m-1}(x)},\n",
    "$$\n",
    "а коэффициент $ \\gamma_m $ выбирается для минимизации:\n",
    "$$\n",
    "\\gamma_m = \\arg\\min_{\\gamma} \\sum_{i=1}^{N} L\\left( y_i, f_{m-1}(x_i) + \\gamma\\, h_m(x_i) \\right).\n",
    "$$\n",
    "Здесь $ L(y, f(x)) $ – функция потерь, например, логарифмическая потеря для классификации или квадратичная ошибка для регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20210707140911/Boosting.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "n_train = 20\n",
    "noise_std = 0.3\n",
    "n_experiments = 100\n",
    "x_grid = np.linspace(0, 2 * np.pi, 100).reshape(-1, 1)\n",
    "y_true = true_function(x_grid.ravel())\n",
    "\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "tree_predictions = run_experiments(tree_model, n_train, \n",
    "                                   noise_std, x_grid, n_experiments)\n",
    "tree_bias, tree_variance = bias_variance_decomposition(tree_predictions, y_true)\n",
    "\n",
    "xgb_model = XGBRegressor(n_estimators=100,\n",
    "                         max_depth=2,\n",
    "                         learning_rate=0.05,\n",
    "                         subsample=0.8,\n",
    "                         colsample_bytree=0.8,\n",
    "                         random_state=42,\n",
    "                         objective=\"reg:squarederror\",\n",
    "                         verbosity=0)\n",
    "\n",
    "xgb_predictions = run_experiments(xgb_model, n_train, \n",
    "                                  noise_std, x_grid, n_experiments)\n",
    "xgb_bias, xgb_variance = bias_variance_decomposition(xgb_predictions, y_true)\n",
    "\n",
    "model_names = [\"Decision Tree\", \"Boosting\"]\n",
    "bias_values = [tree_bias, xgb_bias]\n",
    "variance_values = [tree_variance, xgb_variance]\n",
    "\n",
    "display_results(model_names, bias_values, variance_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод стекинга объединяет прогнозы нескольких базовых моделей посредством мета-модели. Базовые модели (уровень-0) выдают предсказания, которые затем используются в качестве входных признаков для мета-модели (уровень-1). Итоговая модель может быть записана как:\n",
    "  $$\n",
    "  f_{\\text{stack}}(x) = g\\Bigl( h_1(x), h_2(x), \\dots, h_M(x) \\Bigr),\n",
    "  $$\n",
    "  где $ g $ – обучаемая мета-функция.\n",
    "\n",
    "**Особенность:** Позволяет объединить сильные стороны различных моделей, что зачастую приводит к улучшению качества предсказаний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.appliedaicourse.com/blog/wp-content/uploads/2024/10/architecture-of-a-stacking-model-1024x534.jpg\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "n_train = 20\n",
    "noise_std = 0.3\n",
    "n_experiments = 100\n",
    "x_grid = np.linspace(0, 2 * np.pi, 100).reshape(-1, 1)\n",
    "y_true = true_function(x_grid.ravel())\n",
    "\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "tree_predictions = run_experiments(tree_model, n_train, noise_std, x_grid, n_experiments)\n",
    "tree_bias, tree_variance = bias_variance_decomposition(tree_predictions, y_true)\n",
    "\n",
    "estimators = [\n",
    "    ('dt', DecisionTreeRegressor(random_state=42)),\n",
    "    ('ridge', Ridge())\n",
    "]\n",
    "\n",
    "stack_model = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LinearRegression(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stack_predictions = run_experiments(stack_model, n_train, noise_std, x_grid, n_experiments)\n",
    "stack_bias, stack_variance = bias_variance_decomposition(stack_predictions, y_true)\n",
    "\n",
    "model_names = [\"Decision Tree\", \"Stacking\"]\n",
    "bias_values = [tree_bias, stack_bias]\n",
    "variance_values = [tree_variance, stack_variance]\n",
    "\n",
    "display_results(model_names, bias_values, variance_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ансамблевые методы для задачи регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ансамблевые методы могут используются и в задачах регрессии. Однако, итоговая аппроксимация исходной зависимости имеет характер разбиения на регионы, внутри которых модель ведёт себя как некоторая простая функция – обычно постоянная или, при специальных модификациях, линейная.\n",
    "\n",
    "Классические регрессионные деревья (например, `DecisionTreeRegressor`) разделяют пространство признаков на непересекающиеся регионы $ R_1, R_2, \\dots, R_n $. В каждом регионе дерево присваивает константное значение, как правило, равное среднему значению целевой переменной для объектов, попавших в этот регион:\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^{n} c_i \\cdot \\mathbb{I}(x \\in R_i)\n",
    "$$\n",
    "где:\n",
    "- $ c_i $ — среднее значение отклика для $ R_i $,\n",
    "- $\\mathbb{I}(x \\in R_i)$ — индикатор, равный 1, если $x$ принадлежит $R_i$.\n",
    "\n",
    "Таким образом, восстановленная функция является **кусочно постоянной**.\n",
    "\n",
    "#### Итог\n",
    "Таким образом, **если базовой моделью является дерево решений с константным прогнозом в листьях**, восстановленная функция будет иметь вид кусочно постоянного приближения. Если же в листьях обучаются линейные модели, итоговая функция становится кусочно-линейной. Ансамблевые методы, усредняя предсказания базовых моделей, могут сгладить эти переходы, но принцип регионального разбиения пространства сохраняется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 10\n",
    "X = np.random.uniform(0, 2*np.pi, n_samples)\n",
    "X = np.sort(X)\n",
    "y_true = true_function(X)\n",
    "noise = np.random.normal(0, 0.2, n_samples)\n",
    "y = y_true + noise\n",
    "\n",
    "X = X.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "gb_model = XGBRegressor(n_estimators=100,\n",
    "                        random_state=42,\n",
    "                        objective=\"reg:squarederror\",\n",
    "                        verbosity=0)\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "y_test_pred_gb = gb_model.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
    "mse_gb = mean_squared_error(y_test, y_test_pred_gb)\n",
    "print(f\"MSE (Random Forest Regressor): {mse_rf:.2f}\")\n",
    "print(f\"MSE (XGBoost Regressor): {mse_gb:.2f}\")\n",
    "\n",
    "x_grid = np.linspace(0, 2*np.pi, 500).reshape(-1, 1)\n",
    "y_grid_true = true_function(x_grid.ravel())\n",
    "y_grid_rf = rf_model.predict(x_grid)\n",
    "y_grid_gb = gb_model.predict(x_grid)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(x_grid, y_grid_true, color='black', lw=2, label='Истинная функция')\n",
    "plt.plot(x_grid, y_grid_rf, lw=2, label=f'Random Forest')\n",
    "plt.plot(x_grid, y_grid_gb, lw=2, label=f'XGBoost')\n",
    "plt.scatter(X, y, color='gray', alpha=0.6, label='Обучающие данные')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
