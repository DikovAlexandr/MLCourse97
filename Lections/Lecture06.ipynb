{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 6: Компьютерное зрение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## История развития"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В 50–70-х годах XX века компьютерное зрение начинало зарождаться как междисциплинарная область, объединяющая исследования в области обработки изображений, анализа форм и искусственного интеллекта, в те времена, основанного, по большей части, на принятии решений.  \n",
    "\n",
    "На ранних этапах активно применялись методы фильтрации, выделения краёв (операторы Собеля, Превитта, Канни), преобразование Хафа для обнаружения линий и геометрических фигур.\n",
    "\n",
    "В 80–90-х годах развитие компьютерного зрения характеризовалось переходом от чисто геометрических методов к статистическим подходам, появились алгоритмы для выделения устойчивых точек интереса, распознавания объектов, основываясь на вручную сконструированных признаках.\n",
    "\n",
    "Одним из переломных моментов стала идея использования нейронных сетей для обработки изображений:\n",
    "- *Neocognitron (1979):* Первая иерархическую архитектура, имеющая схожесть с современными сверточными нейронными сетями (CNN). \n",
    "- *LeNet (1989):* Ян Лекун и др. разработали LeNet для распознавания рукописных цифр, показав эффективность принципа локальной фильтрации и пулинга.\n",
    "\n",
    "С началом 2010-х годов благодаря доступности больших наборов данных (самый известный пример - ImageNet) и возрастанию вычислительных мощностей GPU и появления API для CUDA, было произведено возвращение CNN и увеличение числа слоев. Так появились:\n",
    "- *AlexNet (2012):* Прорывная архитектура, показавшая существенное снижение ошибки классификации в соревновании на датасете ImageNet.\n",
    "- *VGG:* Простая архитектуры со сверточными ядрами $ (3x3) $.\n",
    "- *GoogLeNet/Inception:* Вложенные модули, позволяющие учитывать признаки разных масштабов.\n",
    "- *ResNet:* Введение остаточных связей для борьбы с проблемой затухающих градиентов, что позволило строить очень глубокие сети.\n",
    "- *YOLO:* Предоставление предсказывай граничных полей и вероятности классов по полным изображениям за одну оценку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/publication/350085174/figure/fig3/AS:1022710452854786@1620844581128/Computer-Vision-Techniques-evolution-from-2012-to-2019-78-Alex-Net-78-Boosted-Cascade.png\" alt=\"Описание изображения\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Типы задач компьютерного зрения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Компьютерное зрение охватывает широкий спектр задач, каждая из которых требует специфических методов и подходов, прежде всего это касается разметки изображений. \n",
    "\n",
    "К основным задачам относятся:\n",
    "1. *Классификация изображений* - определение категории или метки всего изображения.\n",
    "2. *Детекция объектов* - Поиск и локализация объектов на изображении с указанием их координат.   \n",
    "3. *Сегментация изображений*\n",
    "   - *Семантическая сегментация* - классификация каждого пикселя изображения на основе принадлежности к определённому классу.\n",
    "   - *Инстанс-сегментация* - обнаружение отдельных экземпляров объектов с учетом их границ.\n",
    "4. *Распознавание объектов (лиц)* - детекция и идентификация объектов (по сути, совмещение задач классификации и детекции)\n",
    "5. *Работа с пространственными данными (облаками точек)* - определение глубины изображения и построение трёхмерной модели.\n",
    "6. *Анализ и обработка видео* - трекинг объектов, детектирование движений и распознавания действий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Dae-Young-Kang/publication/346091812/figure/fig5/AS:979480482955270@1610537753983/Computer-vision-tasks-Adapted-from.png\" alt=\"Описание изображения\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектуры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изображения представляют собой двумерные (хотя чаще трёхмерные, с учетом цветовых каналов) массивы пикселей - это исходные признаки. Положение пикселей в пространстве, как и непосредственно сигнал в них, должны каким-то образом быть учтены и обобщены. Для этого необходимо извлечь из исходных данных признаки более высокого уровня - для этого и нужна именно глубокая сеть.\n",
    "\n",
    "**Мотивировка:** \n",
    "- Поскольку соседние пиксели часто содержат схожую или взаимодополняющую информацию применение **операции свертки** кажется логичным. С помощью нее извлекаются локальных признаки вроде границ, текстур, углов.\n",
    "- По мере продвижения по слоям сети, признаки комбинируются в более сложные и **высокоуровневые признаки** - формы, паттерны и даже образы.\n",
    "- Необходимо обеспечить устойчивость к смещениям, масштабированию, поворотам, изменениям освещенности. Для этого используется операция **пулинга** и иные архитектурные решения, а также **аугментация** данных для обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сверточные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим входное изображение как тензор:\n",
    "$$\n",
    "\\mathbf{I} \\in \\mathbb{R}^{H \\times W \\times C},\n",
    "$$\n",
    "где $ H $ — высота, $ W $ — ширина, $ C $ — число каналов (например, 3 для RGB).\n",
    "\n",
    "Для каждого сверточного слоя используются фильтры (ядра), которые представляют собой матрицы обучаемых параметров:\n",
    "$$\n",
    "\\mathbf{K} \\in \\mathbb{R}^{k_h \\times k_w \\times C \\times F},\n",
    "$$\n",
    "где:\n",
    "- $k_h$ и $k_w$ — размеры фильтра по высоте и ширине,\n",
    "- $C$ — число входных каналов (совпадает с числом каналов изображения или выходным числом предыдущего слоя),\n",
    "- $F$ — количество фильтров.\n",
    "\n",
    "Операция свертки вычисляется следующим образом. Пусть $ \\mathbf{O} $ — выходной тензор, тогда для каждого фильтра $ f $ и позиции $(i, j)$ выход записывается как:\n",
    "$$\n",
    "O(i, j, f) = \\sum_{c=1}^{C} \\sum_{u=0}^{k_h - 1} \\sum_{v=0}^{k_w - 1} K(u, v, c, f) \\cdot I(i + u, j + v, c) + b(f).\n",
    "$$\n",
    "\n",
    "Для удобства описания вводятся следующие термины:\n",
    "- *Stride* - шаг свертки $ s $ определяет, на сколько пикселей сдвигается фильтр вдоль осей $ H $ и $ W $.\n",
    "- *Padding* - чтобы сохранить размерность или задать определенные граничные условия, к входному изображению добавляются нули по краям (zero-padding). Если используется паддинг $ p $, то размер выхода будет:\n",
    "  $$\n",
    "  H_{\\text{out}} = \\frac{H - k_h + 2p}{s} + 1, \\quad\n",
    "  W_{\\text{out}} = \\frac{W - k_w + 2p}{s} + 1.\n",
    "  $$\n",
    "\n",
    "После операции свертки на каждом элементе выходного тензора применяется нелинейная функция активации (обычно ReLU):\n",
    "$$\n",
    "Y(i,j,f) = \\phi\\Bigl(O(i,j,f)\\Bigr) = \\max(0, O(i,j,f)\\Bigr).\n",
    "$$\n",
    "\n",
    "Пулинг-слои применяются для агрегации информации и снижения размерности. Наиболее распространены:\n",
    "- *Max Pooling* - выбирается максимальное значение в заданном окне.\n",
    "- *Average Pooling* - вычисляется среднее значение пикселей в окне.\n",
    "\n",
    "Если окно имеет размер $ p \\times p $ с шагом $ s_p $, то:\n",
    "$$\n",
    "P(i,j,f) = \\max_{\\substack{0 \\leq u < p\\\\0 \\leq v < p}} Y(i\\cdot s_p + u, j\\cdot s_p + v, f)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://saturncloud.io/images/blog/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way.webp\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://contenthub-static.grammarly.com/blog/wp-content/uploads/2024/09/157421-grammarly-6162-blogvisuals-cnns-Image2-Op1-B_V1_new.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    \"\"\"\n",
    "    Обучение модели на одной эпохе.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in tqdm(enumerate(train_loader), \n",
    "                                          total=len(train_loader),\n",
    "                                          desc=f'Epoch {epoch}'):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}. Loss: {epoch_loss:.4f}\")\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Тестирование модели на тестовой выборке.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, \"\n",
    "          f\"Accuracy: {correct}/{len(test_loader.dataset)} \"\n",
    "          f\"({accuracy:.2f}%)\")\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "def visualize_activations(model, device, image):\n",
    "    \"\"\"\n",
    "    Визуализация активаций сверточных слоев для одного изображения.\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activations[name] = output.detach().cpu()\n",
    "        return hook\n",
    "\n",
    "    model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "    model.conv2.register_forward_hook(get_activation('conv2'))\n",
    "    model.conv3.register_forward_hook(get_activation('conv3'))\n",
    "    model.conv4.register_forward_hook(get_activation('conv4'))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        _ = model(image)\n",
    "\n",
    "    for layer_name, activation in activations.items():\n",
    "        num_channels = activation.shape[1]\n",
    "        num_plots = min(num_channels, 6)\n",
    "        fig, axes = plt.subplots(1, num_plots, figsize=(10, 3))\n",
    "        fig.suptitle(f'Активности слоя {layer_name}', fontsize=16)\n",
    "        for i in range(num_plots):\n",
    "            ax = axes[i] if num_plots > 1 else axes\n",
    "            feature_map = activation[0, i, :, :].numpy()\n",
    "            ax.imshow(feature_map, cmap='viridis')\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f'Канал {i}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Используем устройство:\", device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, \n",
    "                               transform=transform)\n",
    "subset_train = np.arange(10000)\n",
    "train_dataset = Subset(train_dataset, subset_train)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, \n",
    "                              transform=transform)\n",
    "subset_test = np.arange(1000)\n",
    "test_dataset = Subset(test_dataset, subset_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, \n",
    "                         shuffle=False)\n",
    "\n",
    "model = DeepCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, device, train_loader, \n",
    "                       optimizer, criterion, epoch)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_loss, accuracy = test(model, device, test_loader, criterion)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "epochs_range = np.arange(1, epochs + 1)\n",
    "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "ax1.plot(epochs_range, train_losses, 'o-', label='Train Loss')\n",
    "ax1.plot(epochs_range, test_losses, 's-', label='Test Loss')\n",
    "ax1.set_xlabel('Эпоха')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('График сходимости и метрика распознавания')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs_range, test_accuracies, 'd--', color='red', \n",
    "         label='Test Accuracy')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sample_image, sample_label = test_dataset[0]\n",
    "sample_image = sample_image.unsqueeze(0)\n",
    "visualize_activations(model, device, sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Трансформеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформерные модели, изначально разработанные для обработки последовательностей в NLP, адаптируются для анализа изображений. Ключевая идея состоит в разбиении изображения на патчи и последующей обработке их как последовательности токенов.\n",
    "\n",
    "#### Vision Transformer (ViT)\n",
    "##### Разбиение изображений на патчи\n",
    "Пусть имеется изображение $ \\mathbf{I} \\in \\mathbb{R}^{H \\times W \\times C} $. Его разбивают на $ N $ неперекрывающихся патчей размером $ P \\times P $ (при условии, что $ H $ и $ W $ кратны $ P $). Каждый патч выпрямляется в вектор:\n",
    "$$\n",
    "\\mathbf{x}_i \\in \\mathbb{R}^{P^2 \\cdot C}, \\quad i = 1, \\dots, N,\n",
    "$$\n",
    "где $ N = \\frac{H \\times W}{P^2} $.\n",
    "\n",
    "##### Линейное вложение и позиционные кодировки\n",
    "Каждый выпрямленный патч отображается в пространство фиксированной размерности $ D $ с помощью линейного слоя:\n",
    "$$\n",
    "\\mathbf{z}_i = \\mathbf{E} \\, \\mathbf{x}_i,\n",
    "$$\n",
    "где $ \\mathbf{E} \\in \\mathbb{R}^{D \\times (P^2 \\cdot C)} $ — матрица вложения. Для сохранения информации о порядке патчей добавляют позиционные кодировки:\n",
    "$$\n",
    "\\mathbf{z}_i = \\mathbf{z}_i + \\mathbf{p}_i,\n",
    "$$\n",
    "где $ \\mathbf{p}_i $ — позиционный эмбеддинг-вектор для $ i $-го патча.\n",
    "\n",
    "##### Трансформер-энкодер\n",
    "Последовательность эмбеддингов $\\{ \\mathbf{z}_1, \\dots, \\mathbf{z}_N \\}$ подается на стандартный блок трансформера, состоящий из:\n",
    "\n",
    "1. *Многошагового механизма самовнимания (Multi-Head Self-Attention, MHA):*\n",
    "   Для входной матрицы $ \\mathbf{Z} \\in \\mathbb{R}^{N \\times D} $ вычисляются запросы (Query), ключи (Key) и значения (Value):\n",
    "   $$\n",
    "   \\mathbf{Q} = \\mathbf{Z} \\mathbf{W}^Q, \\quad \\mathbf{K} = \\mathbf{Z} \\mathbf{W}^K, \\quad \\mathbf{V} = \\mathbf{Z} \\mathbf{W}^V,\n",
    "   $$\n",
    "   где $ \\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V \\in \\mathbb{R}^{D \\times d_k} $ — матрицы преобразования. Механизм самовнимания вычисляется по формуле:\n",
    "   $$\n",
    "   \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\operatorname{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right) \\mathbf{V}.\n",
    "   $$\n",
    "   Множество таких heads, каждая со своим набором матриц $ \\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V $ вычисляются параллельно, после чего их результаты объединяются.\n",
    "\n",
    "2. *Feed-Forward Network:*\n",
    "   Каждая позиция обрабатывается независимой двухслойной сетью (Multi-Layer Perceptron):\n",
    "   $$\n",
    "   \\text{FFN}(\\mathbf{z}) = \\max(0, \\mathbf{z} \\mathbf{W}_1 + \\mathbf{b}_1) \\, \\mathbf{W}_2 + \\mathbf{b}_2.\n",
    "   $$\n",
    "\n",
    "Эти блоки повторяются несколько раз, после чего выход используется для решения задачи классификации, либо адаптируется под задачи сегментации, детекции и др.\n",
    "\n",
    "Такой подход позволяет учитывать дальние зависимости между различными частями изображения, чего сложно добиться свертками, хотя часть эти подходы комбинируют.\n",
    "\n",
    "Кроме того, полученное представления изображение (вектор в латентном пространстве признаков) может быть использовано для получения мультимодальных моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://viso.ai/wp-content/uploads/2021/09/vision-transformer-vit.png\" alt=\"Описание изображения\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class ViTForMNIST(nn.Module):\n",
    "    def __init__(self, image_size=28, patch_size=7, in_channels=1,\n",
    "                 num_classes=10, embed_dim=64, depth=6, num_heads=4,\n",
    "                 dropout=0.1):\n",
    "        super(ViTForMNIST, self).__init__()\n",
    "\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.patch_embed = nn.Conv2d(in_channels, embed_dim,\n",
    "                                     kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, \n",
    "                                                  embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.patch_embed.weight)\n",
    "        if self.patch_embed.bias is not None:\n",
    "            nn.init.zeros_(self.patch_embed.bias)\n",
    "        \n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        self.patch_embeddings = x.detach().cpu()\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x = x + self.pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        self.encoder_output = x.detach().cpu()\n",
    "\n",
    "        x_cls = x[:, 0]\n",
    "        logits = self.mlp_head(x_cls)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_vit(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    \"\"\"\n",
    "    Обучение модели ViT на одной эпохе.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in tqdm(enumerate(train_loader), \n",
    "                                          total=len(train_loader),\n",
    "                                          desc=f'Epoch {epoch}'):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}. Loss: {epoch_loss:.4f}\")\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def test_vit(model, device, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Тестирование модели ViT на тестовой выборке.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, \"\n",
    "          f\"Accuracy: {correct}/{len(test_loader.dataset)} \"\n",
    "          f\"({accuracy:.2f}%)\")\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "def visualize_vit_activations(model, device, image):\n",
    "    \"\"\"\n",
    "    Визуализация активаций модели ViT для одного изображения.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        _ = model(image)\n",
    "    patch_emb = model.patch_embeddings\n",
    "    patch_emb_avg = patch_emb.mean(dim=2).squeeze(0).numpy()\n",
    "    grid_patch = patch_emb_avg.reshape(int(np.sqrt(model.num_patches)),\n",
    "                                       int(np.sqrt(model.num_patches)))\n",
    "\n",
    "    encoder_out = model.encoder_output[:, 1:, :]\n",
    "    encoder_out_avg = encoder_out.mean(dim=2).squeeze(0).numpy()\n",
    "    grid_encoder = encoder_out_avg.reshape(int(np.sqrt(model.num_patches)),\n",
    "                                           int(np.sqrt(model.num_patches)))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    axes[0].imshow(grid_patch, cmap='viridis')\n",
    "    axes[0].set_title('Усреднённые Patch Embeddings')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(grid_encoder, cmap='viridis')\n",
    "    axes[1].set_title('Усреднённый Encoder Output')\n",
    "    axes[1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Используем устройство:\", device)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, \n",
    "                               transform=transform)\n",
    "subset_train = np.arange(10000)\n",
    "train_dataset = Subset(train_dataset, subset_train)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, \n",
    "                              transform=transform)\n",
    "subset_test = np.arange(1000)\n",
    "test_dataset = Subset(test_dataset, subset_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, \n",
    "                         shuffle=False)\n",
    "\n",
    "model = ViTForMNIST().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    t_loss = train_vit(model, device, train_loader, \n",
    "                       optimizer, criterion, epoch)\n",
    "    train_losses.append(t_loss)\n",
    "    t_loss, acc = test_vit(model, device, test_loader, criterion)\n",
    "    test_losses.append(t_loss)\n",
    "    test_accuracies.append(acc)\n",
    "\n",
    "epochs_range = np.arange(1, epochs + 1)\n",
    "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "ax1.plot(epochs_range, train_losses, 'o-', label='Train Loss')\n",
    "ax1.plot(epochs_range, test_losses, 's-', label='Test Loss')\n",
    "ax1.set_xlabel('Эпоха')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Сходимость и метрика распознавания ViT')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs_range, test_accuracies, 'd--', \n",
    "         color='red', label='Test Accuracy')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sample_image, sample_label = test_dataset[0]\n",
    "sample_image = sample_image.unsqueeze(0)\n",
    "visualize_vit_activations(model, device, sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применение готовых архитектур "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение модели для каждой конкретной задачи затратно по времени, а также требует больших объемов данных и длительного процесса обучения.\n",
    "\n",
    "Тут проявляется себя свойство обобщения у нейронный сетей - если решать какую-либо общую задачу, то от нее можно перейти к частной. Таким образом готовые модели, веса которых зачастую доступны открыто, обученные на огромных датасетах, могут быть легко адаптированы для конкретных задач, поскольку в них уже хорошо обучены слои извлекающие признаки. Такой подход называется transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(\n",
    "            tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}: Train Loss: {epoch_loss:.4f}\")\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Используем устройство:\", device)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(224),\n",
    "                                transforms.Grayscale(num_output_channels=3),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True,\n",
    "                                transform=transform)\n",
    "subset_train = np.arange(10000)\n",
    "train_dataset = Subset(train_dataset, subset_train)\n",
    "\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True,\n",
    "                                transform=transform)\n",
    "subset_test = np.arange(1000)\n",
    "test_dataset = Subset(test_dataset, subset_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, \n",
    "                         shuffle=False)\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, 10)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    t_loss = train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "    train_losses.append(t_loss)\n",
    "    t_loss, acc = test(model, device, test_loader, criterion)\n",
    "    test_losses.append(t_loss)\n",
    "    test_accuracies.append(acc)\n",
    "\n",
    "epochs_range = np.arange(1, epochs + 1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs_range, train_losses, \"o-\", label=\"Train Loss\")\n",
    "plt.plot(epochs_range, test_losses, \"s-\", label=\"Test Loss\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Сходимость ResNet18 на MNIST\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(epochs_range, test_accuracies, \"d--\", \n",
    "         color=\"red\", label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Точность ResNet18 на MNIST\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(\n",
    "            tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}: Train Loss: {epoch_loss:.4f}\")\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Используем устройство:\", device)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(224),\n",
    "                                transforms.Grayscale(num_output_channels=3),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True,\n",
    "                                transform=transform)\n",
    "subset_train = np.arange(10000)\n",
    "train_dataset = Subset(train_dataset, subset_train)\n",
    "\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True,\n",
    "                                transform=transform)\n",
    "subset_test = np.arange(1000)\n",
    "test_dataset = Subset(test_dataset, subset_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "model = torchvision.models.vit_b_16(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "in_features = model.heads[-1].in_features\n",
    "model.heads[-1] = nn.Linear(in_features, 10)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.heads[-1].parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    t_loss = train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "    train_losses.append(t_loss)\n",
    "    t_loss, acc = test(model, device, test_loader, criterion)\n",
    "    test_losses.append(t_loss)\n",
    "    test_accuracies.append(acc)\n",
    "\n",
    "epochs_range = np.arange(1, epochs + 1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs_range, train_losses, \"o-\", label=\"Train Loss\")\n",
    "plt.plot(epochs_range, test_losses, \"s-\", label=\"Test Loss\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Сходимость ViT на MNIST\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(epochs_range, test_accuracies, \"d--\", \n",
    "         color=\"red\", label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Точность ViT на MNIST\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
